{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Electric Barometer","text":"<p>Electric Barometer is a forecasting evaluation and decision-governance framework designed for operational systems where forecast errors carry asymmetric cost and decisions must be reproducible, auditable, and policy-driven.</p> <p>Unlike traditional forecasting toolchains that emphasize symmetric accuracy metrics and model performance in isolation, Electric Barometer treats forecasting as one component of a broader decision system. It separates forecasting, evaluation, selection, and policy tuning into explicit layers, allowing organizations to reason about risk, tradeoffs, and operational readiness in a principled way.</p>"},{"location":"#what-electric-barometer-is-designed-to-solve","title":"What Electric Barometer is designed to solve","text":"<p>Electric Barometer is built for environments where:</p> <ul> <li>Forecast errors have unequal consequences (e.g., under- vs over-production).</li> <li>Decisions must be defensible and repeatable, not ad hoc.</li> <li>Multiple models, features, and configurations must be compared systematically.</li> <li>Evaluation outcomes must translate into actionable policy, not just scores.</li> <li>Forecasting operates over panel or entity-level time series in production workflows.</li> </ul> <p>The framework emphasizes cost-aware evaluation, explicit decision policies, and reproducible artifacts over raw predictive accuracy.</p>"},{"location":"#the-electric-barometer-ecosystem","title":"The Electric Barometer ecosystem","text":"<p>Electric Barometer is an ecosystem of focused libraries and tools, each responsible for a specific layer of the forecasting and decision lifecycle. These components are coordinated through shared concepts, contracts, and workflows, but remain independently versioned and maintainable.</p> <p>At a high level, the ecosystem covers:</p> <ul> <li>Feature engineering and data preparation</li> <li>Forecast generation</li> <li>Cost-aware evaluation and sensitivity analysis</li> <li>Model selection and tie-breaking</li> <li>Policy tuning and parameter optimization</li> <li>Integration testing and end-to-end validation</li> </ul> <p>This documentation site serves as the central narrative and integration layer for the ecosystem. Detailed, repository-specific documentation lives in each library\u2019s own documentation and is surfaced here where appropriate.</p>"},{"location":"#how-to-use-this-documentation","title":"How to use this documentation","text":"<p>This site is organized by intent, not by repository.</p>"},{"location":"#if-you-are-new-to-electric-barometer","title":"If you are new to Electric Barometer","text":"<p>Start with the orientation material: - Start Here \u2014 high-level overview, quickstart, and ecosystem map.</p>"},{"location":"#if-you-want-to-understand-the-theory-and-design","title":"If you want to understand the theory and design","text":"<p>Read the system foundations: - Concepts \u2014 problem framing, asymmetric cost, governance, and readiness. - Metrics \u2014 decision-aware evaluation measures and their interpretation.</p>"},{"location":"#if-you-want-to-run-or-design-workflows","title":"If you want to run or design workflows","text":"<p>Follow the lifecycle: - Workflows \u2014 how data, forecasts, evaluation, selection, and policies connect end-to-end. - Optimization \u2014 how policies and parameters are tuned against objectives.</p>"},{"location":"#if-you-are-using-or-contributing-to-a-specific-library","title":"If you are using or contributing to a specific library","text":"<p>Go to the ecosystem entry points: - Packages \u2014 curated overviews and links to each Electric Barometer repository and its documentation.</p>"},{"location":"#if-you-want-the-formal-foundations","title":"If you want the formal foundations","text":"<p>Consult the research artifacts: - Papers \u2014 technical notes and formal papers defining metrics, frameworks, and governance models.</p>"},{"location":"#maturity-and-scope","title":"Maturity and scope","text":"<p>Electric Barometer is under active development. Core concepts and evaluation principles are stable, while implementation details and tooling continue to evolve. The technical notes and papers represent the formal definitions that guide the system design.</p> <p>This documentation prioritizes clarity, traceability, and long-term maintainability over brevity or marketing polish.</p>"},{"location":"#where-to-go-next","title":"Where to go next","text":"<ul> <li>Begin with Start Here for orientation and setup.</li> <li>Read Concepts to understand the system philosophy.</li> <li>Explore Workflows to see how Electric Barometer operates in practice.</li> <li>Visit Packages to work with specific libraries.</li> <li>Review Papers for formal definitions and theoretical grounding.</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>The Electric Barometer framework is built on a small set of foundational concepts that shape how forecasting systems are evaluated, governed, and used in operational decision-making.</p> <p>These concepts are not implementation details. They define the mental model of the system: how problems are framed, how uncertainty and cost are treated, how measurements relate to decisions, and how choices are governed over time.</p> <p>Understanding these concepts is essential before engaging with metrics, workflows, optimization, or individual libraries.</p>"},{"location":"concepts/#why-concepts-come-first","title":"Why concepts come first","text":"<p>Forecasting systems often fail not because of poor models, but because of unclear assumptions about what is being optimized, how risk is handled, and how decisions are made.</p> <p>Electric Barometer addresses these failures by making core assumptions explicit:</p> <ul> <li>Forecasting is a means to support decisions, not an end in itself.</li> <li>Errors are not interchangeable when costs are asymmetric.</li> <li>Measurement and choice are distinct stages with different responsibilities.</li> <li>Policy and preference must be visible, not embedded implicitly.</li> <li>Suitability for use matters as much as predictive performance.</li> </ul> <p>The Concepts section establishes these principles before introducing tools or techniques.</p>"},{"location":"concepts/#how-to-read-this-section","title":"How to read this section","text":"<p>Each concept page focuses on a single foundational idea and is written to stand on its own. Together, they form a coherent system.</p> <p>The recommended reading order is:</p> <ol> <li> <p>Problem Framing    Defines how forecasting problems should be framed as decision problems rather than prediction tasks.</p> </li> <li> <p>Asymmetric Cost    Explains why the direction and context of error matter more than magnitude alone in operational settings.</p> </li> <li> <p>Evaluation vs Decisioning    Separates measurement from action, clarifying the role of metrics versus policy.</p> </li> <li> <p>Governance    Describes how evaluation and decision processes are controlled, audited, and evolved over time.</p> </li> <li> <p>Readiness and the Readiness Adjustment Layer (RAL)    Introduces readiness as a decision-facing concept and explains how evaluation outputs are made suitable for operational use.</p> </li> <li> <p>Glossary    Provides a shared vocabulary used consistently throughout the Electric Barometer ecosystem.</p> </li> </ol> <p>While the concepts build on one another, they are designed to be referenced independently as needed.</p>"},{"location":"concepts/#scope-and-boundaries","title":"Scope and boundaries","text":"<p>The Concepts section intentionally avoids:</p> <ul> <li>Formal mathematical definitions</li> <li>Metric equations or algorithms</li> <li>Package-specific behavior</li> <li>Implementation or configuration details</li> </ul> <p>Those topics are addressed elsewhere:</p> <ul> <li>Metrics formalize evaluation measures</li> <li>Workflows describe how systems operate end-to-end</li> <li>Optimization covers selection, tuning, and policy application</li> <li>Packages document individual libraries and APIs</li> <li>Papers provide formal and theoretical foundations</li> </ul> <p>Concepts define why the system is structured as it is, not how it is implemented.</p>"},{"location":"concepts/#concepts-as-a-stable-foundation","title":"Concepts as a stable foundation","text":"<p>Concepts change more slowly than code.</p> <p>By establishing clear conceptual boundaries, Electric Barometer enables:</p> <ul> <li>Consistent interpretation of metrics and results</li> <li>Transparent decision-making across teams and time</li> <li>Evolution of tools without redefining fundamentals</li> <li>Shared understanding between technical and non-technical stakeholders</li> </ul> <p>These concepts serve as the stable foundation on which the rest of the ecosystem is built.</p>"},{"location":"concepts/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Begin with Problem Framing to understand how forecasting problems are defined.</li> <li>Explore Metrics to see how these concepts are operationalized in evaluation.</li> <li>Follow Workflows to understand how concepts translate into execution.</li> <li>Consult Papers for formal definitions and deeper theoretical grounding.</li> </ul> <p>The Concepts section is the intellectual core of Electric Barometer. Everything else builds on it.</p>"},{"location":"concepts/asymmetric-cost/","title":"Asymmetric Cost in Forecasting","text":"<p>In many operational forecasting problems, not all errors are equal. A forecast that is too low can have consequences that are fundamentally different from a forecast that is too high, even when the magnitude of the error is the same. This imbalance is known as asymmetric cost, and it is one of the core motivations for the Electric Barometer framework.</p> <p>Traditional forecasting evaluation often assumes that errors are symmetric and interchangeable. Electric Barometer rejects this assumption and treats cost asymmetry as a first-class design concern.</p>"},{"location":"concepts/asymmetric-cost/#the-limitation-of-symmetric-accuracy","title":"The limitation of symmetric accuracy","text":"<p>Most common accuracy metrics\u2014such as mean absolute error (MAE), mean squared error (MSE), or root mean squared error (RMSE)\u2014implicitly assume that over-forecasting and under-forecasting are equally undesirable.</p> <p>Under this assumption:</p> <ul> <li>A forecast that overshoots demand by 10 units is treated the same as one that undershoots by 10 units.</li> <li>Errors are evaluated purely by magnitude, not by direction.</li> <li>Operational consequences are abstracted away from the evaluation process.</li> </ul> <p>While this assumption simplifies analysis, it often fails in real-world operational systems, where the direction of an error can matter more than its size.</p>"},{"location":"concepts/asymmetric-cost/#directional-error-and-operational-consequence","title":"Directional error and operational consequence","text":"<p>In operational settings, forecast errors are rarely neutral. The same numerical deviation can lead to very different outcomes depending on its direction.</p> <p>Examples include:</p> <ul> <li>Under-forecasting demand leading to stockouts, missed revenue, or service degradation.</li> <li>Over-forecasting demand leading to waste, excess inventory, or unnecessary labor.</li> <li>Forecasting too late versus too early in time-sensitive processes.</li> </ul> <p>These outcomes are not symmetric, and they are not interchangeable. Treating them as such obscures the true risk profile of a forecasting system.</p>"},{"location":"concepts/asymmetric-cost/#underbuild-vs-overbuild-why-sign-matters","title":"Underbuild vs overbuild: why sign matters","text":"<p>Asymmetric cost is often most visible in underbuild versus overbuild scenarios.</p> <p>An underbuild typically represents a failure to meet realized demand. Its consequences may include lost sales, customer dissatisfaction, or operational bottlenecks. An overbuild, by contrast, may result in inefficiency, waste, or opportunity cost, but not necessarily lost demand.</p> <p>In these cases:</p> <ul> <li>The sign of the forecast error encodes meaning.</li> <li>The operational penalty is not proportional to absolute error alone.</li> <li>Two forecasts with identical accuracy can have very different real-world impact.</li> </ul> <p>Ignoring this distinction can lead to systematically poor decisions, even when accuracy metrics appear strong.</p>"},{"location":"concepts/asymmetric-cost/#cost-is-contextual-not-universal","title":"Cost is contextual, not universal","text":"<p>Importantly, asymmetric cost is not a universal constant. It depends on context.</p> <p>The relative cost of under-forecasting versus over-forecasting varies by:</p> <ul> <li>Industry and domain</li> <li>Product or resource type</li> <li>Time horizon and planning window</li> <li>Organizational priorities and risk tolerance</li> </ul> <p>There is no single \u201ccorrect\u201d cost ratio that applies across all systems. Any attempt to encode asymmetric cost must therefore be explicit, configurable, and grounded in the operational realities of the decision being made.</p>"},{"location":"concepts/asymmetric-cost/#from-accuracy-to-decision-regret","title":"From accuracy to decision regret","text":"<p>When costs are asymmetric, accuracy alone is an insufficient proxy for decision quality.</p> <p>What matters instead is decision regret: the realized consequence of a decision made under uncertainty, relative to an alternative decision that could have been made with perfect information.</p> <p>This reframing shifts evaluation away from abstract prediction error and toward the outcomes that decisions actually produce. It emphasizes:</p> <ul> <li>The downstream effects of forecast-driven actions</li> <li>The tradeoffs embedded in operational choices</li> <li>The alignment between evaluation criteria and real-world objectives</li> </ul> <p>Asymmetric cost makes this shift unavoidable.</p>"},{"location":"concepts/asymmetric-cost/#why-asymmetric-cost-requires-explicit-policy","title":"Why asymmetric cost requires explicit policy","text":"<p>Because cost asymmetry is contextual and directional, it cannot be handled implicitly or assumed away.</p> <p>Any system that takes asymmetric cost seriously must make explicit choices about:</p> <ul> <li>How different types of errors are weighted</li> <li>Which risks are prioritized over others</li> <li>How tradeoffs are resolved when no option is strictly optimal</li> </ul> <p>These choices are not purely technical. They encode policy, preference, and risk posture. Treating them as hidden implementation details undermines transparency and accountability.</p>"},{"location":"concepts/asymmetric-cost/#how-electric-barometer-approaches-asymmetric-cost","title":"How Electric Barometer approaches asymmetric cost","text":"<p>Electric Barometer treats asymmetric cost as a foundational concept rather than an afterthought.</p> <p>The framework:</p> <ul> <li>Separates forecasting from evaluation and decision policy</li> <li>Makes cost assumptions explicit rather than implicit</li> <li>Supports evaluation measures that distinguish error direction</li> <li>Encourages reproducible, auditable decision criteria</li> </ul> <p>By doing so, Electric Barometer enables organizations to align forecasting systems with the operational realities they are meant to support, rather than optimizing for abstract notions of accuracy that may not reflect true risk.</p> <p>Asymmetric cost is not an edge case\u2014it is the norm in operational decision-making. Electric Barometer is designed accordingly.</p>"},{"location":"concepts/evaluation-vs-decisioning/","title":"Evaluation vs Decisioning","text":"<p>Forecasting systems are often evaluated as if evaluation and decision-making were the same problem. In practice, they are distinct stages with different goals, assumptions, and failure modes. Conflating them leads to misleading conclusions about model quality and suboptimal operational outcomes.</p> <p>Electric Barometer explicitly separates evaluation from decisioning, treating each as a first-class layer in the forecasting lifecycle.</p>"},{"location":"concepts/evaluation-vs-decisioning/#what-evaluation-is-meant-to-do","title":"What evaluation is meant to do","text":"<p>Evaluation answers the question:</p> <p>How does a forecasting system behave relative to a defined objective?</p> <p>In this context, evaluation is concerned with:</p> <ul> <li>Comparing alternative forecasts or models</li> <li>Measuring performance against explicit criteria</li> <li>Understanding tradeoffs and sensitivities</li> <li>Producing reproducible, inspectable results</li> </ul> <p>Evaluation operates in a controlled setting. Its purpose is to surface information about system behavior, not to decide what action to take.</p>"},{"location":"concepts/evaluation-vs-decisioning/#what-decisioning-is-meant-to-do","title":"What decisioning is meant to do","text":"<p>Decisioning answers a different question:</p> <p>Given the available information, what action should be taken?</p> <p>Decisioning involves:</p> <ul> <li>Choosing between competing options</li> <li>Applying policy, preference, or risk tolerance</li> <li>Resolving tradeoffs that cannot be optimized simultaneously</li> <li>Committing to an action that has real-world consequences</li> </ul> <p>Unlike evaluation, decisioning is inherently normative. It reflects priorities, constraints, and organizational objectives that extend beyond technical performance.</p>"},{"location":"concepts/evaluation-vs-decisioning/#why-the-distinction-matters","title":"Why the distinction matters","text":"<p>When evaluation and decisioning are treated as interchangeable, several problems arise:</p> <ul> <li>Evaluation metrics are implicitly treated as decision rules.</li> <li>Hidden assumptions about cost and risk are baked into scores.</li> <li>Decisions appear objective when they are actually policy-driven.</li> <li>It becomes difficult to explain or audit why a particular choice was made.</li> </ul> <p>A model that performs well under an evaluation metric may still be a poor choice once operational constraints or asymmetric risks are considered. Without a clear boundary between evaluation and decisioning, these discrepancies are easy to miss.</p>"},{"location":"concepts/evaluation-vs-decisioning/#metrics-are-not-decisions","title":"Metrics are not decisions","text":"<p>A common failure mode in forecasting systems is allowing a single metric to dictate decisions automatically.</p> <p>Metrics summarize behavior under specific assumptions. They do not encode:</p> <ul> <li>Organizational risk tolerance</li> <li>Context-specific tradeoffs</li> <li>Operational constraints</li> <li>Downstream consequences beyond the metric definition</li> </ul> <p>Treating a metric as a decision rule collapses complex judgment into a scalar value, often without realizing what has been assumed or excluded.</p> <p>Evaluation should inform decisions, not replace them.</p>"},{"location":"concepts/evaluation-vs-decisioning/#decisioning-introduces-policy","title":"Decisioning introduces policy","text":"<p>Decisioning requires explicit policy. Policy defines how evaluation results are interpreted and acted upon.</p> <p>Policy may specify, for example:</p> <ul> <li>How to trade off different types of error</li> <li>When stability is preferred over marginal improvement</li> <li>How ties or near-ties are resolved</li> <li>When conservative versus aggressive choices are favored</li> </ul> <p>These choices are not inherent properties of the data or the model. They reflect intent. Making them explicit allows decisions to be understood, debated, and revised.</p>"},{"location":"concepts/evaluation-vs-decisioning/#the-role-of-uncertainty-and-tradeoffs","title":"The role of uncertainty and tradeoffs","text":"<p>Evaluation often produces ambiguous outcomes:</p> <ul> <li>Multiple models may perform similarly.</li> <li>Improvements in one area may degrade performance elsewhere.</li> <li>Performance may vary across segments or time periods.</li> </ul> <p>Decisioning exists to resolve these ambiguities. It acknowledges that not all objectives can be optimized simultaneously and that some choices require prioritization rather than optimization.</p> <p>Separating evaluation from decisioning prevents false precision and encourages deliberate tradeoff management.</p>"},{"location":"concepts/evaluation-vs-decisioning/#how-electric-barometer-separates-the-two","title":"How Electric Barometer separates the two","text":"<p>Electric Barometer enforces a clear boundary between evaluation and decisioning:</p> <ul> <li>Evaluation produces structured, reproducible measurements.</li> <li>Decisioning applies explicit policies to those measurements.</li> <li>Metrics are treated as inputs, not directives.</li> <li>Selection and tuning are governed by declared rules rather than implicit assumptions.</li> </ul> <p>This separation allows organizations to change decision policy without rewriting evaluation logic, and to improve evaluation rigor without prematurely committing to actions.</p>"},{"location":"concepts/evaluation-vs-decisioning/#why-this-separation-improves-governance","title":"Why this separation improves governance","text":"<p>By disentangling evaluation from decisioning, Electric Barometer improves:</p> <ul> <li>Transparency \u2014 decisions can be traced back to both measurements and policy.</li> <li>Reproducibility \u2014 the same inputs and rules yield the same outcomes.</li> <li>Accountability \u2014 policy choices are visible and reviewable.</li> <li>Adaptability \u2014 policies can evolve as priorities change.</li> </ul> <p>Evaluation tells you what happened. Decisioning determines what you do about it.</p> <p>Electric Barometer treats both as essential, but distinct, components of a responsible forecasting system.</p>"},{"location":"concepts/glossary/","title":"Glossary","text":"<p>This glossary defines core terms used throughout the Electric Barometer ecosystem. The definitions here are conceptual and system-level, rather than implementation-specific. Where formal or mathematical definitions exist, they are provided in the relevant technical notes or papers.</p> <p>The purpose of this glossary is to establish a shared vocabulary and reduce ambiguity across evaluation, decisioning, and governance discussions.</p>"},{"location":"concepts/glossary/#asymmetric-cost","title":"Asymmetric Cost","text":"<p>A property of a decision problem in which different types or directions of error incur different consequences. In forecasting contexts, this often refers to situations where under-forecasting and over-forecasting have unequal operational or economic impact.</p>"},{"location":"concepts/glossary/#decision-regret","title":"Decision Regret","text":"<p>The realized cost or consequence of a decision made under uncertainty, relative to an alternative decision that could have been made with perfect information. Decision regret focuses on outcomes rather than prediction error alone.</p>"},{"location":"concepts/glossary/#decisioning","title":"Decisioning","text":"<p>The process of selecting an action based on available information, evaluation results, and explicit policy. Decisioning is inherently normative and reflects organizational priorities, risk tolerance, and constraints.</p>"},{"location":"concepts/glossary/#evaluation","title":"Evaluation","text":"<p>The process of measuring and comparing the behavior of forecasting systems relative to explicit criteria. Evaluation produces structured, reproducible measurements intended to inform decisioning, not replace it.</p>"},{"location":"concepts/glossary/#forecasting-system","title":"Forecasting System","text":"<p>A combination of data inputs, feature transformations, models, and configurations that produces forecasts. In Electric Barometer, forecasting systems are evaluated and selected as whole systems rather than isolated models.</p>"},{"location":"concepts/glossary/#governance","title":"Governance","text":"<p>The set of rules, policies, and controls that determine how evaluation results are interpreted, how decisions are made, and how those decisions are reviewed, audited, or revised over time.</p>"},{"location":"concepts/glossary/#metric","title":"Metric","text":"<p>A quantitative measure used to summarize some aspect of forecasting system behavior. Metrics provide information for evaluation but do not, by themselves, encode decision policy.</p>"},{"location":"concepts/glossary/#operational-context","title":"Operational Context","text":"<p>The real-world environment in which forecasts are used, including constraints, costs, timing, and downstream consequences. Operational context determines how forecast errors translate into impact.</p>"},{"location":"concepts/glossary/#policy","title":"Policy","text":"<p>An explicit set of rules or preferences that govern how evaluation outputs are translated into decisions. Policies encode tradeoffs, priorities, and risk posture rather than technical performance alone.</p>"},{"location":"concepts/glossary/#reproducibility","title":"Reproducibility","text":"<p>The property that the same inputs, evaluation criteria, and decision policies yield the same results and decisions when applied consistently. Reproducibility is a core requirement for auditability and governance.</p>"},{"location":"concepts/glossary/#selection","title":"Selection","text":"<p>The act of choosing one forecasting system or configuration from a set of evaluated alternatives, based on evaluation outputs and decision policy.</p>"},{"location":"concepts/glossary/#sensitivity","title":"Sensitivity","text":"<p>The degree to which evaluation outcomes or decisions change in response to variations in assumptions, parameters, or inputs. Sensitivity analysis helps reveal tradeoffs and robustness.</p>"},{"location":"concepts/glossary/#tie-breaking","title":"Tie-breaking","text":"<p>A policy-driven mechanism for resolving situations in which multiple forecasting systems perform similarly under evaluation criteria. Tie-breaking rules prevent arbitrary or unstable decisions.</p>"},{"location":"concepts/glossary/#tradeoff","title":"Tradeoff","text":"<p>A situation in which improving performance along one dimension degrades performance along another. Tradeoffs are unavoidable in multi-objective decision problems and must be managed explicitly.</p>"},{"location":"concepts/glossary/#uncertainty","title":"Uncertainty","text":"<p>The inherent lack of certainty in future outcomes, forecasts, or system behavior. Uncertainty motivates the separation of evaluation from decisioning and the use of policy-driven choices.</p>"},{"location":"concepts/glossary/#workflow","title":"Workflow","text":"<p>An ordered sequence of stages through which data, forecasts, evaluations, and decisions flow. In Electric Barometer, workflows are designed to be explicit, reproducible, and inspectable.</p>"},{"location":"concepts/glossary/#readiness","title":"Readiness","text":"<p>A measure of how prepared a forecasting system or decision policy is to support operational action, considering risk, uncertainty, and cost. Readiness emphasizes suitability for use rather than raw performance.</p>"},{"location":"concepts/governance/","title":"Governance","text":"<p>Forecasting systems do not operate in isolation. They inform decisions that affect operations, cost, service levels, and risk. As a result, forecasting systems require governance\u2014not only over models and data, but over how evaluation results are interpreted and how decisions are made.</p> <p>In Electric Barometer, governance refers to the explicit structures, policies, and controls that ensure forecasting-driven decisions are transparent, reproducible, and aligned with organizational intent.</p>"},{"location":"concepts/governance/#why-governance-is-necessary","title":"Why governance is necessary","text":"<p>In many forecasting workflows, decisions emerge implicitly:</p> <ul> <li>Metrics are chosen without documenting why.</li> <li>Models are selected because they score highest on a leaderboard.</li> <li>Tradeoffs are resolved informally or inconsistently.</li> <li>Decisions cannot be easily explained after the fact.</li> </ul> <p>These patterns create fragility. When outcomes are questioned, it becomes difficult to reconstruct what assumptions were made, which alternatives were considered, and why a particular choice prevailed.</p> <p>Governance exists to prevent this ambiguity by making decision logic explicit and reviewable.</p>"},{"location":"concepts/governance/#governance-is-not-bureaucracy","title":"Governance is not bureaucracy","text":"<p>Governance is often misunderstood as an administrative burden. In practice, it serves a different purpose.</p> <p>Effective governance:</p> <ul> <li>Clarifies responsibility and intent</li> <li>Reduces reliance on implicit judgment</li> <li>Improves consistency across time and teams</li> <li>Enables faster iteration by removing ambiguity</li> </ul> <p>Rather than slowing decision-making, well-designed governance accelerates it by establishing clear rules and expectations.</p>"},{"location":"concepts/governance/#the-scope-of-governance-in-forecasting-systems","title":"The scope of governance in forecasting systems","text":"<p>In the context of Electric Barometer, governance applies to several layers:</p> <ul> <li>Evaluation criteria \u2014 what is measured and why</li> <li>Cost assumptions \u2014 how different outcomes are valued</li> <li>Decision policies \u2014 how evaluation results are acted upon</li> <li>Tie-breaking rules \u2014 how ambiguity is resolved</li> <li>Change control \u2014 how updates to metrics or policies are introduced</li> </ul> <p>Governance does not prescribe a single \u201ccorrect\u201d choice at any of these layers. Instead, it ensures that choices are deliberate, documented, and reproducible.</p>"},{"location":"concepts/governance/#explicit-policy-as-a-governance-mechanism","title":"Explicit policy as a governance mechanism","text":"<p>At the heart of governance is policy.</p> <p>Policy defines how evaluation outputs are translated into decisions. It encodes preferences, priorities, and risk posture that cannot be inferred from data alone.</p> <p>Examples of policy decisions include:</p> <ul> <li>Preferring conservative forecasts under high uncertainty</li> <li>Penalizing certain types of error more heavily than others</li> <li>Favoring stability over marginal performance gains</li> <li>Requiring additional evidence before adopting a new model</li> </ul> <p>By treating these as explicit policy rather than implicit behavior, Electric Barometer enables governance without embedding assumptions deep in code or metrics.</p>"},{"location":"concepts/governance/#governance-and-reproducibility","title":"Governance and reproducibility","text":"<p>A governed system must be reproducible.</p> <p>Reproducibility means that:</p> <ul> <li>The same inputs and evaluation criteria yield the same measurements</li> <li>The same policies applied to those measurements yield the same decisions</li> <li>Decisions can be traced back to both data and policy</li> </ul> <p>Without reproducibility, governance collapses into anecdote. Electric Barometer emphasizes reproducible artifacts and structured outputs as a foundation for governed decision-making.</p>"},{"location":"concepts/governance/#auditing-and-explainability","title":"Auditing and explainability","text":"<p>Governance enables explanation after the fact.</p> <p>When decisions are questioned\u2014by operators, stakeholders, or leadership\u2014a governed system can answer:</p> <ul> <li>What alternatives were evaluated?</li> <li>What criteria were applied?</li> <li>What tradeoffs were considered?</li> <li>What policy rules influenced the outcome?</li> </ul> <p>This capability is essential in operational environments where decisions carry financial, reputational, or safety implications.</p>"},{"location":"concepts/governance/#governance-across-time","title":"Governance across time","text":"<p>Forecasting systems evolve. Data changes, priorities shift, and constraints emerge.</p> <p>Governance ensures that:</p> <ul> <li>Changes to metrics or policies are intentional</li> <li>Historical decisions remain interpretable under the rules that existed at the time</li> <li>Policy evolution does not silently invalidate past evaluations</li> </ul> <p>By separating evaluation logic from decision policy, Electric Barometer allows governance to evolve without destabilizing the system.</p>"},{"location":"concepts/governance/#how-electric-barometer-supports-governance","title":"How Electric Barometer supports governance","text":"<p>Electric Barometer is designed to make governance practical rather than aspirational.</p> <p>The framework:</p> <ul> <li>Separates evaluation from decisioning</li> <li>Encourages explicit policy definition</li> <li>Produces structured, inspectable evaluation outputs</li> <li>Supports reproducible selection and tuning workflows</li> <li>Treats governance as a system property, not an external process</li> </ul> <p>Governance is not an optional overlay. It is a core requirement for any forecasting system intended to support real-world operational decisions.</p> <p>Electric Barometer is built with that requirement in mind.</p>"},{"location":"concepts/problem-framing/","title":"Problem Framing in Forecasting Systems","text":"<p>Many failures in forecasting systems do not originate from poor models or insufficient data. They arise earlier, at the level of problem framing. When a forecasting problem is framed incorrectly, even technically strong solutions can lead to misleading evaluations and suboptimal decisions.</p> <p>Electric Barometer treats problem framing as a first-class concern, recognizing that how a problem is defined determines what is measured, optimized, and ultimately acted upon.</p>"},{"location":"concepts/problem-framing/#forecasting-is-not-the-problem","title":"Forecasting is not the problem","text":"<p>In operational environments, forecasting is rarely the end goal. Forecasts exist to support decisions about production, allocation, staffing, inventory, or service.</p> <p>When forecasting is treated as the primary objective:</p> <ul> <li>Accuracy becomes the dominant success criterion.</li> <li>Evaluation focuses on statistical error rather than operational impact.</li> <li>Decisions are implicitly delegated to whatever metric performs best.</li> </ul> <p>This framing obscures the fact that forecasting is an input to a broader decision system, not the decision itself.</p>"},{"location":"concepts/problem-framing/#the-difference-between-prediction-and-decision-problems","title":"The difference between prediction and decision problems","text":"<p>A prediction problem asks:</p> <p>What is the most likely future outcome?</p> <p>A decision problem asks:</p> <p>What action should be taken, given uncertainty about the future?</p> <p>These are related but distinct questions. Prediction focuses on estimating outcomes, while decision-making focuses on choosing actions under uncertainty and constraint.</p> <p>When prediction problems are substituted for decision problems, evaluation criteria drift away from what actually matters operationally.</p>"},{"location":"concepts/problem-framing/#objective-mismatch-and-hidden-assumptions","title":"Objective mismatch and hidden assumptions","text":"<p>Problem framing determines the objective being optimized. If the framing is incomplete or implicit, objectives become misaligned with real-world goals.</p> <p>Common symptoms of objective mismatch include:</p> <ul> <li>Optimizing accuracy when cost or risk is the true concern</li> <li>Treating all errors as equivalent despite asymmetric consequences</li> <li>Ignoring downstream constraints that limit feasible actions</li> <li>Assuming that the best-performing model should always be selected</li> </ul> <p>These mismatches often go unnoticed because they are embedded in the framing rather than the implementation.</p>"},{"location":"concepts/problem-framing/#forecast-granularity-and-decision-relevance","title":"Forecast granularity and decision relevance","text":"<p>The appropriate granularity of a forecast depends on how it will be used.</p> <p>Poor framing can occur when:</p> <ul> <li>Forecasts are generated at a level that does not align with decision cadence</li> <li>Temporal resolution exceeds what decisions can realistically act upon</li> <li>Aggregation hides variability that matters for risk</li> <li>Disaggregation introduces noise without decision value</li> </ul> <p>Framing requires clarity about who makes decisions, when they are made, and what actions they enable.</p>"},{"location":"concepts/problem-framing/#constraints-define-the-real-problem","title":"Constraints define the real problem","text":"<p>Operational decisions are constrained by factors such as:</p> <ul> <li>Capacity limits</li> <li>Lead times</li> <li>Resource availability</li> <li>Regulatory or contractual rules</li> <li>Organizational risk tolerance</li> </ul> <p>A forecasting problem framed without these constraints is incomplete. It may produce accurate predictions that cannot be acted upon or evaluated meaningfully.</p> <p>Electric Barometer encourages framing problems in terms of feasible decisions, not just predicted outcomes.</p>"},{"location":"concepts/problem-framing/#from-optimization-to-tradeoff-management","title":"From optimization to tradeoff management","text":"<p>Many forecasting problems are framed as optimization tasks: minimize error, maximize accuracy, or improve a single score.</p> <p>In reality, operational systems involve tradeoffs:</p> <ul> <li>Cost versus service level</li> <li>Stability versus responsiveness</li> <li>Efficiency versus robustness</li> <li>Short-term gain versus long-term risk</li> </ul> <p>Proper problem framing acknowledges that not all objectives can be optimized simultaneously. The goal shifts from finding a single optimum to managing tradeoffs explicitly.</p>"},{"location":"concepts/problem-framing/#why-problem-framing-precedes-evaluation","title":"Why problem framing precedes evaluation","text":"<p>Evaluation metrics encode assumptions about what matters. If the problem is framed incorrectly, evaluation will faithfully optimize the wrong objective.</p> <p>This creates a false sense of rigor: metrics improve, dashboards look better, but decisions do not.</p> <p>By clarifying the problem framing first\u2014objectives, constraints, tradeoffs, and decision context\u2014evaluation can be designed to support meaningful decisions rather than abstract performance goals.</p>"},{"location":"concepts/problem-framing/#how-electric-barometer-approaches-problem-framing","title":"How Electric Barometer approaches problem framing","text":"<p>Electric Barometer emphasizes problem framing as a prerequisite to evaluation and decisioning.</p> <p>The framework encourages practitioners to:</p> <ul> <li>Define the decision being supported, not just the forecast being produced</li> <li>Identify asymmetric costs and contextual tradeoffs</li> <li>Make constraints and policies explicit</li> <li>Separate measurement from choice</li> <li>Treat forecasting systems as components of a larger decision process</li> </ul> <p>By anchoring forecasting work in correctly framed decision problems, Electric Barometer helps ensure that technical effort translates into operational value.</p> <p>Problem framing is not a preliminary step to be rushed or assumed. It is the foundation upon which effective evaluation, governance, and decision-making are built.</p>"},{"location":"concepts/readiness-and-ral/","title":"Readiness and the Readiness Adjustment Layer (RAL)","text":"<p>Forecasting systems are often evaluated solely on how well they predict outcomes. In operational environments, however, predictive performance alone is not sufficient. A forecast may be accurate on average yet still be unsuitable for decision-making due to risk, instability, or misalignment with operational constraints.</p> <p>Electric Barometer introduces the concept of readiness to address this gap. Readiness focuses on whether a forecasting system is fit for use in a given operational context, not merely whether it produces accurate predictions.</p>"},{"location":"concepts/readiness-and-ral/#what-readiness-means","title":"What readiness means","text":"<p>Readiness refers to the degree to which a forecasting system supports reliable, defensible, and actionable decisions.</p> <p>A ready system is one that:</p> <ul> <li>Aligns with the operational decision it supports</li> <li>Accounts for asymmetric cost and risk</li> <li>Behaves consistently under uncertainty</li> <li>Produces outputs that can be governed and audited</li> <li>Is appropriate for the constraints and cadence of use</li> </ul> <p>Readiness is not an intrinsic property of a model. It emerges from the interaction between forecasts, evaluation criteria, decision policy, and operational context.</p>"},{"location":"concepts/readiness-and-ral/#accuracy-is-not-readiness","title":"Accuracy is not readiness","text":"<p>Accuracy metrics summarize prediction error under specific assumptions. They do not capture whether a forecasting system is suitable for operational use.</p> <p>A system may score well on traditional accuracy measures while still being unready if it:</p> <ul> <li>Produces volatile or unstable outputs</li> <li>Performs inconsistently across segments or time</li> <li>Fails under edge cases or rare conditions</li> <li>Encourages decisions that expose unacceptable risk</li> <li>Assumes cost symmetry where none exists</li> </ul> <p>Readiness addresses these limitations by shifting the focus from error minimization to decision suitability.</p>"},{"location":"concepts/readiness-and-ral/#readiness-as-a-decision-facing-concept","title":"Readiness as a decision-facing concept","text":"<p>Readiness is evaluated relative to a decision, not in isolation.</p> <p>Key questions include:</p> <ul> <li>Can this system be trusted to support action?</li> <li>Does it behave acceptably under uncertainty?</li> <li>Are its tradeoffs aligned with organizational priorities?</li> <li>Can its behavior be explained and defended?</li> </ul> <p>These questions cannot be answered by accuracy metrics alone. They require a broader view of system behavior and its consequences.</p>"},{"location":"concepts/readiness-and-ral/#the-role-of-adjustment-in-readiness","title":"The role of adjustment in readiness","text":"<p>In many cases, forecasting systems are close to being usable but require adjustment to align with operational realities.</p> <p>Examples include:</p> <ul> <li>Penalizing certain types of error more heavily</li> <li>Dampening volatility to improve stability</li> <li>Biasing forecasts conservatively under high uncertainty</li> <li>Adjusting outputs to reflect policy constraints</li> </ul> <p>These adjustments are not attempts to improve predictive accuracy. They are attempts to improve decision fitness.</p>"},{"location":"concepts/readiness-and-ral/#the-readiness-adjustment-layer-ral","title":"The Readiness Adjustment Layer (RAL)","text":"<p>The Readiness Adjustment Layer (RAL) is the conceptual mechanism through which Electric Barometer incorporates readiness into the decision process.</p> <p>RAL represents a layer between raw evaluation outputs and final decisioning. Its purpose is to:</p> <ul> <li>Modify or contextualize evaluation results</li> <li>Incorporate policy-driven adjustments</li> <li>Reflect operational risk and preference</li> <li>Improve alignment between evaluation and action</li> </ul> <p>RAL does not replace evaluation metrics. It operates on top of them, transforming evaluation outputs into decision-ready inputs.</p>"},{"location":"concepts/readiness-and-ral/#ral-is-policy-aware-not-model-specific","title":"RAL is policy-aware, not model-specific","text":"<p>RAL is intentionally separated from model training and forecasting.</p> <p>This separation ensures that:</p> <ul> <li>Forecasting systems remain comparable under a common evaluation framework</li> <li>Adjustments reflect decision policy rather than model internals</li> <li>Readiness logic can evolve without retraining models</li> <li>Governance is applied consistently across alternatives</li> </ul> <p>RAL treats readiness as a decision concern, not a modeling trick.</p>"},{"location":"concepts/readiness-and-ral/#readiness-governance-and-reproducibility","title":"Readiness, governance, and reproducibility","text":"<p>Because readiness adjustments encode policy, they must be explicit and reproducible.</p> <p>A governed readiness process requires:</p> <ul> <li>Clear documentation of adjustment rules</li> <li>Traceability from evaluation outputs to adjusted results</li> <li>The ability to reproduce decisions under the same conditions</li> <li>Visibility into how readiness influenced selection</li> </ul> <p>Electric Barometer treats readiness adjustments as first-class artifacts, subject to the same governance and audit expectations as evaluation results.</p>"},{"location":"concepts/readiness-and-ral/#why-readiness-matters-in-operational-systems","title":"Why readiness matters in operational systems","text":"<p>Operational decisions often prioritize robustness over optimality.</p> <p>A slightly less accurate but stable and well-understood forecasting system may be preferable to a more accurate but volatile or fragile alternative. Readiness captures this distinction.</p> <p>By explicitly accounting for readiness, Electric Barometer helps organizations avoid decisions that look optimal on paper but fail in practice.</p>"},{"location":"concepts/readiness-and-ral/#how-readiness-fits-into-the-electric-barometer-lifecycle","title":"How readiness fits into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Evaluation measures behavior under defined criteria</li> <li>Readiness adjustments contextualize those measurements</li> <li>Decisioning applies policy to select or tune systems</li> <li>Governance ensures the process is transparent and reproducible</li> </ul> <p>Readiness acts as the bridge between measurement and action.</p> <p>The Readiness Adjustment Layer formalizes this bridge, ensuring that forecasting systems are evaluated not only for how well they predict, but for how well they support real-world decisions.</p>"},{"location":"electric-barometer/","title":"Electric Barometer","text":"<p>Electric Barometer is a decision-governance and evaluation framework for cost-aware forecasting, asymmetric risk, and operational readiness.</p> <p>It provides a structured ecosystem of metrics, contracts, evaluation tools, and optimization policies designed to support production-grade decision systems\u2014particularly where over- and under-prediction carry materially different consequences.</p>"},{"location":"electric-barometer/#what-electric-barometer-is","title":"What Electric Barometer is","text":"<p>Electric Barometer is:</p> <ul> <li>A framework, not a single model</li> <li>Cost-aware by design, explicitly encoding asymmetric risk</li> <li>Focused on decision impact, not just forecast accuracy</li> <li>Built to support production workflows, governance, and iteration</li> </ul> <p>The ecosystem is modular and composable, with clear separation between metrics, evaluation, optimization, and integration layers.</p>"},{"location":"electric-barometer/#core-concepts","title":"Core concepts","text":"<p>At a high level, Electric Barometer centers on:</p> <ul> <li> <p>Cost-weighted evaluation   Metrics that reflect real operational asymmetry rather than symmetric error</p> </li> <li> <p>Readiness and plausibility   Signals that capture whether forecasts are operationally actionable</p> </li> <li> <p>Decision calibration   Policies and tuning layers that translate forecasts into decisions</p> </li> <li> <p>Governance and transparency   Explicit contracts, diagnostics, and auditability across the lifecycle</p> </li> </ul>"},{"location":"electric-barometer/#package-ecosystem","title":"Package ecosystem","text":"<p>Electric Barometer is composed of multiple focused packages, each with a narrow responsibility:</p> <ul> <li>Metrics \u2014 cost-weighted and readiness-aware evaluation primitives</li> <li>Contracts \u2014 canonical data structures and shared semantics</li> <li>Evaluation \u2014 diagnostics, validation, and model selection</li> <li>Optimization \u2014 policies, tuning, and search utilities</li> <li>Features \u2014 feature engineering utilities</li> <li>Adapters \u2014 integration layers for external frameworks</li> <li>Integration \u2014 tooling and conventions for composing the system</li> </ul> <p>Each package exposes a minimal API surface and is documented individually under the Packages section.</p>"},{"location":"electric-barometer/#how-to-use-these-docs","title":"How to use these docs","text":"<ul> <li>Use Packages to explore API references for each component</li> <li>Use Concepts to understand the underlying ideas and design choices</li> <li>Use Guides to see how components fit together in practice</li> </ul> <p>This documentation site serves as the authoritative reference for the Electric Barometer ecosystem.</p> <p>If you want, next we can: - refine the Concepts section hierarchy - wire the Packages nav cleanly now that all leaf repos are synced - or draft a short \u201cWhy Electric Barometer?\u201d page for positioning</p>"},{"location":"guides/","title":"Guides","text":"<p>This section provides practical guidance for applying Electric Barometer in real systems.</p> <p>Where Concepts define the theoretical and diagnostic foundations, and Workflows define the canonical execution pipeline, Guides address recurring implementation and interpretation questions that arise when the framework is applied in practice.</p> <p>Guides are intentionally concrete, scoped, and non-exhaustive.</p>"},{"location":"guides/#what-guides-are-and-are-not","title":"What guides are (and are not)","text":"<p>Guides exist to clarify how to use Electric Barometer correctly \u2014 not to extend the framework or prescribe modeling choices.</p> <p>They are intended to:</p> <ul> <li>illustrate correct application patterns,</li> <li>clarify common points of confusion,</li> <li>document defensible conventions,</li> <li>and highlight failure modes and misuses.</li> </ul> <p>They do not:</p> <ul> <li>introduce new metrics or diagnostics,</li> <li>redefine framework semantics,</li> <li>prescribe forecasting models or features,</li> <li>or replace the formal papers.</li> </ul> <p>When a concept requires formal definition, it belongs in Papers, not here.</p>"},{"location":"guides/#when-to-use-the-guides","title":"When to use the guides","text":"<p>You should consult this section when you are asking questions such as:</p> <ul> <li>How should I think about tolerance selection in practice?</li> <li>What does it mean for a forecast to be \u201cready enough\u201d?</li> <li>How do I interpret readiness metrics under discrete demand?</li> <li>When is readiness adjustment appropriate \u2014 and when is it not?</li> <li>How should governance outcomes be communicated downstream?</li> </ul> <p>If you are asking what the framework is, start with Concepts. If you are asking how the system runs, start with Workflows.</p>"},{"location":"guides/#guide-categories","title":"Guide categories","text":"<p>The guides in this section are organized around common application themes:</p> <ul> <li> <p>Interpretation guides   How to interpret readiness diagnostics, tolerance behavior, and cost asymmetry in operational   contexts.</p> </li> <li> <p>Application guides   How to apply Electric Barometer to specific problem classes (e.g., production, staffing,   replenishment) without hard-coding domain assumptions into the framework.</p> </li> <li> <p>Governance and policy guides   How to reason about governance outcomes, decision closure, and policy communication.</p> </li> <li> <p>Anti-patterns and pitfalls   Common misuses of readiness metrics, adjustment layers, and evaluation outputs.</p> </li> </ul> <p>Each guide is written to stand alone and may be read independently.</p>"},{"location":"guides/#scope-and-non-goals","title":"Scope and non-goals","text":"<p>Guides deliberately avoid:</p> <ul> <li>step-by-step tutorials,</li> <li>code walkthroughs,</li> <li>platform-specific instructions,</li> <li>or performance optimization advice.</li> </ul> <p>Those concerns are orthogonal to Electric Barometer\u2019s role as a readiness and governance framework.</p>"},{"location":"guides/#relationship-to-the-rest-of-the-documentation","title":"Relationship to the rest of the documentation","text":"<p>The documentation layers are intentionally hierarchical:</p> <ol> <li>Papers define formal constructs and guarantees.</li> <li>Concepts summarize and contextualize those constructs.</li> <li>Workflows specify canonical execution order and artifacts.</li> <li>Guides support correct and defensible use in practice.</li> </ol> <p>Guides must remain consistent with all layers above them.</p>"},{"location":"guides/#reading-order-recommended","title":"Reading order (recommended)","text":"<p>If you are new to Electric Barometer:</p> <ol> <li>Start with Concepts</li> <li>Review Workflows</li> <li>Use Guides as reference material during application</li> </ol>"},{"location":"guides/#final-note","title":"Final note","text":"<p>Guides are living documents.</p> <p>They may expand over time to address recurring questions or observed failure modes, but they will not redefine the framework. Any change in formal meaning belongs upstream.</p> <p>This boundary is intentional.</p>"},{"location":"guides/add-a-feature-transform/","title":"Add a Feature Transform","text":"<p>Feature transforms convert raw data into representations that forecasting systems can use effectively. In Electric Barometer, feature transforms are treated as first-class, auditable components of a forecasting system rather than ad hoc preprocessing steps.</p> <p>This guide describes the process for adding a new feature transform in a way that is consistent, reproducible, and compatible with the Electric Barometer ecosystem.</p>"},{"location":"guides/add-a-feature-transform/#when-you-should-add-a-feature-transform","title":"When you should add a feature transform","text":"<p>Add a new feature transform when:</p> <ul> <li>Raw inputs do not express the signal needed for forecasting</li> <li>Domain structure must be encoded explicitly (e.g., seasonality, lags, exposure)</li> <li>Operational constraints require derived features</li> <li>Reuse across multiple forecasting systems is expected</li> <li>Feature logic should be governed and reviewed independently</li> </ul> <p>If the logic is highly model-specific or experimental, it may not belong as a shared transform.</p>"},{"location":"guides/add-a-feature-transform/#conceptual-requirements","title":"Conceptual requirements","text":"<p>Before writing any code, clarify the following:</p> <ul> <li>Purpose \u2014 What signal does the transform introduce?</li> <li>Scope \u2014 Is it general-purpose or domain-specific?</li> <li>Granularity \u2014 At what entity and temporal level does it operate?</li> <li>Dependencies \u2014 What inputs does it require?</li> <li>Stability \u2014 Should it evolve frequently or remain fixed?</li> </ul> <p>These decisions determine whether the transform belongs in a shared features library or as local preprocessing logic.</p>"},{"location":"guides/add-a-feature-transform/#where-feature-transforms-live","title":"Where feature transforms live","text":"<p>Feature transforms are implemented and documented in the features repository, which is the source of truth for:</p> <ul> <li>transform definitions</li> <li>expected inputs and outputs</li> <li>validation logic</li> <li>usage examples</li> </ul> <p>The Electric Barometer documentation site references and routes to these materials but does not duplicate them.</p>"},{"location":"guides/add-a-feature-transform/#implementation-overview","title":"Implementation overview","text":"<p>At a high level, adding a feature transform involves:</p> <ol> <li>Defining the transform interface</li> <li>Implementing the transformation logic</li> <li>Documenting behavior and assumptions</li> <li>Validating correctness and stability</li> <li>Making the transform discoverable and reusable</li> </ol> <p>The exact mechanics depend on the features library, but the process is consistent.</p>"},{"location":"guides/add-a-feature-transform/#step-1-define-the-transform-contract","title":"Step 1: Define the transform contract","text":"<p>A feature transform should have a clear contract:</p> <ul> <li>What inputs it expects</li> <li>What outputs it produces</li> <li>Any assumptions about ordering, completeness, or availability</li> <li>How missing or edge cases are handled</li> </ul> <p>The contract should be explicit enough that downstream users do not need to inspect the implementation to understand behavior.</p>"},{"location":"guides/add-a-feature-transform/#step-2-implement-the-transform","title":"Step 2: Implement the transform","text":"<p>Implement the transform according to the conventions of the features repository.</p> <p>Guidelines:</p> <ul> <li>Keep logic focused and composable</li> <li>Avoid embedding model-specific assumptions</li> <li>Prefer clarity over cleverness</li> <li>Make cost, lag, or windowing choices explicit</li> </ul> <p>Transforms should be deterministic and side-effect free.</p>"},{"location":"guides/add-a-feature-transform/#step-3-document-intent-and-usage","title":"Step 3: Document intent and usage","text":"<p>Every feature transform must be documented alongside its implementation.</p> <p>Documentation should explain:</p> <ul> <li>What the transform represents conceptually</li> <li>When it should be used</li> <li>When it should not be used</li> <li>Any known limitations or tradeoffs</li> </ul> <p>This documentation lives in the features repository and is surfaced in the ecosystem docs through the Packages section.</p>"},{"location":"guides/add-a-feature-transform/#step-4-validate-correctness-and-stability","title":"Step 4: Validate correctness and stability","text":"<p>Validation ensures that the transform behaves as intended across expected scenarios.</p> <p>Validation may include:</p> <ul> <li>Unit tests over representative inputs</li> <li>Edge-case handling (missing data, boundaries)</li> <li>Consistency checks across entities or time</li> <li>Performance considerations for large panels</li> </ul> <p>A transform that cannot be validated reproducibly should not be shared.</p>"},{"location":"guides/add-a-feature-transform/#step-5-make-the-transform-discoverable","title":"Step 5: Make the transform discoverable","text":"<p>Once implemented and validated:</p> <ul> <li>Ensure it is included in the features package documentation</li> <li>Add or update usage examples if appropriate</li> <li>Reference it from higher-level guides or workflows if it represents a common pattern</li> </ul> <p>Discoverability reduces duplication and encourages consistent feature usage across forecasting systems.</p>"},{"location":"guides/add-a-feature-transform/#governance-considerations","title":"Governance considerations","text":"<p>Feature transforms influence evaluation outcomes and downstream decisions. As such, they are subject to governance.</p> <p>Good governance practices include:</p> <ul> <li>Versioning changes to transform behavior</li> <li>Avoiding silent modifications to existing transforms</li> <li>Documenting rationale for changes</li> <li>Preserving backward compatibility where possible</li> </ul> <p>Transforms should evolve deliberately, not implicitly.</p>"},{"location":"guides/add-a-feature-transform/#how-feature-transforms-fit-into-the-electric-barometer-lifecycle","title":"How feature transforms fit into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Feature transforms shape the inputs to forecasting systems</li> <li>Evaluation measures the behavior of systems built on those features</li> <li>Decisioning and readiness depend on the stability and interpretability of features</li> <li>Governance ensures feature logic remains transparent and reproducible</li> </ul> <p>Treating feature transforms as governed components improves trust in the entire forecasting pipeline.</p>"},{"location":"guides/add-a-feature-transform/#where-to-go-next","title":"Where to go next","text":"<ul> <li>See Packages \u2192 Features for available transforms and implementation details</li> <li>Follow Workflows to understand how features are used end-to-end</li> <li>Review Governance to understand how changes to feature logic are managed</li> </ul> <p>Adding a feature transform is not just a coding task. It is a design decision that affects evaluation, decisioning, and operational outcomes across the Electric Barometer ecosystem.</p>"},{"location":"guides/add-a-metric/","title":"Add a Metric","text":"<p>Metrics define how forecasting systems are evaluated. In Electric Barometer, metrics are not treated as generic accuracy scores, but as decision-facing measurement tools designed to surface behavior, tradeoffs, and risk under explicit assumptions.</p> <p>This guide describes the process for adding a new metric in a way that is conceptually sound, reproducible, and compatible with the Electric Barometer ecosystem.</p>"},{"location":"guides/add-a-metric/#when-you-should-add-a-metric","title":"When you should add a metric","text":"<p>Add a new metric when:</p> <ul> <li>Existing metrics do not capture an important aspect of system behavior</li> <li>A specific tradeoff or risk dimension needs to be measured explicitly</li> <li>Operational cost or consequence cannot be expressed through current measures</li> <li>Evaluation requirements vary meaningfully by context or use case</li> <li>You need to formalize a concept that is currently implicit or ad hoc</li> </ul> <p>If a metric only marginally improves an existing measure or duplicates behavior under a different name, it may not belong as a first-class metric.</p>"},{"location":"guides/add-a-metric/#conceptual-requirements","title":"Conceptual requirements","text":"<p>Before implementing a new metric, clarify the following:</p> <ul> <li>Purpose \u2014 What behavior does the metric measure?</li> <li>Decision relevance \u2014 What decisions does it inform?</li> <li>Assumptions \u2014 What cost, symmetry, or context assumptions does it encode?</li> <li>Scope \u2014 Is it general-purpose or domain-specific?</li> <li>Comparability \u2014 Can it be meaningfully compared across systems or time?</li> </ul> <p>Metrics should answer a clear question. If that question cannot be stated plainly, the metric is likely underspecified.</p>"},{"location":"guides/add-a-metric/#metrics-are-measurements-not-decisions","title":"Metrics are measurements, not decisions","text":"<p>In Electric Barometer, metrics serve evaluation\u2014not decisioning.</p> <p>A metric should:</p> <ul> <li>Measure behavior under explicit assumptions</li> <li>Produce reproducible, inspectable outputs</li> <li>Support comparison across alternatives</li> </ul> <p>A metric should not:</p> <ul> <li>Encode implicit policy choices</li> <li>Automatically select winners</li> <li>Obscure tradeoffs behind a single score</li> </ul> <p>Decisioning logic belongs in policy and optimization layers, not inside metrics.</p>"},{"location":"guides/add-a-metric/#where-metrics-live","title":"Where metrics live","text":"<p>Metrics are implemented and documented in the metrics repository, which is the source of truth for:</p> <ul> <li>metric definitions</li> <li>assumptions and parameters</li> <li>expected inputs and outputs</li> <li>implementation details</li> </ul> <p>The Electric Barometer documentation site explains why and when metrics are used, while the metrics repository documents how they are implemented.</p>"},{"location":"guides/add-a-metric/#step-1-define-the-metric-formally","title":"Step 1: Define the metric formally","text":"<p>Every metric requires a clear definition.</p> <p>A complete definition specifies:</p> <ul> <li>What is being measured</li> <li>What inputs are required</li> <li>How outputs are computed</li> <li>What assumptions are encoded</li> <li>What higher or lower values represent</li> </ul> <p>Formal definitions may be mathematical, procedural, or both, but they must be unambiguous.</p>"},{"location":"guides/add-a-metric/#step-2-identify-encoded-assumptions","title":"Step 2: Identify encoded assumptions","text":"<p>Metrics always encode assumptions, whether explicit or implicit.</p> <p>Examples include:</p> <ul> <li>Symmetric versus asymmetric treatment of error</li> <li>Relative weighting of different outcomes</li> <li>Aggregation across entities or time</li> <li>Sensitivity to scale or variance</li> </ul> <p>Identifying these assumptions early prevents misuse and misinterpretation later.</p>"},{"location":"guides/add-a-metric/#step-3-implement-the-metric","title":"Step 3: Implement the metric","text":"<p>Implement the metric according to the conventions of the metrics repository.</p> <p>Guidelines:</p> <ul> <li>Keep logic transparent and readable</li> <li>Avoid embedding policy decisions</li> <li>Make parameters explicit and configurable</li> <li>Ensure deterministic behavior</li> </ul> <p>Metrics should be pure functions of their inputs wherever possible.</p>"},{"location":"guides/add-a-metric/#step-4-document-interpretation","title":"Step 4: Document interpretation","text":"<p>Metric documentation must explain how to interpret results.</p> <p>This includes:</p> <ul> <li>What the metric is sensitive to</li> <li>What it is insensitive to</li> <li>Common failure modes or edge cases</li> <li>How it should be compared across systems</li> </ul> <p>Interpretation guidance is as important as the computation itself.</p>"},{"location":"guides/add-a-metric/#step-5-validate-behavior","title":"Step 5: Validate behavior","text":"<p>Validation ensures that the metric behaves as intended.</p> <p>Validation may include:</p> <ul> <li>Tests on synthetic or controlled data</li> <li>Sensitivity checks across parameter values</li> <li>Comparison against known baselines</li> <li>Verification under edge conditions</li> </ul> <p>A metric that cannot be validated should not be relied upon for evaluation.</p>"},{"location":"guides/add-a-metric/#step-6-make-the-metric-discoverable","title":"Step 6: Make the metric discoverable","text":"<p>Once implemented and validated:</p> <ul> <li>Ensure it is included in the metrics documentation</li> <li>Add examples where appropriate</li> <li>Reference it from conceptual or workflow guides if it represents a common pattern</li> </ul> <p>Discoverability encourages consistent usage and reduces the proliferation of redundant measures.</p>"},{"location":"guides/add-a-metric/#governance-considerations","title":"Governance considerations","text":"<p>Metrics influence decisions indirectly by shaping evaluation outcomes. As such, they require governance.</p> <p>Good governance practices include:</p> <ul> <li>Versioning changes to metric definitions</li> <li>Avoiding silent behavior changes</li> <li>Documenting rationale for new metrics or modifications</li> <li>Preserving comparability over time where possible</li> </ul> <p>Metrics should evolve deliberately, not opportunistically.</p>"},{"location":"guides/add-a-metric/#how-metrics-fit-into-the-electric-barometer-lifecycle","title":"How metrics fit into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Metrics provide structured measurements of forecasting system behavior</li> <li>Evaluation aggregates and compares those measurements</li> <li>Readiness and policy layers contextualize metric outputs</li> <li>Decisioning applies explicit rules to select or tune systems</li> <li>Governance ensures the process remains transparent and reproducible</li> </ul> <p>Metrics inform decisions, but they do not make them.</p>"},{"location":"guides/add-a-metric/#where-to-go-next","title":"Where to go next","text":"<ul> <li>See Packages \u2192 Metrics for available metrics and implementation details</li> <li>Read Concepts \u2192 Asymmetric Cost to understand why many metrics are directional</li> <li>Follow Workflows to see how metrics are used in evaluation and selection</li> </ul> <p>Adding a metric is not just a technical task. It is a statement about what behavior matters and why. In Electric Barometer, that statement is made explicitly.</p>"},{"location":"guides/add-an-adapter/","title":"Add an Adapter","text":"<p>Adapters connect Electric Barometer to external systems, data sources, or execution environments. They translate between ecosystem-neutral interfaces and context-specific inputs or outputs without embedding domain assumptions into core logic.</p> <p>This guide describes how to add a new adapter in a way that preserves modularity, reproducibility, and governance across the Electric Barometer ecosystem.</p>"},{"location":"guides/add-an-adapter/#when-you-should-add-an-adapter","title":"When you should add an adapter","text":"<p>Add an adapter when:</p> <ul> <li>Integrating a new data source, storage system, or execution platform</li> <li>Connecting Electric Barometer to an external forecasting engine or service</li> <li>Translating between external schemas and internal contracts</li> <li>Supporting environment-specific I/O (filesystems, databases, APIs)</li> <li>Isolating operational concerns from evaluation and decision logic</li> </ul> <p>If logic alters evaluation behavior or encodes policy, it likely does not belong in an adapter.</p>"},{"location":"guides/add-an-adapter/#what-adapters-are-responsible-for","title":"What adapters are responsible for","text":"<p>Adapters serve as translation layers. Their responsibility is to move information across boundaries, not to interpret it.</p> <p>Adapters may handle:</p> <ul> <li>Data ingestion and extraction</li> <li>Schema translation and validation</li> <li>Serialization and deserialization</li> <li>Environment-specific configuration</li> <li>Integration with external services</li> </ul> <p>Adapters should not define metrics, policies, or decision rules.</p>"},{"location":"guides/add-an-adapter/#what-adapters-should-not-do","title":"What adapters should not do","text":"<p>Adapters should avoid:</p> <ul> <li>Embedding domain or business logic</li> <li>Modifying evaluation assumptions</li> <li>Applying cost weights or policy preferences</li> <li>Performing feature engineering beyond basic translation</li> <li>Making decisions based on evaluation outputs</li> </ul> <p>Keeping adapters thin preserves clarity and prevents coupling.</p>"},{"location":"guides/add-an-adapter/#where-adapters-live","title":"Where adapters live","text":"<p>Adapters are implemented and documented in the adapters repository, which is the source of truth for:</p> <ul> <li>adapter interfaces</li> <li>supported systems and environments</li> <li>configuration requirements</li> <li>usage examples</li> </ul> <p>The Electric Barometer documentation site references adapters conceptually and routes users to the appropriate repository documentation.</p>"},{"location":"guides/add-an-adapter/#step-1-define-the-integration-boundary","title":"Step 1: Define the integration boundary","text":"<p>Before writing code, define the boundary the adapter will bridge.</p> <p>Clarify:</p> <ul> <li>What external system is being integrated</li> <li>What data enters or leaves Electric Barometer</li> <li>What contracts or schemas apply on each side</li> <li>Where responsibility for validation lies</li> </ul> <p>A clear boundary prevents leakage of external concerns into core logic.</p>"},{"location":"guides/add-an-adapter/#step-2-define-adapter-inputs-and-outputs","title":"Step 2: Define adapter inputs and outputs","text":"<p>Every adapter should have a well-defined interface.</p> <p>This includes:</p> <ul> <li>Input formats and required fields</li> <li>Output formats and guarantees</li> <li>Error handling behavior</li> <li>Assumptions about ordering, completeness, or latency</li> </ul> <p>Interfaces should be explicit enough to support testing and reuse without inspecting implementation details.</p>"},{"location":"guides/add-an-adapter/#step-3-implement-translation-logic","title":"Step 3: Implement translation logic","text":"<p>Implement the adapter according to the conventions of the adapters repository.</p> <p>Guidelines:</p> <ul> <li>Keep logic focused on translation</li> <li>Prefer explicit mappings over implicit inference</li> <li>Avoid side effects where possible</li> <li>Fail fast on invalid or unsupported inputs</li> </ul> <p>Adapters should behave deterministically given the same inputs and configuration.</p>"},{"location":"guides/add-an-adapter/#step-4-document-usage-and-limitations","title":"Step 4: Document usage and limitations","text":"<p>Every adapter must be documented alongside its implementation.</p> <p>Documentation should explain:</p> <ul> <li>What systems or environments it supports</li> <li>How it should be configured</li> <li>Known limitations or constraints</li> <li>Expected performance characteristics</li> </ul> <p>Clear documentation prevents misuse and reduces the need for ad hoc fixes.</p>"},{"location":"guides/add-an-adapter/#step-5-validate-integration-behavior","title":"Step 5: Validate integration behavior","text":"<p>Validation ensures that the adapter behaves correctly under expected conditions.</p> <p>Validation may include:</p> <ul> <li>Integration tests against representative external systems</li> <li>Schema validation tests</li> <li>Error-path testing</li> <li>Round-trip consistency checks</li> </ul> <p>Adapters that cannot be validated reliably should not be used in production workflows.</p>"},{"location":"guides/add-an-adapter/#governance-considerations","title":"Governance considerations","text":"<p>Adapters influence how data and results enter and exit the system. As such, they are subject to governance.</p> <p>Good governance practices include:</p> <ul> <li>Versioning adapter behavior and interfaces</li> <li>Avoiding silent schema changes</li> <li>Documenting external dependencies</li> <li>Preserving backward compatibility where feasible</li> </ul> <p>Adapters should evolve deliberately as integration requirements change.</p>"},{"location":"guides/add-an-adapter/#how-adapters-fit-into-the-electric-barometer-lifecycle","title":"How adapters fit into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Adapters connect external systems to internal workflows</li> <li>Feature transforms and metrics operate on adapter-provided data</li> <li>Evaluation and decisioning remain ecosystem-neutral</li> <li>Governance ensures integrations remain transparent and reproducible</li> </ul> <p>Adapters enable flexibility without sacrificing clarity or control.</p>"},{"location":"guides/add-an-adapter/#where-to-go-next","title":"Where to go next","text":"<ul> <li>See Packages \u2192 Adapters for available adapters and implementation details</li> <li>Follow Workflows to understand how adapters participate in end-to-end runs</li> <li>Review Governance to understand how integration changes are managed</li> </ul> <p>Adding an adapter is an architectural decision. By isolating integration concerns, Electric Barometer maintains a clean separation between external systems and internal decision logic.</p>"},{"location":"guides/release-process/","title":"Release Process","text":"<p>Releases define how changes become official, discoverable, and reproducible across the Electric Barometer ecosystem. A clear release process ensures that code, documentation, and research artifacts remain aligned while preserving traceability and governance.</p> <p>This guide describes the release philosophy and high-level process used across Electric Barometer repositories.</p>"},{"location":"guides/release-process/#why-releases-matter","title":"Why releases matter","text":"<p>In a multi-repository ecosystem, releases are more than version numbers. They serve as:</p> <ul> <li>Stable reference points for evaluation and decisioning</li> <li>Anchors for documentation and research artifacts</li> <li>Mechanisms for reproducibility and auditability</li> <li>Signals of maturity and intent</li> </ul> <p>Without a disciplined release process, changes become difficult to track, explain, or reproduce.</p>"},{"location":"guides/release-process/#release-principles","title":"Release principles","text":"<p>Electric Barometer releases follow a small set of guiding principles:</p> <ul> <li>Explicit versioning \u2014 changes are introduced intentionally and visibly</li> <li>Reproducibility \u2014 released artifacts can be rebuilt or referenced consistently</li> <li>Traceability \u2014 decisions can be linked to the versions in use at the time</li> <li>Minimal coupling \u2014 repositories are versioned independently</li> <li>Documentation alignment \u2014 releases correspond to documented behavior</li> </ul> <p>These principles apply across code, documentation, and papers.</p>"},{"location":"guides/release-process/#what-constitutes-a-release","title":"What constitutes a release","text":"<p>A release represents a coherent set of changes that are intended for use.</p> <p>Depending on the repository, a release may include:</p> <ul> <li>Code changes and new functionality</li> <li>Updated documentation or guides</li> <li>Schema or contract updates</li> <li>Bug fixes or performance improvements</li> <li>Formal research artifacts (e.g., compiled papers)</li> </ul> <p>Not all commits warrant a release. Releases mark meaningful checkpoints.</p>"},{"location":"guides/release-process/#versioning-strategy","title":"Versioning strategy","text":"<p>Electric Barometer repositories use semantic-style versioning to communicate change intent.</p> <p>At a high level:</p> <ul> <li>Major versions introduce breaking changes or conceptual shifts</li> <li>Minor versions add functionality in a backward-compatible way</li> <li>Patch versions fix bugs or clarify behavior without altering interfaces</li> </ul> <p>Version numbers are part of the governance signal and should be chosen deliberately.</p>"},{"location":"guides/release-process/#releasing-code-repositories","title":"Releasing code repositories","text":"<p>For code repositories, the release process typically involves:</p> <ol> <li>Ensuring the main branch reflects the intended release state</li> <li>Running tests and validation checks</li> <li>Updating documentation where necessary</li> <li>Creating a version tag</li> <li>Publishing the release through the hosting platform</li> </ol> <p>The release tag serves as the canonical reference point for consumers and downstream systems.</p>"},{"location":"guides/release-process/#releasing-documentation","title":"Releasing documentation","text":"<p>Documentation changes may be released independently of code, but should remain consistent with released behavior.</p> <p>Best practices include:</p> <ul> <li>Updating documentation before or alongside a code release</li> <li>Avoiding references to unreleased features</li> <li>Using versioned links where appropriate</li> <li>Treating documentation as a first-class artifact</li> </ul> <p>Documentation releases reinforce trust in the ecosystem.</p>"},{"location":"guides/release-process/#releasing-research-artifacts","title":"Releasing research artifacts","text":"<p>Technical notes and papers follow a similar philosophy but differ in format.</p> <p>Releases for research artifacts typically include:</p> <ul> <li>Compiled documents (e.g., PDFs)</li> <li>Versioned release tags</li> <li>Immutable artifacts attached to the release</li> </ul> <p>This approach ensures that formal definitions and analyses remain stable and citable.</p>"},{"location":"guides/release-process/#coordinating-releases-across-repositories","title":"Coordinating releases across repositories","text":"<p>Electric Barometer repositories are intentionally decoupled. Not all releases need to be synchronized.</p> <p>Coordination is recommended when:</p> <ul> <li>Changes affect shared contracts or schemas</li> <li>A new concept or metric spans multiple repositories</li> <li>Documentation relies on behavior introduced in multiple places</li> </ul> <p>In these cases, releases should reference one another explicitly to preserve clarity.</p>"},{"location":"guides/release-process/#governance-and-change-control","title":"Governance and change control","text":"<p>Releases are a governance mechanism.</p> <p>Good governance practices include:</p> <ul> <li>Documenting the intent and scope of each release</li> <li>Avoiding silent or undocumented changes</li> <li>Preserving access to historical versions</li> <li>Clearly communicating breaking changes</li> </ul> <p>A release should make it easier\u2014not harder\u2014to explain what changed and why.</p>"},{"location":"guides/release-process/#how-releases-fit-into-the-electric-barometer-lifecycle","title":"How releases fit into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Releases define the versions used in evaluation and decision workflows</li> <li>Documentation references released behavior</li> <li>Research artifacts formalize released concepts</li> <li>Governance relies on versioned artifacts for traceability</li> </ul> <p>Releases provide the temporal structure that allows the ecosystem to evolve without losing coherence.</p>"},{"location":"guides/release-process/#where-to-go-next","title":"Where to go next","text":"<ul> <li>See Packages for repository-specific release details</li> <li>Review Governance to understand how releases support auditability</li> <li>Consult Papers for formally released definitions and analyses</li> </ul> <p>A disciplined release process is essential to maintaining trust, reproducibility, and clarity across the Electric Barometer ecosystem.</p>"},{"location":"guides/run-an-evaluation/","title":"Run an Evaluation","text":"<p>Evaluation is the process of measuring and comparing the behavior of forecasting systems under explicit criteria. In Electric Barometer, evaluation is treated as a distinct and reproducible stage that informs decisioning without making decisions itself.</p> <p>This guide describes how to run an evaluation in a way that is consistent, interpretable, and compatible with the Electric Barometer ecosystem.</p>"},{"location":"guides/run-an-evaluation/#what-an-evaluation-is-and-is-not","title":"What an evaluation is (and is not)","text":"<p>An evaluation answers the question:</p> <p>How do alternative forecasting systems behave under defined measurement criteria?</p> <p>An evaluation does not answer:</p> <ul> <li>Which system should be chosen</li> <li>What action should be taken</li> <li>What policy is preferred</li> </ul> <p>Those questions are resolved later through Decisioning and governance.</p> <p>Keeping this boundary clear is essential for transparency and reproducibility.</p>"},{"location":"guides/run-an-evaluation/#when-you-should-run-an-evaluation","title":"When you should run an evaluation","text":"<p>Run an evaluation when:</p> <ul> <li>Comparing multiple forecasting systems or configurations</li> <li>Assessing the impact of new features or transforms</li> <li>Introducing or validating new metrics</li> <li>Investigating tradeoffs or sensitivity</li> <li>Preparing inputs for selection or policy tuning</li> </ul> <p>Evaluations should be repeatable and inspectable, not one-off experiments.</p>"},{"location":"guides/run-an-evaluation/#prerequisites","title":"Prerequisites","text":"<p>Before running an evaluation, ensure that:</p> <ul> <li>Forecasting systems are clearly defined</li> <li>Inputs and data slices are fixed and documented</li> <li>Metrics and parameters are explicitly selected</li> <li>The evaluation window and granularity are appropriate</li> <li>Assumptions about cost and context are understood</li> </ul> <p>For conceptual grounding, review Problem Framing and Asymmetric Cost before proceeding.</p>"},{"location":"guides/run-an-evaluation/#step-1-define-the-evaluation-scope","title":"Step 1: Define the evaluation scope","text":"<p>Begin by defining what is being evaluated.</p> <p>This includes:</p> <ul> <li>Which forecasting systems or variants are included</li> <li>Which entities, segments, or time periods are in scope</li> <li>The temporal resolution of evaluation</li> <li>Any exclusions or filters applied</li> </ul> <p>The scope should align with the decision context the evaluation is meant to support, as described in Concepts.</p>"},{"location":"guides/run-an-evaluation/#step-2-select-evaluation-metrics","title":"Step 2: Select evaluation metrics","text":"<p>Choose metrics that reflect the behavior you want to measure.</p> <p>Consider:</p> <ul> <li>Whether cost asymmetry is relevant</li> <li>Which tradeoffs are important to surface</li> <li>How metrics aggregate across entities or time</li> <li>Whether multiple metrics are required</li> </ul> <p>Metrics should be selected intentionally. If a new measure is required, see Add a Metric.</p>"},{"location":"guides/run-an-evaluation/#step-3-configure-metric-parameters","title":"Step 3: Configure metric parameters","text":"<p>Many metrics require parameters that encode assumptions.</p> <p>Examples include:</p> <ul> <li>Relative weighting of different error types</li> <li>Thresholds or tolerances</li> <li>Normalization or scaling choices</li> </ul> <p>Parameter choices should be explicit and recorded as part of the evaluation configuration. These assumptions often reflect policy and should be consistent with Governance principles.</p>"},{"location":"guides/run-an-evaluation/#step-4-execute-the-evaluation","title":"Step 4: Execute the evaluation","text":"<p>Run the evaluation using the configured inputs and metrics.</p> <p>Execution should:</p> <ul> <li>Be deterministic given the same inputs</li> <li>Produce structured outputs</li> <li>Capture metadata describing the run</li> <li>Avoid modifying source data or forecasts</li> </ul> <p>Evaluation execution is a measurement process, not an optimization step.</p>"},{"location":"guides/run-an-evaluation/#step-5-inspect-evaluation-outputs","title":"Step 5: Inspect evaluation outputs","text":"<p>After execution, review the evaluation results.</p> <p>Inspection may involve:</p> <ul> <li>Comparing metrics across systems</li> <li>Examining distributions or segment-level behavior</li> <li>Identifying tradeoffs or sensitivities</li> <li>Checking for anomalies or unexpected patterns</li> </ul> <p>At this stage, evaluation outputs should be interpreted\u2014but not acted upon\u2014until readiness and policy considerations are applied.</p>"},{"location":"guides/run-an-evaluation/#step-6-preserve-evaluation-artifacts","title":"Step 6: Preserve evaluation artifacts","text":"<p>Evaluation artifacts are essential for governance and reproducibility.</p> <p>Artifacts may include:</p> <ul> <li>Metric outputs and summaries</li> <li>Configuration files or parameters</li> <li>Input identifiers or hashes</li> <li>Execution metadata</li> </ul> <p>Preserving these artifacts enables auditability and supports later review under Governance.</p>"},{"location":"guides/run-an-evaluation/#what-comes-after-evaluation","title":"What comes after evaluation","text":"<p>Evaluation produces information, not decisions.</p> <p>Typical next steps include:</p> <ul> <li>Applying readiness adjustments   (see Readiness and RAL)</li> <li>Comparing systems under explicit policy rules</li> <li>Performing sensitivity analysis</li> <li>Selecting or tuning systems</li> </ul> <p>These steps belong to decisioning and optimization, not evaluation itself.</p>"},{"location":"guides/run-an-evaluation/#governance-considerations","title":"Governance considerations","text":"<p>Evaluations influence downstream decisions and must therefore be governed.</p> <p>Good governance practices include:</p> <ul> <li>Documenting evaluation intent and scope</li> <li>Avoiding ad hoc metric selection</li> <li>Preserving historical evaluation results</li> <li>Ensuring consistency across comparable runs</li> </ul> <p>For a full discussion, see Governance.</p>"},{"location":"guides/run-an-evaluation/#how-evaluation-fits-into-the-electric-barometer-lifecycle","title":"How evaluation fits into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Feature transforms define system inputs   (see Add a Feature Transform)</li> <li>Forecasting systems produce candidate outputs</li> <li>Evaluation measures behavior under explicit criteria</li> <li>Readiness and policy layers contextualize results</li> <li>Decisioning selects or tunes systems</li> <li>Governance ensures traceability and accountability</li> </ul> <p>Evaluation is the measurement backbone of this lifecycle.</p>"},{"location":"guides/run-an-evaluation/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Add or refine metrics using Add a Metric</li> <li>Apply policy and tuning in Optimization</li> <li>Review Evaluation vs Decisioning to reinforce role boundaries</li> </ul> <p>Running an evaluation is not about declaring winners. It is about producing reliable, decision-relevant information. Electric Barometer is designed to make that process explicit and reproducible.</p>"},{"location":"guides/tune-a-policy/","title":"Tune a Policy","text":"<p>Policy tuning determines how evaluation results are translated into decisions. In Electric Barometer, policies encode preferences, tradeoffs, and risk posture rather than technical performance alone.</p> <p>This guide describes how to tune a policy in a way that is explicit, reproducible, and governed\u2014without conflating policy choices with evaluation or modeling logic.</p>"},{"location":"guides/tune-a-policy/#what-policy-tuning-is-and-is-not","title":"What policy tuning is (and is not)","text":"<p>Policy tuning answers the question:</p> <p>Given evaluation outputs, how should tradeoffs be resolved to support a specific operational objective?</p> <p>Policy tuning does not:</p> <ul> <li>Change how metrics are computed</li> <li>Modify forecasting models</li> <li>Reinterpret evaluation results ad hoc</li> <li>Replace evaluation with heuristics</li> </ul> <p>Evaluation measures behavior. Policy determines action.</p> <p>For the conceptual boundary, see Evaluation vs Decisioning.</p>"},{"location":"guides/tune-a-policy/#when-you-should-tune-a-policy","title":"When you should tune a policy","text":"<p>Tune a policy when:</p> <ul> <li>Evaluation surfaces meaningful tradeoffs</li> <li>Cost asymmetry must be adjusted or refined</li> <li>Organizational priorities or risk tolerance change</li> <li>Multiple systems perform similarly under evaluation</li> <li>A policy must be adapted to a new operational context</li> </ul> <p>Policy tuning is expected and normal. Treating policy as fixed often leads to brittle or misaligned decisions.</p>"},{"location":"guides/tune-a-policy/#prerequisites","title":"Prerequisites","text":"<p>Before tuning a policy, ensure that:</p> <ul> <li>An evaluation has been run and preserved   (see Run an Evaluation)</li> <li>Evaluation metrics and parameters are understood   (see Metrics)</li> <li>The decision context is clearly framed   (see Problem Framing)</li> <li>Governance expectations are clear   (see Governance)</li> </ul> <p>Policy tuning without these prerequisites risks optimizing the wrong objective.</p>"},{"location":"guides/tune-a-policy/#step-1-clarify-the-decision-objective","title":"Step 1: Clarify the decision objective","text":"<p>Begin by stating the decision objective explicitly.</p> <p>Examples include:</p> <ul> <li>Minimizing downside risk</li> <li>Favoring stability over responsiveness</li> <li>Penalizing specific failure modes</li> <li>Balancing competing operational costs</li> </ul> <p>Objectives should be articulated in plain language before being encoded into policy parameters.</p>"},{"location":"guides/tune-a-policy/#step-2-identify-tunable-policy-parameters","title":"Step 2: Identify tunable policy parameters","text":"<p>Policies often expose parameters that control how evaluation outputs are interpreted.</p> <p>Examples include:</p> <ul> <li>Relative weighting of different error types</li> <li>Thresholds for acceptable performance</li> <li>Penalties applied under specific conditions</li> <li>Preferences applied during tie-breaking</li> </ul> <p>These parameters represent policy levers, not model parameters.</p>"},{"location":"guides/tune-a-policy/#step-3-explore-policy-sensitivity","title":"Step 3: Explore policy sensitivity","text":"<p>Before committing to a tuned policy, explore how decisions change as parameters vary.</p> <p>Sensitivity exploration may involve:</p> <ul> <li>Sweeping parameter ranges</li> <li>Comparing selections under different settings</li> <li>Identifying regions of instability or indifference</li> <li>Observing tradeoffs across segments or scenarios</li> </ul> <p>Sensitivity analysis helps distinguish robust policies from fragile ones.</p>"},{"location":"guides/tune-a-policy/#step-4-apply-readiness-considerations","title":"Step 4: Apply readiness considerations","text":"<p>Policy tuning should account for readiness, not just evaluation scores.</p> <p>Consider:</p> <ul> <li>Stability across time or entities</li> <li>Behavior under uncertainty or sparse data</li> <li>Consistency with operational constraints</li> </ul> <p>Readiness adjustments may modify or contextualize evaluation outputs prior to decisioning. For conceptual grounding, see Readiness and RAL.</p>"},{"location":"guides/tune-a-policy/#step-5-select-and-document-the-tuned-policy","title":"Step 5: Select and document the tuned policy","text":"<p>Once a policy configuration is selected:</p> <ul> <li>Record parameter values explicitly</li> <li>Document the rationale for choices</li> <li>Note tradeoffs that were accepted</li> <li>Identify conditions under which retuning may be required</li> </ul> <p>Documentation is essential for governance and future review.</p>"},{"location":"guides/tune-a-policy/#step-6-validate-policy-outcomes","title":"Step 6: Validate policy outcomes","text":"<p>Validation ensures that the tuned policy behaves as intended.</p> <p>Validation may include:</p> <ul> <li>Replaying historical evaluations</li> <li>Stress-testing under edge scenarios</li> <li>Verifying stability across segments</li> <li>Confirming alignment with stated objectives</li> </ul> <p>Policy validation focuses on decision outcomes, not metric optimization.</p>"},{"location":"guides/tune-a-policy/#governance-considerations","title":"Governance considerations","text":"<p>Policy tuning encodes organizational intent and must therefore be governed.</p> <p>Good governance practices include:</p> <ul> <li>Versioning policy configurations</li> <li>Avoiding silent parameter changes</li> <li>Preserving historical policy states</li> <li>Linking decisions to the policy in effect at the time</li> </ul> <p>Governance ensures that policy evolution remains transparent and auditable. See Governance for details.</p>"},{"location":"guides/tune-a-policy/#how-policy-tuning-fits-into-the-electric-barometer-lifecycle","title":"How policy tuning fits into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Evaluation measures system behavior   (see Run an Evaluation)</li> <li>Readiness contextualizes evaluation outputs   (see Readiness and RAL)</li> <li>Policy tuning defines how tradeoffs are resolved</li> <li>Decisioning applies tuned policies consistently</li> <li>Governance ensures traceability across time</li> </ul> <p>Policy tuning is the bridge between measurement and action.</p>"},{"location":"guides/tune-a-policy/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Review Optimization for selection and tuning mechanisms</li> <li>Revisit Asymmetric Cost to reassess tradeoffs</li> <li>Consult Papers for formal policy and framework definitions</li> </ul> <p>Tuning a policy is not about finding a single \u201cbest\u201d setting. It is about making tradeoffs explicit, defensible, and aligned with the decision context Electric Barometer is designed to support.</p>"},{"location":"metrics/","title":"Other Metrics","text":"<p>Electric Barometer is designed to support multiple evaluation metrics, not just a single score. While Cost-Weighted Service Loss (CWSL) is the flagship metric for asymmetric cost, other metrics may be appropriate depending on context, constraints, and evaluation goals.</p> <p>This section provides orientation and guidance for metrics beyond CWSL.</p>"},{"location":"metrics/#why-multiple-metrics-exist","title":"Why multiple metrics exist","text":"<p>No single metric can capture all aspects of forecasting system behavior.</p> <p>Different metrics emphasize different properties, such as:</p> <ul> <li>Average accuracy</li> <li>Variability or stability</li> <li>Bias or directional tendency</li> <li>Sensitivity to outliers</li> <li>Operational feasibility</li> </ul> <p>Using multiple metrics allows evaluators to understand tradeoffs rather than collapse behavior into a single number.</p>"},{"location":"metrics/#the-role-of-non-cwsl-metrics","title":"The role of non-CWSL metrics","text":"<p>Metrics other than CWSL typically serve one of three roles:</p> <ol> <li> <p>Diagnostic    Help explain why a system behaves the way it does</p> </li> <li> <p>Complementary    Surface properties not directly captured by cost-weighted loss</p> </li> <li> <p>Contextual    Provide additional signal under specific operating conditions</p> </li> </ol> <p>These metrics inform understanding; they do not override policy or decision logic.</p>"},{"location":"metrics/#relationship-to-cwsl","title":"Relationship to CWSL","text":"<p>CWSL is the reference metric for cost-aware evaluation, but it is not intended to stand alone.</p> <p>Common patterns include:</p> <ul> <li>Using symmetric accuracy metrics to contextualize CWSL results</li> <li>Inspecting bias metrics to explain directional loss</li> <li>Using stability metrics to support readiness assessment</li> <li>Comparing metrics to identify tradeoffs or sensitivities</li> </ul> <p>Disagreement between metrics is expected and often informative.</p>"},{"location":"metrics/#examples-of-other-metric-categories","title":"Examples of other metric categories","text":"<p>While Electric Barometer does not prescribe a fixed taxonomy, commonly useful categories include:</p> <ul> <li> <p>Symmetric accuracy metrics   e.g., absolute or squared error measures</p> </li> <li> <p>Bias and directional metrics   e.g., systematic over- or under-forecasting tendencies</p> </li> <li> <p>Stability and volatility metrics   e.g., sensitivity to noise or regime changes</p> </li> <li> <p>Coverage or feasibility metrics   e.g., constraint violations or infeasible outputs</p> </li> </ul> <p>Specific metrics within these categories may vary by application.</p>"},{"location":"metrics/#what-this-section-does-not-do","title":"What this section does not do","text":"<p>This section does not:</p> <ul> <li>Define or endorse a fixed list of metrics</li> <li>Replace CWSL as the primary cost-aware measure</li> <li>Provide implementation-level details</li> <li>Prescribe how metrics should be combined</li> </ul> <p>Metric selection and prioritization are governed by policy and decision context.</p> <p>See Policies for how metrics are used downstream.</p>"},{"location":"metrics/#when-to-introduce-a-new-metric","title":"When to introduce a new metric","text":"<p>A new metric may be appropriate when:</p> <ul> <li>Existing metrics fail to capture an important behavior</li> <li>A specific risk or constraint must be surfaced explicitly</li> <li>Diagnostic insight is needed beyond cost-weighted loss</li> <li>Evaluation results are ambiguous or unstable</li> </ul> <p>New metrics should be introduced deliberately, with clear documentation of purpose and assumptions.</p>"},{"location":"metrics/#governance-considerations","title":"Governance considerations","text":"<p>Like all evaluation metrics, additional metrics must be governed.</p> <p>Good governance includes:</p> <ul> <li>Explicit documentation of metric intent</li> <li>Clear separation from decision logic</li> <li>Stable definitions over time</li> <li>Versioning and traceability</li> </ul> <p>Metrics should illuminate decisions, not obscure them.</p>"},{"location":"metrics/#how-other-metrics-fit-into-the-lifecycle","title":"How other metrics fit into the lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Forecasting systems generate outputs</li> <li>Multiple metrics measure different aspects of behavior</li> <li>Evaluation surfaces tradeoffs and patterns</li> <li>Readiness and policy contextualize results</li> <li>Decisions are made deliberately and governed</li> </ul> <p>Other metrics enrich evaluation without replacing core principles.</p>"},{"location":"metrics/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Review Metrics Overview for the role of metrics in the framework</li> <li>Explore CWSL for cost-aware evaluation</li> <li>See Evaluation vs Decisioning for role clarity</li> <li>Consult Guides for evaluation workflows</li> </ul> <p>Electric Barometer treats metrics as lenses, not verdicts. Other metrics exist to broaden understanding, not to fragment decision-making.</p>"},{"location":"metrics/cwsl/","title":"Cost-Weighted Service Loss (CWSL)","text":"<p>Cost-Weighted Service Loss (CWSL) is a core evaluation metric in the Electric Barometer framework. It is designed to measure forecasting system behavior under asymmetric cost, where different error directions incur different operational consequences.</p> <p>CWSL provides a cost-aligned view of forecast error that complements traditional accuracy metrics and supports governed, decision-aware evaluation.</p>"},{"location":"metrics/cwsl/#why-cwsl-exists","title":"Why CWSL exists","text":"<p>Most common accuracy metrics treat forecast errors as symmetric and interchangeable. In operational environments, this assumption rarely holds.</p> <p>CWSL exists to:</p> <ul> <li>Make asymmetric cost explicit</li> <li>Preserve information about error direction</li> <li>Surface operational risk that symmetric metrics obscure</li> <li>Support evaluation under clearly stated assumptions</li> </ul> <p>CWSL is a measurement tool, not a decision rule. It informs decisioning without replacing policy or governance.</p> <p>For conceptual grounding, see Asymmetric Cost and Evaluation vs Decisioning.</p>"},{"location":"metrics/cwsl/#what-cwsl-measures","title":"What CWSL measures","text":"<p>At a high level, CWSL measures:</p> <ul> <li>Forecast error magnitude</li> <li>Weighted by the direction of the error</li> <li>Aggregated across a defined evaluation scope</li> </ul> <p>The resulting value represents expected loss under explicit cost-weighting assumptions.</p> <p>CWSL does not assume universal cost ratios. All weighting choices are contextual and must be made explicitly.</p>"},{"location":"metrics/cwsl/#how-to-navigate-the-cwsl-documentation","title":"How to navigate the CWSL documentation","text":"<p>The CWSL documentation is organized by intent:</p> <ul> <li> <p>Definition   Formal definition of CWSL, including structure, scope, and encoded assumptions.</p> </li> <li> <p>Interpretation   Guidance on how to read and compare CWSL values responsibly.</p> </li> <li> <p>Examples   Illustrative scenarios demonstrating how CWSL behaves under different conditions.</p> </li> <li> <p>Assumptions   Detailed discussion of the assumptions CWSL encodes and their implications.</p> </li> </ul> <p>Each page focuses on a distinct aspect of understanding CWSL. Together, they form a complete, non-overlapping explanation.</p>"},{"location":"metrics/cwsl/#what-cwsl-is-not","title":"What CWSL is not","text":"<p>CWSL does not:</p> <ul> <li>Select forecasting systems</li> <li>Encode organizational policy</li> <li>Resolve tradeoffs automatically</li> <li>Guarantee operational readiness</li> <li>Replace governance or decision logic</li> </ul> <p>Those responsibilities belong to readiness, policy tuning, and decisioning layers.</p> <p>See Readiness and RAL and Tune a Policy for how CWSL outputs are used downstream.</p>"},{"location":"metrics/cwsl/#how-cwsl-fits-into-the-electric-barometer-lifecycle","title":"How CWSL fits into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Forecasting systems generate candidate outputs</li> <li>CWSL measures cost-aligned error behavior</li> <li>Evaluation compares systems under explicit assumptions</li> <li>Readiness contextualizes evaluation results</li> <li>Policy tuning defines how tradeoffs are resolved</li> <li>Decisioning applies governed rules</li> <li>Releases preserve reproducibility</li> </ul> <p>CWSL is foundational, but not exclusive. It exemplifies how Electric Barometer treats metrics as explicit, interpretable lenses rather than opaque scores.</p>"},{"location":"metrics/cwsl/#when-to-use-cwsl","title":"When to use CWSL","text":"<p>CWSL is appropriate when:</p> <ul> <li>Error direction has unequal consequences</li> <li>Operational risk matters more than average accuracy</li> <li>Evaluation must reflect cost asymmetry explicitly</li> <li>Tradeoffs between stability and responsiveness are relevant</li> </ul> <p>In contexts where errors are truly symmetric and consequences are minimal, simpler metrics may suffice.</p>"},{"location":"metrics/cwsl/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Start with Definition if you are new to CWSL</li> <li>Read Interpretation to understand how to compare results</li> <li>Explore Examples to build intuition</li> <li>Review Assumptions to understand what CWSL encodes</li> <li>See Metrics for other evaluation lenses</li> <li>Consult Papers for formal mathematical treatments</li> </ul> <p>CWSL is the reference metric for asymmetric cost in Electric Barometer. It demonstrates how evaluation can be made explicit, interpretable, and aligned with real-world operational risk.</p>"},{"location":"metrics/cwsl/definition/","title":"Cost-Weighted Service Loss (CWSL): Definition","text":"<p>Cost-Weighted Service Loss (CWSL) is an evaluation metric designed to measure forecasting system behavior under asymmetric cost. It quantifies the expected operational loss induced by forecast error when different error directions incur different consequences.</p> <p>CWSL is a measurement tool, not a decision rule. It encodes assumptions about cost asymmetry explicitly and produces interpretable, reproducible values suitable for evaluation and comparison.</p>"},{"location":"metrics/cwsl/definition/#purpose-of-cwsl","title":"Purpose of CWSL","text":"<p>CWSL exists to address a limitation of traditional accuracy metrics: their inability to distinguish between errors that are operationally unequal.</p> <p>Specifically, CWSL is intended to:</p> <ul> <li>Reflect asymmetric consequences of under- and over-forecasting</li> <li>Preserve information about error direction</li> <li>Translate forecast error into a cost-aligned loss signal</li> <li>Support comparison of forecasting systems under explicit assumptions</li> </ul> <p>CWSL does not prescribe actions or policies. It provides a lens through which forecast behavior can be evaluated.</p>"},{"location":"metrics/cwsl/definition/#conceptual-definition","title":"Conceptual definition","text":"<p>At a conceptual level, CWSL measures loss as a weighted function of forecast error, where weights differ depending on the direction of the error.</p> <p>Key elements include:</p> <ul> <li>A realized outcome</li> <li>A forecasted value</li> <li>The signed error between them</li> <li>Direction-specific cost weights</li> <li>An aggregation over observations</li> </ul> <p>Unlike symmetric loss functions, CWSL assigns different penalties to under-forecasting and over-forecasting, even when the magnitude of error is identical.</p>"},{"location":"metrics/cwsl/definition/#error-direction-and-weighting","title":"Error direction and weighting","text":"<p>CWSL distinguishes between two fundamental error regimes:</p> <ul> <li>Under-forecasting \u2014 when the forecast is below the realized outcome</li> <li>Over-forecasting \u2014 when the forecast exceeds the realized outcome</li> </ul> <p>Each regime is associated with its own cost weight. These weights reflect the relative operational impact of the two error types.</p> <p>The choice of weights is contextual and must be made explicitly. CWSL does not assume a universal or \u201ccorrect\u201d cost ratio.</p>"},{"location":"metrics/cwsl/definition/#formal-structure","title":"Formal structure","text":"<p>Formally, CWSL is defined as an aggregation of per-observation losses, where each loss is computed as:</p> <ul> <li>The magnitude of the forecast error</li> <li>Multiplied by a cost weight determined by error direction</li> </ul> <p>The aggregation may occur across:</p> <ul> <li>Time</li> <li>Entities</li> <li>Segments</li> <li>Evaluation windows</li> </ul> <p>The resulting value represents the expected cost-aligned loss under the specified weighting assumptions.</p> <p>Exact mathematical formulations are provided in the formal technical notes and papers.</p>"},{"location":"metrics/cwsl/definition/#relationship-to-accuracy-metrics","title":"Relationship to accuracy metrics","text":"<p>CWSL differs from common accuracy metrics in several important ways:</p> <ul> <li>It is direction-aware, not magnitude-only</li> <li>It encodes explicit cost assumptions</li> <li>It measures operational loss, not abstract error</li> <li>It may rank forecasting systems differently than symmetric metrics</li> </ul> <p>A system that performs well under MAE or RMSE may perform poorly under CWSL if it systematically incurs costly errors.</p>"},{"location":"metrics/cwsl/definition/#assumptions-encoded-by-cwsl","title":"Assumptions encoded by CWSL","text":"<p>Every metric encodes assumptions. CWSL makes its assumptions explicit.</p> <p>CWSL assumes:</p> <ul> <li>Error direction matters</li> <li>Costs can be represented as relative weights</li> <li>Loss is proportional to error magnitude within each regime</li> <li>Aggregation reflects expected operational exposure</li> </ul> <p>These assumptions should be reviewed and validated against the decision context before relying on CWSL outputs.</p>"},{"location":"metrics/cwsl/definition/#scope-and-limitations","title":"Scope and limitations","text":"<p>CWSL is designed for evaluation, not decisioning.</p> <p>It does not:</p> <ul> <li>Select forecasting systems</li> <li>Resolve tradeoffs automatically</li> <li>Encode organizational policy</li> <li>Replace readiness or governance layers</li> </ul> <p>CWSL provides information. How that information is used depends on policy and decisioning processes.</p>"},{"location":"metrics/cwsl/definition/#how-cwsl-fits-into-the-electric-barometer-framework","title":"How CWSL fits into the Electric Barometer framework","text":"<p>Within the Electric Barometer lifecycle:</p> <ul> <li>Forecasting systems generate candidate outputs</li> <li>CWSL measures cost-aligned error behavior</li> <li>Evaluation compares systems under explicit assumptions</li> <li>Readiness and policy contextualize results</li> <li>Decisioning applies explicit rules</li> <li>Governance ensures transparency and reproducibility</li> </ul> <p>CWSL is a foundational metric within this framework, but it is not the only possible metric.</p>"},{"location":"metrics/cwsl/definition/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Read Interpretation to understand how to read CWSL values</li> <li>Review Assumptions to examine cost and modeling implications</li> <li>See Examples for applied usage scenarios</li> <li>Consult Asymmetric Cost for conceptual grounding</li> <li>Refer to Papers for formal mathematical definitions</li> </ul> <p>CWSL formalizes asymmetric cost in evaluation. It makes tradeoffs visible without making decisions on behalf of the user.</p>"},{"location":"metrics/cwsl/examples/","title":"Cost-Weighted Service Loss (CWSL): Examples","text":"<p>This page provides illustrative examples of how Cost-Weighted Service Loss (CWSL) behaves under different forecasting scenarios. The goal is to build intuition for how asymmetric cost weighting affects evaluation outcomes, not to prescribe specific weights or policies.</p> <p>All examples assume that evaluation is performed prior to readiness adjustment or decisioning.</p>"},{"location":"metrics/cwsl/examples/#example-1-symmetric-error-asymmetric-cost","title":"Example 1: Symmetric error, asymmetric cost","text":"<p>Consider two forecasts with identical absolute error magnitudes but opposite directions.</p> <ul> <li>Forecast A under-forecasts demand by 10 units</li> <li>Forecast B over-forecasts demand by 10 units</li> </ul> <p>Under symmetric accuracy metrics, both forecasts receive identical scores.</p> <p>Under CWSL:</p> <ul> <li>If under-forecasting carries a higher cost weight than over-forecasting,   Forecast A incurs greater loss.</li> <li>If over-forecasting carries a higher cost weight, Forecast B incurs greater loss.</li> </ul> <p>CWSL distinguishes these cases explicitly, even though the error magnitudes are the same.</p>"},{"location":"metrics/cwsl/examples/#example-2-two-systems-with-similar-accuracy","title":"Example 2: Two systems with similar accuracy","text":"<p>Suppose two forecasting systems exhibit similar performance under MAE.</p> <ul> <li>System X produces small errors but frequently under-forecasts</li> <li>System Y produces slightly larger errors but tends to over-forecast</li> </ul> <p>If under-forecasting is more costly in the operational context, CWSL may rank System Y as preferable despite its slightly worse symmetric accuracy.</p> <p>This example illustrates how CWSL can reverse rankings produced by traditional accuracy metrics when error direction matters.</p>"},{"location":"metrics/cwsl/examples/#example-3-sensitivity-to-cost-weights","title":"Example 3: Sensitivity to cost weights","text":"<p>CWSL outcomes depend on the chosen cost weights.</p> <p>Consider a single forecasting system evaluated under different cost weight configurations:</p> <ul> <li>Configuration 1 heavily penalizes under-forecasting</li> <li>Configuration 2 treats both error directions more evenly</li> </ul> <p>The same forecast errors may yield substantially different CWSL values under these configurations.</p> <p>This sensitivity is intentional. It reflects the fact that cost assumptions are contextual and must be made explicit rather than hidden inside the metric.</p>"},{"location":"metrics/cwsl/examples/#example-4-aggregation-across-time","title":"Example 4: Aggregation across time","text":"<p>CWSL aggregates per-observation losses across an evaluation window.</p> <p>A forecasting system may:</p> <ul> <li>Perform well on average</li> <li>Incur occasional large losses during specific periods</li> </ul> <p>CWSL captures the cumulative impact of these losses. A system that avoids rare but costly failures may outperform a system with lower average error but higher downside exposure.</p> <p>This behavior makes CWSL useful for identifying risk-sensitive differences between systems.</p>"},{"location":"metrics/cwsl/examples/#example-5-segment-level-behavior","title":"Example 5: Segment-level behavior","text":"<p>Consider a forecasting system evaluated across multiple segments (e.g., locations, products, or time bands).</p> <ul> <li>In some segments, the system over-forecasts</li> <li>In others, it under-forecasts</li> </ul> <p>CWSL can be computed and inspected at the segment level to reveal where cost-aligned losses concentrate.</p> <p>This enables evaluators to distinguish between globally acceptable behavior and localized risk, without collapsing everything into a single score prematurely.</p>"},{"location":"metrics/cwsl/examples/#example-6-comparing-stability-versus-responsiveness","title":"Example 6: Comparing stability versus responsiveness","text":"<p>Two forecasting systems may differ in volatility:</p> <ul> <li>System A responds aggressively to recent changes</li> <li>System B is more stable but slower to adapt</li> </ul> <p>If aggressive responses increase the likelihood of costly under-forecasting during spikes, CWSL may favor the more stable system, even if its point forecasts lag temporarily.</p> <p>This highlights how CWSL interacts with system behavior, not just point accuracy.</p>"},{"location":"metrics/cwsl/examples/#what-these-examples-illustrate","title":"What these examples illustrate","text":"<p>Across these examples, several patterns emerge:</p> <ul> <li>Error direction materially affects loss</li> <li>Cost assumptions shape evaluation outcomes</li> <li>Systems with similar accuracy can have very different CWSL profiles</li> <li>Aggregation and segmentation matter for interpretation</li> <li>CWSL surfaces risk that symmetric metrics may hide</li> </ul> <p>These behaviors are features, not flaws. They reflect the realities of operational decision-making under asymmetric cost.</p>"},{"location":"metrics/cwsl/examples/#what-examples-do-not-determine","title":"What examples do not determine","text":"<p>These examples do not determine:</p> <ul> <li>Which cost weights are \u201ccorrect\u201d</li> <li>Which system should be selected</li> <li>How tradeoffs should be resolved</li> <li>What policy should be applied</li> </ul> <p>Those choices belong to readiness, policy tuning, and governance layers.</p>"},{"location":"metrics/cwsl/examples/#how-to-use-cwsl-examples-responsibly","title":"How to use CWSL examples responsibly","text":"<p>Use examples like these to:</p> <ul> <li>Build intuition about metric behavior</li> <li>Communicate tradeoffs to stakeholders</li> <li>Identify evaluation sensitivity</li> <li>Guide further analysis</li> </ul> <p>Do not treat examples as prescriptions. CWSL is a measurement tool whose meaning depends on context.</p>"},{"location":"metrics/cwsl/examples/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Review Interpretation to understand how to read CWSL values</li> <li>Examine Assumptions to see what CWSL encodes</li> <li>Revisit Evaluation vs Decisioning for role clarity</li> <li>Consult Readiness and RAL for decision-facing adjustments</li> </ul> <p>CWSL examples are meant to sharpen understanding, not to decide outcomes. They illustrate why asymmetric cost must be measured explicitly in operational forecasting systems.</p>"},{"location":"metrics/cwsl/interpretation/","title":"Cost-Weighted Service Loss (CWSL): Interpretation","text":"<p>This page explains how to interpret Cost-Weighted Service Loss (CWSL) values produced during evaluation. Interpretation focuses on what CWSL tells you about forecasting system behavior under asymmetric cost\u2014not on how to make decisions or choose policies.</p> <p>CWSL is an evaluation metric. Its outputs must be interpreted in context and combined with readiness and policy layers before action is taken.</p>"},{"location":"metrics/cwsl/interpretation/#what-a-cwsl-value-represents","title":"What a CWSL value represents","text":"<p>A CWSL value represents cost-aligned loss accumulated over an evaluation scope under explicit cost-weighting assumptions.</p> <p>Interpreting a CWSL value requires understanding:</p> <ul> <li>The evaluation scope (time, entities, segments)</li> <li>The cost weights applied to error directions</li> <li>The aggregation method used</li> <li>The comparison baseline (if any)</li> </ul> <p>Lower CWSL values indicate lower expected loss under the specified assumptions.</p>"},{"location":"metrics/cwsl/interpretation/#interpreting-cwsl-comparatively","title":"Interpreting CWSL comparatively","text":"<p>CWSL is most meaningful when used comparatively.</p> <p>When comparing two forecasting systems under the same configuration:</p> <ul> <li>The system with lower CWSL incurs lower expected cost-aligned loss</li> <li>Differences reflect both error magnitude and error direction</li> <li>Rankings may differ from symmetric accuracy metrics</li> </ul> <p>Comparisons are only valid when evaluation scope and parameters are held constant.</p>"},{"location":"metrics/cwsl/interpretation/#directional-meaning-of-differences","title":"Directional meaning of differences","text":"<p>A difference in CWSL can arise from multiple sources:</p> <ul> <li>More frequent errors in a costly direction</li> <li>Larger errors in a costly direction</li> <li>Reduced exposure to high-cost regimes</li> <li>Improved stability during critical periods</li> </ul> <p>Interpreting why one system has lower CWSL often requires segment-level or temporal inspection rather than relying on a single aggregate value.</p>"},{"location":"metrics/cwsl/interpretation/#scale-and-magnitude-considerations","title":"Scale and magnitude considerations","text":"<p>CWSL values are expressed in relative units, not universal cost units.</p> <p>This means:</p> <ul> <li>Absolute values are less meaningful than relative differences</li> <li>Comparisons across evaluations with different weights or scopes are not directly comparable</li> <li>Trends over time must be interpreted under consistent configurations</li> </ul> <p>CWSL should not be treated as a dollar estimate unless explicitly calibrated to do so.</p>"},{"location":"metrics/cwsl/interpretation/#relationship-to-symmetric-metrics","title":"Relationship to symmetric metrics","text":"<p>CWSL should not be interpreted as a replacement for all accuracy metrics.</p> <p>Instead:</p> <ul> <li>Symmetric metrics summarize average error behavior</li> <li>CWSL summarizes cost-aligned risk exposure</li> <li>Disagreement between metrics highlights tradeoffs</li> </ul> <p>A system that scores well on MAE but poorly on CWSL may be accurate yet operationally risky under the chosen assumptions.</p>"},{"location":"metrics/cwsl/interpretation/#interpreting-near-ties-and-overlap","title":"Interpreting near-ties and overlap","text":"<p>When CWSL values are close:</p> <ul> <li>Differences may fall within noise or uncertainty</li> <li>Segment-level behavior may diverge despite similar aggregates</li> <li>Policy and readiness considerations become more important</li> </ul> <p>Near-ties should not be resolved by over-interpreting small numerical differences. This is where explicit Policy Tuning and Tie-breaking rules apply.</p>"},{"location":"metrics/cwsl/interpretation/#sensitivity-aware-interpretation","title":"Sensitivity-aware interpretation","text":"<p>CWSL is intentionally sensitive to cost weights.</p> <p>As such:</p> <ul> <li>Interpretation should consider how rankings change under different weight settings</li> <li>Stable rankings across reasonable ranges suggest robustness</li> <li>Volatile rankings suggest sensitivity to assumptions</li> </ul> <p>Sensitivity analysis supports responsible interpretation. See Examples for illustrations.</p>"},{"location":"metrics/cwsl/interpretation/#segment-level-interpretation","title":"Segment-level interpretation","text":"<p>CWSL can be computed at multiple granularities.</p> <p>Interpreting segment-level CWSL helps identify:</p> <ul> <li>Concentrated risk in specific entities or periods</li> <li>Asymmetric exposure hidden by aggregates</li> <li>Opportunities for targeted mitigation</li> </ul> <p>Aggregate CWSL values should be viewed as summaries, not substitutes for inspection.</p>"},{"location":"metrics/cwsl/interpretation/#what-cwsl-does-not-tell-you","title":"What CWSL does not tell you","text":"<p>CWSL does not tell you:</p> <ul> <li>Which system should be selected</li> <li>What policy is preferred</li> <li>Whether a system is \u201cready\u201d for use</li> <li>How much cost will be realized in absolute terms</li> </ul> <p>Those determinations require readiness adjustment, policy tuning, and governance.</p>"},{"location":"metrics/cwsl/interpretation/#interpreting-cwsl-responsibly","title":"Interpreting CWSL responsibly","text":"<p>Responsible interpretation of CWSL involves:</p> <ul> <li>Treating values as conditional on assumptions</li> <li>Comparing like with like</li> <li>Inspecting distributional behavior</li> <li>Avoiding overconfidence in single scores</li> <li>Escalating decisions to policy and governance layers</li> </ul> <p>For conceptual grounding, review Evaluation vs Decisioning and Readiness and RAL.</p>"},{"location":"metrics/cwsl/interpretation/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Review Assumptions to understand what CWSL encodes</li> <li>Explore Examples for scenario-based intuition</li> <li>Revisit Definition for formal structure</li> <li>Consult Asymmetric Cost for conceptual grounding</li> </ul> <p>CWSL interpretation is about understanding what the metric reveals, not about declaring outcomes. It is one input into a governed decision process.</p>"},{"location":"metrics/other-metrics/","title":"Other Metrics","text":"<p>This section documents the supporting readiness diagnostics used within the Electric Barometer framework, excluding Cost-Weighted Service Loss (CWSL), which is documented separately.</p> <p>These metrics do not compete with CWSL. Instead, they decompose readiness behavior along dimensions that CWSL alone cannot isolate, such as reliability, severity, tolerance responsiveness, and structural failure modes.</p>"},{"location":"metrics/other-metrics/#role-of-supporting-metrics","title":"Role of supporting metrics","text":"<p>Electric Barometer does not rely on a single scalar metric to characterize forecast readiness.</p> <p>Operational failure modes are multidimensional: - some systems fail often but shallowly, - others fail rarely but catastrophically, - some respond to adjustment, - others are structurally insensitive.</p> <p>The metrics in this section exist to diagnose these patterns explicitly, rather than forcing them into a single objective.</p>"},{"location":"metrics/other-metrics/#relationship-to-cwsl","title":"Relationship to CWSL","text":"<p>Cost-Weighted Service Loss (CWSL) measures aggregate, asymmetric cost exposure. It answers:</p> <p>How much effective throughput is lost under a declared cost structure?</p> <p>The metrics documented here answer complementary questions, such as:</p> <ul> <li>How often does the forecast fail to cover demand?</li> <li>How severe are failures when they occur?</li> <li>How does behavior change under tolerance or adjustment?</li> <li>Are failures structural or tunable?</li> </ul> <p>They are intended to be interpreted alongside CWSL, not instead of it.</p>"},{"location":"metrics/other-metrics/#metric-categories","title":"Metric categories","text":"<p>The metrics in this section fall into four broad categories.</p>"},{"location":"metrics/other-metrics/#reliability-metrics","title":"Reliability metrics","text":"<p>These metrics describe how often forecasts avoid failure, independent of magnitude.</p> <p>Examples include: - No\u2013Shortfall Level (NSL) \u2014 frequency of full coverage, - tolerance-based hit rates (e.g., HR@\u03c4).</p> <p>Reliability metrics are sensitive to occurrence, not severity.</p>"},{"location":"metrics/other-metrics/#severity-metrics","title":"Severity metrics","text":"<p>These metrics describe how bad failures are when they occur.</p> <p>Examples include: - Underbuild Depth (UD) \u2014 conditional shortfall magnitude.</p> <p>Severity metrics isolate tail behavior that is obscured by frequency-based measures.</p>"},{"location":"metrics/other-metrics/#tolerance-and-responsiveness-metrics","title":"Tolerance and responsiveness metrics","text":"<p>These metrics describe how forecasts behave under admissible perturbation, such as: - changes in tolerance, - bounded readiness adjustment, - or cost asymmetry sweeps.</p> <p>They are used to assess responsiveness, not to optimize outcomes.</p>"},{"location":"metrics/other-metrics/#structural-diagnostics","title":"Structural diagnostics","text":"<p>These diagnostics do not measure performance at all.</p> <p>Instead, they determine whether: - evaluation semantics are admissible, - readiness interventions are structurally meaningful, - or policy application is valid.</p> <p>Examples include: - Demand Quantization Compatibility (DQC), - Forecast Primitive Compatibility (FPC).</p> <p>These diagnostics gate interpretation and policy; they are not scored or ranked.</p>"},{"location":"metrics/other-metrics/#interpretive-boundaries","title":"Interpretive boundaries","text":"<p>All metrics documented here share the following properties:</p> <ul> <li>they are evaluative, not prescriptive,</li> <li>they rely only on observable quantities,</li> <li>they do not imply operational action,</li> <li>and they must be interpreted under governed unit semantics.</li> </ul> <p>No metric in this section should be optimized directly.</p>"},{"location":"metrics/other-metrics/#how-these-metrics-are-used","title":"How these metrics are used","text":"<p>In practice, these metrics are used to:</p> <ul> <li>explain why two forecasts with similar CWSL behave differently,</li> <li>identify structural incompatibility before adjustment or deployment,</li> <li>support governance decisions with explicit evidence,</li> <li>and communicate readiness risk in operational reviews.</li> </ul> <p>They provide resolution, not decisions.</p>"},{"location":"metrics/other-metrics/#navigation","title":"Navigation","text":"<p>Use the pages in this section to understand individual metrics in detail.</p> <p>For composite readiness evaluation and cost-sensitive analysis, refer to: - Cost-Weighted Service Loss (CWSL), - and Forecast Readiness Score (FRS).</p>"},{"location":"metrics/other-metrics/#summary","title":"Summary","text":"<p>CWSL captures cost exposure. The metrics in this section explain why that exposure arises.</p> <p>Together, they form a diagnostic stack that supports defensible readiness evaluation without collapsing interpretation into a single number.</p>"},{"location":"optimization/","title":"Optimization","text":"<p>The Optimization section describes how evaluation outputs are translated into governed, decision-ready outcomes within the Electric Barometer framework. It defines how tradeoffs are explored, assumptions are tuned, ambiguity is resolved, and selections are made deliberately.</p> <p>Optimization does not change forecasts or metrics. It operates above evaluation, encoding preference, risk posture, and intent explicitly.</p>"},{"location":"optimization/#what-optimization-does","title":"What optimization does","text":"<p>Within Electric Barometer, optimization:</p> <ul> <li>Explores tradeoffs surfaced by evaluation</li> <li>Makes asymmetric cost assumptions explicit</li> <li>Applies policy to evaluation outputs</li> <li>Resolves ambiguity through tie-breaking</li> <li>Produces decision-ready selections</li> <li>Preserves traceability and governance</li> </ul> <p>Optimization turns measurement into actionable alignment.</p>"},{"location":"optimization/#what-optimization-is-not","title":"What optimization is not","text":"<p>Optimization is not:</p> <ul> <li>Model training or retraining</li> <li>Metric definition or computation</li> <li>Implicit ranking by numerical noise</li> <li>Automated decision-making without policy</li> <li>A replacement for governance</li> </ul> <p>For role clarity, see Evaluation vs Decisioning.</p>"},{"location":"optimization/#position-in-the-lifecycle","title":"Position in the lifecycle","text":"<p>Optimization sits between evaluation and decisioning.</p> <p>A simplified lifecycle:</p> <ol> <li>Forecasting systems generate outputs</li> <li>Metrics evaluate behavior</li> <li>Optimization explores tradeoffs and applies policy</li> <li>Decisions are made deliberately and governed</li> </ol> <p>Optimization ensures that decisions reflect intent rather than accident.</p>"},{"location":"optimization/#core-components-of-optimization","title":"Core components of optimization","text":"<p>The Optimization section is organized around four core components.</p>"},{"location":"optimization/#cost-ratio-configuration","title":"Cost ratio configuration","text":"<p>Cost ratios encode asymmetric cost assumptions that influence evaluation outcomes.</p> <ul> <li>Defines relative penalties for different error types</li> <li>Treated as a policy parameter, not a fact</li> <li>Tuned explicitly and governed</li> </ul> <p>See Cost Ratio Optimization.</p>"},{"location":"optimization/#policies","title":"Policies","text":"<p>Policies define how evaluation outputs are interpreted and acted upon.</p> <ul> <li>Combine metrics and constraints</li> <li>Encode preference and risk posture</li> <li>Separate measurement from decision rules</li> </ul> <p>See Policies.</p>"},{"location":"optimization/#tuning","title":"Tuning","text":"<p>Tuning refines policy parameters and assumptions to align outcomes with the decision context.</p> <ul> <li>Adjusts cost ratios and thresholds</li> <li>Explores sensitivity and robustness</li> <li>Commits to explicit configurations</li> </ul> <p>See Tuning and Tune a Policy.</p>"},{"location":"optimization/#tie-breaking","title":"Tie-breaking","text":"<p>Tie-breaking resolves ambiguity when evaluation does not produce a clear winner.</p> <ul> <li>Applies explicit secondary criteria</li> <li>Avoids false precision</li> <li>Ensures consistent outcomes under noise</li> </ul> <p>See Tie-Breaking.</p>"},{"location":"optimization/#relationship-to-metrics","title":"Relationship to metrics","text":"<p>Metrics measure behavior; optimization decides how to use those measurements.</p> <p>For example:</p> <ul> <li>CWSL measures cost-aligned loss</li> <li>Optimization determines acceptable ranges, priorities, and tradeoffs</li> <li>Policies and tuning decide how CWSL influences selection</li> </ul> <p>This separation keeps metrics reusable and decisions adaptable.</p>"},{"location":"optimization/#relationship-to-readiness-and-ral","title":"Relationship to readiness and RAL","text":"<p>Optimization commonly operates on readiness-adjusted outputs rather than raw metric values.</p> <p>Readiness considerations may:</p> <ul> <li>Penalize instability or uncertainty</li> <li>Constrain acceptable solutions</li> <li>Shape tuning and tie-breaking rules</li> </ul> <p>See Readiness and RAL for conceptual grounding.</p>"},{"location":"optimization/#governance-and-traceability","title":"Governance and traceability","text":"<p>Optimization directly influences decisions and must therefore be governed.</p> <p>Governance ensures that:</p> <ul> <li>Assumptions are explicit</li> <li>Changes are intentional and documented</li> <li>Historical decisions remain interpretable</li> <li>Outcomes are auditable</li> </ul> <p>For governance principles, see Governance.</p>"},{"location":"optimization/#when-to-engage-optimization","title":"When to engage optimization","text":"<p>Engage optimization when:</p> <ul> <li>Evaluation surfaces meaningful tradeoffs</li> <li>Asymmetric cost assumptions matter</li> <li>Multiple systems perform similarly</li> <li>Decision context or risk tolerance changes</li> <li>Sensitivity to assumptions must be understood</li> </ul> <p>Optimization is expected, not exceptional.</p>"},{"location":"optimization/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Start with Cost Ratio Optimization for asymmetric weighting</li> <li>Read Policies to understand decision rules</li> <li>Use Tuning to refine assumptions</li> <li>Apply Tie-Breaking to resolve ambiguity</li> <li>Revisit Metrics for measurement context</li> </ul> <p>Optimization is where Electric Barometer turns evaluation into governed decisions. It exists to ensure that tradeoffs are handled deliberately, transparently, and defensibly.</p>"},{"location":"optimization/cost-ratio/","title":"Cost Ratio Optimization","text":"<p>Cost ratio optimization determines how relative costs of different error types are set and adjusted within the Electric Barometer framework. It governs how asymmetric cost assumptions are parameterized, not how forecasts are generated or how metrics are computed.</p> <p>This document describes what the cost ratio represents, why it must be optimized explicitly, and how it fits into governed decision-making.</p>"},{"location":"optimization/cost-ratio/#what-the-cost-ratio-represents","title":"What the cost ratio represents","text":"<p>The cost ratio expresses the relative penalty assigned to different directions or types of error. Most commonly, it encodes the tradeoff between:</p> <ul> <li>Under-forecasting loss</li> <li>Over-forecasting loss</li> </ul> <p>The ratio does not represent an absolute cost. It expresses relative preference or risk posture under a given operational context.</p> <p>Cost ratios are policy parameters, not data-derived truths.</p>"},{"location":"optimization/cost-ratio/#why-cost-ratios-must-be-explicit","title":"Why cost ratios must be explicit","text":"<p>In many forecasting systems, asymmetric cost is handled implicitly:</p> <ul> <li>Hard-coded penalty weights</li> <li>Unstated business rules</li> <li>Metric behavior treated as \u201cobjective\u201d</li> </ul> <p>These approaches obscure assumptions and make decisions difficult to explain or reproduce.</p> <p>Electric Barometer requires cost ratios to be explicit so that:</p> <ul> <li>Tradeoffs are visible</li> <li>Sensitivity can be analyzed</li> <li>Policies can evolve deliberately</li> <li>Decisions can be audited</li> </ul> <p>For conceptual grounding, see Asymmetric Cost and Governance.</p>"},{"location":"optimization/cost-ratio/#cost-ratio-vs-metrics","title":"Cost ratio vs metrics","text":"<p>Cost ratios are inputs to metrics, not metrics themselves.</p> <p>For example:</p> <ul> <li>CWSL uses cost ratios to weight error direction   (see CWSL Definition)</li> <li>Changing the cost ratio changes metric output</li> <li>The metric computation itself remains unchanged</li> </ul> <p>This separation ensures that measurement and policy remain distinct.</p> <p>See Evaluation vs Decisioning for why this distinction matters.</p>"},{"location":"optimization/cost-ratio/#when-to-optimize-a-cost-ratio","title":"When to optimize a cost ratio","text":"<p>Cost ratio optimization is appropriate when:</p> <ul> <li>Evaluation results depend strongly on weighting assumptions</li> <li>Organizational priorities or risk tolerance change</li> <li>Different operational contexts require different tradeoffs</li> <li>Near-ties emerge under multiple metrics</li> <li>Sensitivity to asymmetric cost must be explored</li> </ul> <p>Cost ratios should not be treated as fixed constants unless the decision context is static.</p>"},{"location":"optimization/cost-ratio/#cost-ratio-as-a-policy-lever","title":"Cost ratio as a policy lever","text":"<p>Within Electric Barometer, the cost ratio is treated as a policy lever.</p> <p>Policy levers:</p> <ul> <li>Encode preferences, not facts</li> <li>May vary across contexts</li> <li>Are subject to governance</li> <li>Can be tuned without retraining models</li> </ul> <p>Optimizing a cost ratio means selecting values that best align evaluation outcomes with decision objectives\u2014not discovering a \u201ctrue\u201d cost.</p>"},{"location":"optimization/cost-ratio/#approaches-to-cost-ratio-optimization","title":"Approaches to cost ratio optimization","text":"<p>Cost ratio optimization may involve:</p> <ul> <li>Sweeping candidate ratios and observing evaluation outcomes</li> <li>Identifying regions of stability or sensitivity</li> <li>Comparing selections under different ratios</li> <li>Stress-testing decisions under extreme assumptions</li> </ul> <p>Optimization here is exploratory and comparative, not purely numerical.</p> <p>For procedural guidance, see Tune a Policy.</p>"},{"location":"optimization/cost-ratio/#relationship-to-readiness-and-ral","title":"Relationship to readiness and RAL","text":"<p>Cost ratios influence readiness indirectly.</p> <p>Extreme or unstable ratios may:</p> <ul> <li>Over-amplify rare events</li> <li>Produce volatile rankings</li> <li>Reduce decision robustness</li> </ul> <p>Readiness considerations may therefore constrain or adjust cost ratio usage.</p> <p>See Readiness and RAL for how cost assumptions are contextualized before decisioning.</p>"},{"location":"optimization/cost-ratio/#governance-considerations","title":"Governance considerations","text":"<p>Because cost ratios encode organizational intent, they must be governed.</p> <p>Good governance practices include:</p> <ul> <li>Versioning cost ratio configurations</li> <li>Documenting rationale for chosen values</li> <li>Preserving historical ratios for auditability</li> <li>Avoiding silent or implicit changes</li> </ul> <p>Governance ensures that changes in cost posture are intentional and explainable.</p>"},{"location":"optimization/cost-ratio/#how-cost-ratio-optimization-fits-into-the-lifecycle","title":"How cost ratio optimization fits into the lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Metrics measure behavior under given cost ratios</li> <li>Cost ratios define asymmetric weighting assumptions</li> <li>Evaluation surfaces tradeoffs under those assumptions</li> <li>Readiness contextualizes metric outputs</li> <li>Policy tuning selects appropriate ratios</li> <li>Decisioning applies governed rules</li> <li>Releases preserve reproducibility</li> </ul> <p>Cost ratio optimization is the hinge between evaluation and policy.</p>"},{"location":"optimization/cost-ratio/#what-cost-ratio-optimization-does-not-do","title":"What cost ratio optimization does not do","text":"<p>Cost ratio optimization does not:</p> <ul> <li>Improve forecast accuracy</li> <li>Change model behavior</li> <li>Replace evaluation metrics</li> <li>Eliminate tradeoffs</li> <li>Decide actions automatically</li> </ul> <p>It clarifies assumptions so that decisions can be made responsibly.</p>"},{"location":"optimization/cost-ratio/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Review CWSL Interpretation to see how cost ratios affect evaluation</li> <li>Follow Tune a Policy for procedural tuning guidance</li> <li>Revisit Asymmetric Cost for conceptual grounding</li> <li>Consult Papers for formal treatments of cost-aware optimization</li> </ul> <p>Cost ratio optimization makes tradeoffs explicit. In Electric Barometer, explicit tradeoffs are a prerequisite for governed, defensible decision-making.</p>"},{"location":"optimization/policies/","title":"Policies","text":"<p>Policies define how evaluation results are translated into decisions within the Electric Barometer framework. They encode preferences, tradeoffs, and risk posture explicitly, rather than allowing them to emerge implicitly from metrics or implementations.</p> <p>This document explains what policies are, why they matter, and how they operate within optimization and decisioning workflows.</p>"},{"location":"optimization/policies/#what-a-policy-is","title":"What a policy is","text":"<p>A policy is an explicit set of rules that governs how evaluation outputs are interpreted and acted upon.</p> <p>A policy may specify:</p> <ul> <li>How multiple metrics are combined or prioritized</li> <li>How asymmetric cost assumptions are applied</li> <li>How tradeoffs are resolved when objectives conflict</li> <li>How near-ties or ambiguous outcomes are handled</li> <li>When conservative versus aggressive behavior is preferred</li> <li>Whether evaluation outputs are structurally admissible</li> </ul> <p>Policies encode intent, not technical capability.</p>"},{"location":"optimization/policies/#what-a-policy-is-not","title":"What a policy is not","text":"<p>A policy is not:</p> <ul> <li>A forecasting model</li> <li>An evaluation metric</li> <li>A cost function embedded in code</li> <li>An optimization algorithm</li> <li>A heuristic applied ad hoc</li> </ul> <p>Policies operate after evaluation and before decisions.</p> <p>However, some policies operate on the validity of evaluation itself, rather than on the ranking of results.</p> <p>For conceptual grounding, see Evaluation vs Decisioning.</p>"},{"location":"optimization/policies/#why-policies-must-be-explicit","title":"Why policies must be explicit","text":"<p>In many systems, policy exists implicitly:</p> <ul> <li>The \u201cbest\u201d metric wins by default</li> <li>Small numerical differences are over-interpreted</li> <li>Preferences are hard-coded or undocumented</li> <li>Structural assumptions are left unchecked</li> <li>Decisions cannot be reconstructed later</li> </ul> <p>Electric Barometer requires policies to be explicit so that:</p> <ul> <li>Decisions are explainable</li> <li>Tradeoffs are visible</li> <li>Structural assumptions are surfaced</li> <li>Changes are intentional</li> <li>Outcomes are reproducible</li> </ul> <p>For governance implications, see Governance.</p>"},{"location":"optimization/policies/#policy-inputs","title":"Policy inputs","text":"<p>Policies consume structured evaluation outputs.</p> <p>Common inputs include:</p> <ul> <li>Metric values (e.g., CWSL)</li> <li>Segment-level summaries</li> <li>Sensitivity or stability indicators</li> <li>Readiness-adjusted scores</li> <li>Constraints or thresholds</li> <li>Structural diagnostics (e.g., demand quantization properties)</li> </ul> <p>Policies do not reinterpret raw data or forecasts directly.</p>"},{"location":"optimization/policies/#policy-outputs","title":"Policy outputs","text":"<p>Policy application produces decision-ready outcomes, such as:</p> <ul> <li>Ranked forecasting systems</li> <li>Selected configurations</li> <li>Eligibility or exclusion flags</li> <li>Tie-breaking outcomes</li> <li>Signals for retuning or escalation</li> <li>Validation or rejection of evaluation results</li> </ul> <p>These outputs are then passed to decisioning or operational layers.</p>"},{"location":"optimization/policies/#relationship-to-metrics","title":"Relationship to metrics","text":"<p>Metrics measure behavior under assumptions. Policies determine what to do with those measurements \u2014 or whether they are valid to use at all.</p> <p>For example:</p> <ul> <li>CWSL measures cost-aligned loss   (see CWSL)</li> <li>A policy may prioritize lower CWSL under certain constraints</li> <li>Another policy may tolerate higher CWSL for improved stability</li> <li>A structural policy may determine that a metric is invalid unless certain conditions are met</li> </ul> <p>This separation ensures that metrics remain reusable across contexts while policies remain responsible for interpretability and correctness.</p>"},{"location":"optimization/policies/#structural-and-admissibility-policies-dqc","title":"Structural and admissibility policies (DQC)","text":"<p>Not all policies express preferences. Some policies govern compatibility and admissibility.</p> <p>Demand Quantization Compatibility (DQC) is an example of a structural policy:</p> <ul> <li>It determines whether realized demand lies on a discrete grid</li> <li>It infers the governing unit size (\u0394*)</li> <li>It classifies demand as CONTINUOUS, QUANTIZED, or PACKED</li> <li>It enforces unit compatibility by requiring forecasts to be snapped to \u0394* when necessary</li> </ul> <p>Under DQC:</p> <ul> <li>Evaluation without snapping may be mathematically invalid</li> <li>Tolerance parameters (\u03c4) must be interpreted in grid units, not raw numeric units</li> <li>Certain evaluation results may be rejected or corrected by policy before decisioning</li> </ul> <p>DQC does not measure performance. It governs whether performance measurements are meaningful.</p>"},{"location":"optimization/policies/#relationship-to-cost-ratios","title":"Relationship to cost ratios","text":"<p>Cost ratios are policy parameters.</p> <p>They influence how metrics like CWSL behave, but they do not determine decisions on their own.</p> <p>Structural policies such as DQC operate orthogonally to cost ratios: - Cost ratios govern tradeoffs - DQC governs unit compatibility and validity</p> <p>See Cost Ratio Optimization for how cost ratios are tuned within policy frameworks.</p>"},{"location":"optimization/policies/#relationship-to-readiness-and-ral","title":"Relationship to readiness and RAL","text":"<p>Policies often operate on readiness-adjusted outputs rather than raw metrics.</p> <p>Readiness adjustments may:</p> <ul> <li>Penalize unstable systems</li> <li>Dampen extreme sensitivity</li> <li>Reflect operational constraints</li> </ul> <p>Structural policies apply before or alongside readiness adjustments to ensure that the inputs to RAL are valid.</p> <p>See Readiness and RAL for conceptual grounding.</p>"},{"location":"optimization/policies/#policy-tuning-and-evolution","title":"Policy tuning and evolution","text":"<p>Policies are expected to evolve.</p> <p>Tuning a policy may involve:</p> <ul> <li>Adjusting cost ratios</li> <li>Refining thresholds or priorities</li> <li>Changing tie-breaking rules</li> <li>Revising admissibility criteria</li> <li>Adapting to new operational contexts</li> </ul> <p>Policy tuning should be explicit and governed. See Tune a Policy for procedural guidance.</p>"},{"location":"optimization/policies/#governance-considerations","title":"Governance considerations","text":"<p>Policies encode organizational intent and must therefore be governed.</p> <p>Good governance practices include:</p> <ul> <li>Versioning policy definitions</li> <li>Documenting rationale and tradeoffs</li> <li>Preserving historical policies</li> <li>Recording structural assumptions</li> <li>Linking decisions to the policy in effect at the time</li> </ul> <p>Governance ensures that policy changes remain transparent, auditable, and defensible.</p>"},{"location":"optimization/policies/#how-policies-fit-into-the-electric-barometer-lifecycle","title":"How policies fit into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Metrics measure forecasting system behavior</li> <li>Structural diagnostics assess validity and compatibility</li> <li>Cost ratios define asymmetric weighting assumptions</li> <li>Evaluation produces structured outputs</li> <li>Readiness contextualizes evaluation results</li> <li>Policies apply explicit decision and admissibility rules</li> <li>Decisioning commits to actions</li> <li>Releases preserve reproducibility</li> </ul> <p>Policies are the connective tissue between measurement and action.</p>"},{"location":"optimization/policies/#what-policies-enable","title":"What policies enable","text":"<p>Explicit policies enable:</p> <ul> <li>Consistent decision-making across time and teams</li> <li>Responsible handling of tradeoffs</li> <li>Structural correctness of evaluation</li> <li>Clear explanation of outcomes</li> <li>Safe evolution of evaluation systems</li> <li>Alignment between technical systems and organizational goals</li> </ul> <p>Without policies, decisions drift. Without structural policies, decisions may rest on invalid measurements. With explicit policies, decisions are governed.</p>"},{"location":"optimization/policies/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Review Tune a Policy for hands-on tuning</li> <li>Revisit Evaluation vs Decisioning for role clarity</li> <li>Explore Cost Ratio Optimization for asymmetric weighting</li> <li>Consult Papers for formal policy frameworks</li> </ul> <p>Policies make decisions defensible. In Electric Barometer, defensibility includes correctness.</p>"},{"location":"optimization/tie-breaking/","title":"Tie-Breaking","text":"<p>Tie-breaking defines how decisions are resolved when evaluation outcomes do not produce a clear winner. In the Electric Barometer framework, tie-breaking is an explicit, policy-driven process\u2014not an implicit consequence of numerical noise or metric ordering.</p> <p>This document explains when tie-breaking is required, how it operates, and why it must be governed.</p>"},{"location":"optimization/tie-breaking/#why-tie-breaking-exists","title":"Why tie-breaking exists","text":"<p>In real evaluation scenarios, it is common for:</p> <ul> <li>Multiple forecasting systems to perform similarly</li> <li>Differences in metric values to fall within noise or uncertainty</li> <li>Tradeoffs to cancel out across metrics or segments</li> <li>Rankings to change under small parameter variations</li> </ul> <p>In these cases, attempting to force a single \u201cbest\u201d outcome from evaluation alone introduces false precision.</p> <p>Tie-breaking exists to resolve ambiguity deliberately, rather than accidentally.</p>"},{"location":"optimization/tie-breaking/#what-tie-breaking-is","title":"What tie-breaking is","text":"<p>Tie-breaking is the application of explicit rules to resolve ambiguity between otherwise acceptable alternatives.</p> <p>Tie-breaking rules may consider:</p> <ul> <li>Stability versus responsiveness</li> <li>Downside risk exposure</li> <li>Readiness-adjusted performance</li> <li>Simplicity or operational robustness</li> <li>Historical consistency or inertia</li> </ul> <p>Tie-breaking encodes preference, not performance.</p>"},{"location":"optimization/tie-breaking/#what-tie-breaking-is-not","title":"What tie-breaking is not","text":"<p>Tie-breaking is not:</p> <ul> <li>Selecting the smallest numerical difference</li> <li>Re-running evaluation with slightly different parameters</li> <li>Adding hidden heuristics to metrics</li> <li>Treating noise as signal</li> <li>Avoiding decision responsibility</li> </ul> <p>For conceptual grounding, see Evaluation vs Decisioning.</p>"},{"location":"optimization/tie-breaking/#when-tie-breaking-should-be-applied","title":"When tie-breaking should be applied","text":"<p>Tie-breaking should be applied when:</p> <ul> <li>Multiple systems fall within an acceptable performance band</li> <li>CWSL or other metrics produce near-identical values</li> <li>Rankings are unstable under reasonable parameter variation</li> <li>Segment-level tradeoffs offset one another</li> <li>Evaluation outputs alone do not support a confident choice</li> </ul> <p>Tie-breaking should not be applied to override clearly inferior systems.</p>"},{"location":"optimization/tie-breaking/#relationship-to-metrics","title":"Relationship to metrics","text":"<p>Metrics surface behavior; they do not resolve ambiguity.</p> <p>For example:</p> <ul> <li>Two systems may have similar CWSL values</li> <li>One may be more volatile, the other more stable</li> <li>Metrics alone cannot encode which is preferred</li> </ul> <p>Tie-breaking rules act on metric outputs, not inside metric computation.</p>"},{"location":"optimization/tie-breaking/#relationship-to-readiness-and-ral","title":"Relationship to readiness and RAL","text":"<p>Tie-breaking often operates on readiness-adjusted outputs rather than raw metrics.</p> <p>Readiness considerations may:</p> <ul> <li>Penalize unstable systems</li> <li>Discount performance in sparse or uncertain regimes</li> <li>Favor robustness over marginal gains</li> </ul> <p>See Readiness and RAL for conceptual grounding.</p>"},{"location":"optimization/tie-breaking/#tie-breaking-as-a-policy-component","title":"Tie-breaking as a policy component","text":"<p>Tie-breaking is a component of policy, not a separate system.</p> <p>Policies may specify:</p> <ul> <li>Primary metrics for evaluation</li> <li>Acceptable performance bands</li> <li>Secondary criteria for tie-breaking</li> <li>Ordering or precedence rules</li> <li>Escalation or deferral conditions</li> </ul> <p>See Policies for how tie-breaking fits into policy design.</p>"},{"location":"optimization/tie-breaking/#common-tie-breaking-criteria","title":"Common tie-breaking criteria","text":"<p>While criteria vary by context, common tie-breaking dimensions include:</p> <ul> <li>Stability \u2014 preference for less volatile behavior</li> <li>Downside risk \u2014 avoidance of rare but costly failures</li> <li>Readiness \u2014 suitability for operational use</li> <li>Simplicity \u2014 fewer dependencies or assumptions</li> <li>Consistency \u2014 alignment with historical behavior</li> </ul> <p>These criteria should be declared explicitly, not inferred post hoc.</p>"},{"location":"optimization/tie-breaking/#governance-considerations","title":"Governance considerations","text":"<p>Tie-breaking rules directly influence decisions and must be governed.</p> <p>Good governance practices include:</p> <ul> <li>Documenting tie-breaking rules explicitly</li> <li>Versioning policy definitions</li> <li>Avoiding ad hoc or case-by-case overrides</li> <li>Preserving historical tie-breaking context</li> <li>Linking decisions to the rules in effect at the time</li> </ul> <p>Governance ensures that tie-breaking remains fair, consistent, and explainable.</p>"},{"location":"optimization/tie-breaking/#how-tie-breaking-fits-into-the-electric-barometer-lifecycle","title":"How tie-breaking fits into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Metrics measure behavior</li> <li>Evaluation compares systems</li> <li>Readiness contextualizes results</li> <li>Policies define acceptable outcomes</li> <li>Tie-breaking resolves ambiguity</li> <li>Decisioning commits to action</li> <li>Releases preserve traceability</li> </ul> <p>Tie-breaking is the final step before decision commitment when evaluation alone is insufficient.</p>"},{"location":"optimization/tie-breaking/#what-tie-breaking-enables","title":"What tie-breaking enables","text":"<p>Explicit tie-breaking enables:</p> <ul> <li>Responsible handling of uncertainty</li> <li>Avoidance of false precision</li> <li>Stable decisions under noise</li> <li>Clear explanation of outcomes</li> <li>Alignment between technical results and organizational priorities</li> </ul> <p>Without tie-breaking, ambiguity is resolved implicitly. With tie-breaking, it is resolved intentionally.</p>"},{"location":"optimization/tie-breaking/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Review Policies to see how tie-breaking fits into policy design</li> <li>Revisit CWSL Interpretation for understanding near-ties</li> <li>See Tune a Policy for procedural guidance</li> <li>Consult Governance for oversight principles</li> </ul> <p>Tie-breaking is not a failure of evaluation. It is a recognition that decision-making under uncertainty requires explicit judgment.</p>"},{"location":"optimization/tuning/","title":"Tuning","text":"<p>Tuning is the process of adjusting parameters and policies to align evaluation outcomes with a specific decision context. In the Electric Barometer framework, tuning operates above metrics and models, shaping how tradeoffs are resolved rather than how predictions are generated.</p> <p>This document describes what tuning is, what it applies to, and how it fits into governed optimization workflows.</p>"},{"location":"optimization/tuning/#what-tuning-is","title":"What tuning is","text":"<p>Tuning is the deliberate adjustment of decision-facing parameters to improve alignment between evaluation results and operational objectives.</p> <p>Tuning may involve:</p> <ul> <li>Adjusting cost ratios</li> <li>Refining policy thresholds</li> <li>Modifying tie-breaking criteria</li> <li>Selecting acceptable performance bands</li> <li>Balancing stability versus responsiveness</li> </ul> <p>Tuning encodes preference and risk posture explicitly.</p>"},{"location":"optimization/tuning/#what-tuning-is-not","title":"What tuning is not","text":"<p>Tuning is not:</p> <ul> <li>Training or retraining forecasting models</li> <li>Optimizing loss functions during model fitting</li> <li>Modifying metric definitions</li> <li>Searching for a single numerical optimum</li> <li>Overfitting to historical outcomes</li> </ul> <p>Those activities belong to modeling and evaluation stages.</p> <p>For role clarity, see Evaluation vs Decisioning.</p>"},{"location":"optimization/tuning/#why-tuning-is-necessary","title":"Why tuning is necessary","text":"<p>Operational contexts evolve:</p> <ul> <li>Costs change</li> <li>Constraints shift</li> <li>Risk tolerance varies</li> <li>Decision cadence adjusts</li> </ul> <p>A fixed set of assumptions rarely remains optimal over time. Tuning allows the system to adapt without rewriting metrics or models, preserving stability while remaining responsive.</p>"},{"location":"optimization/tuning/#what-can-be-tuned","title":"What can be tuned","text":"<p>Within Electric Barometer, tunable elements include:</p> <ul> <li> <p>Cost ratios   (see Cost Ratio Optimization)</p> </li> <li> <p>Policy parameters   (see Policies)</p> </li> <li> <p>Tie-breaking rules   (see Tie-Breaking)</p> </li> <li> <p>Readiness adjustments   (see Readiness and RAL)</p> </li> </ul> <p>Tuning focuses on how evaluation results are interpreted, not on how they are computed.</p>"},{"location":"optimization/tuning/#tuning-versus-optimization","title":"Tuning versus optimization","text":"<p>Although related, tuning and optimization serve different purposes.</p> <ul> <li>Optimization explores tradeoffs and candidate configurations</li> <li>Tuning selects and refines configurations based on intent</li> </ul> <p>Optimization surfaces possibilities. Tuning commits to choices.</p> <p>For procedural guidance, see Tune a Policy.</p>"},{"location":"optimization/tuning/#tuning-workflow-overview","title":"Tuning workflow overview","text":"<p>A typical tuning workflow includes:</p> <ol> <li> <p>Running an evaluation    (see Run an Evaluation)</p> </li> <li> <p>Identifying tunable parameters</p> </li> <li> <p>Exploring sensitivity and tradeoffs</p> </li> <li> <p>Applying readiness considerations</p> </li> <li> <p>Selecting tuned values</p> </li> <li> <p>Documenting and versioning the result</p> </li> </ol> <p>This process emphasizes transparency and reproducibility over speed.</p>"},{"location":"optimization/tuning/#sensitivity-aware-tuning","title":"Sensitivity-aware tuning","text":"<p>Responsible tuning requires sensitivity analysis.</p> <p>This includes:</p> <ul> <li>Observing how outcomes change across parameter ranges</li> <li>Identifying regions of stability</li> <li>Avoiding brittle configurations</li> <li>Understanding interactions between parameters</li> </ul> <p>Sensitivity-aware tuning helps prevent over-commitment to fragile assumptions.</p>"},{"location":"optimization/tuning/#relationship-to-governance","title":"Relationship to governance","text":"<p>Tuning directly affects decisions and must therefore be governed.</p> <p>Governance ensures that:</p> <ul> <li>Tuning actions are intentional</li> <li>Changes are documented and reviewable</li> <li>Historical configurations are preserved</li> <li>Decisions can be traced to tuning state</li> </ul> <p>For governance principles, see Governance.</p>"},{"location":"optimization/tuning/#how-tuning-fits-into-the-electric-barometer-lifecycle","title":"How tuning fits into the Electric Barometer lifecycle","text":"<p>Within the Electric Barometer framework:</p> <ul> <li>Metrics measure behavior</li> <li>Evaluation produces structured outputs</li> <li>Optimization explores tradeoffs</li> <li>Tuning refines assumptions and policies</li> <li>Readiness contextualizes results</li> <li>Decisioning commits to actions</li> <li>Releases preserve reproducibility</li> </ul> <p>Tuning is the step where abstract evaluation becomes actionable alignment.</p>"},{"location":"optimization/tuning/#what-tuning-enables","title":"What tuning enables","text":"<p>Explicit tuning enables:</p> <ul> <li>Adaptation to changing operational contexts</li> <li>Clear articulation of tradeoffs</li> <li>Separation of measurement from preference</li> <li>Stable evolution of decision systems</li> <li>Alignment between technical evaluation and business intent</li> </ul> <p>Without tuning, systems drift. With tuning, they adapt deliberately.</p>"},{"location":"optimization/tuning/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Review Policies to understand policy structure</li> <li>Explore Cost Ratio Optimization for asymmetric weighting</li> <li>See Tie-Breaking for resolving ambiguity</li> <li>Follow Tune a Policy for hands-on guidance</li> <li>Consult Papers for formal treatments of tuning and optimization</li> </ul> <p>Tuning is not about finding perfection. It is about making tradeoffs explicit, governed, and aligned with the decisions Electric Barometer is designed to support.</p>"},{"location":"packages/eb-adapters/","title":"eb-adapters","text":"<p>eb-adapters provides a small, focused collection of adapter classes that wrap external forecasting and regression libraries so they can be used interchangeably inside the ElectricBarometer ecosystem.</p> <p>The goal of this package is interface normalization, not modeling innovation. Each adapter exposes a consistent, scikit-learn-like API:</p> <ul> <li><code>fit(X, y, sample_weight=None)</code> \u2192 returns <code>self</code></li> <li><code>predict(X)</code> \u2192 returns a one-dimensional numpy array</li> </ul> <p>This allows ElectricBarometer evaluation, selection, and cost-aware comparison utilities to operate uniformly across native scikit-learn estimators and non-sklearn forecasting engines.</p>"},{"location":"packages/eb-adapters/#what-this-package-does","title":"What this package does","text":"<ul> <li>Wraps external libraries (Prophet, statsmodels, CatBoost, LightGBM, etc.)</li> <li>Normalizes their interfaces to a common <code>.fit / .predict</code> contract</li> <li>Preserves initialization parameters so models are cloneable</li> <li>Keeps optional dependencies truly optional</li> </ul> <p>eb-adapters does not: - Define loss functions or evaluation metrics - Perform model selection or ranking - Implement forecasting logic itself</p> <p>Those responsibilities live in other ElectricBarometer packages.</p>"},{"location":"packages/eb-adapters/#adapter-philosophy","title":"Adapter philosophy","text":"<p>Adapters are intentionally thin:</p> <ul> <li>Minimal logic beyond input normalization</li> <li>No hidden state outside the wrapped model</li> <li>Clear, documented assumptions about how <code>X</code> and <code>y</code> are interpreted</li> <li>Safe to import even when optional dependencies are not installed</li> </ul> <p>If a model cannot reasonably conform to this contract, it likely does not belong in this package.</p>"},{"location":"packages/eb-adapters/#available-adapters","title":"Available adapters","text":"<p>Current adapters include:</p> <ul> <li>Prophet (univariate time-series forecasting)</li> <li>statsmodels ARIMA / SARIMAX</li> <li>CatBoost regressor</li> <li>LightGBM regressor</li> </ul> <p>Each adapter is documented in the API section and generated directly from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-adapters/#relationship-to-other-electricbarometer-packages","title":"Relationship to other ElectricBarometer packages","text":"<ul> <li>eb-metrics: defines cost-aware error and loss functions</li> <li>eb-evaluation: performs model evaluation, comparison, and selection</li> <li>eb-adapters: makes external models compatible with those workflows</li> </ul> <p>This separation keeps responsibilities clean and composable.</p>"},{"location":"packages/eb-adapters/#api-documentation","title":"API documentation","text":"<p>See the API section for detailed adapter reference material generated automatically from the source code.</p>"},{"location":"packages/eb-adapters/api/base/","title":"Base Adapter API","text":"<p>This section documents the core adapter interfaces and utilities used by eb-adapters.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-adapters/api/base/#base-adapter-package","title":"Base Adapter Package","text":""},{"location":"packages/eb-adapters/api/base/#eb_adapters.base","title":"<code>eb_adapters.base</code>","text":""},{"location":"packages/eb-adapters/api/base/#eb_adapters.base.BaseAdapter","title":"<code>BaseAdapter</code>","text":"<p>Minimal base class defining the adapter contract for ElectricBarometer.</p> <p>This class documents the expected interface for wrapping non-scikit-learn forecasting or regression engines so they can be evaluated and selected alongside native scikit-learn estimators.</p> <p>Subclasses are expected to present a scikit-learn-like API:</p> <ul> <li><code>fit(X, y, sample_weight=None)</code> returning <code>self</code></li> <li><code>predict(X)</code> returning a one-dimensional numpy array</li> </ul> <p>The ElectricBarometer engine does not distinguish between native scikit-learn estimators and adapters; it simply calls <code>fit</code> and <code>predict</code>. This base class serves as a clear, documented contract for adapter authors.</p>"},{"location":"packages/eb-adapters/api/base/#eb_adapters.base.BaseAdapter.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the underlying forecasting or regression model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix. For pure time-series models, this may be ignored or used only for alignment.</p> required <code>y</code> <code>ndarray</code> <p>One-dimensional target vector.</p> required <code>sample_weight</code> <code>ndarray | None</code> <p>Optional per-sample weights. Adapters may ignore this argument if weighting is not supported by the underlying model.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseAdapter</code> <p>The fitted adapter instance (self).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not override this method.</p>"},{"location":"packages/eb-adapters/api/base/#eb_adapters.base.BaseAdapter.predict","title":"<code>predict(X)</code>","text":"<p>Generate predictions from the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix used to generate predictions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>One-dimensional array of predictions.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not override this method.</p>"},{"location":"packages/eb-adapters/api/catboost/","title":"CatBoost Adapter API","text":"<p>This section documents the CatBoost adapter used to integrate <code>catboost.CatBoostRegressor</code> into the eb-adapters package.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-adapters/api/catboost/#catboost-adapter-module","title":"CatBoost Adapter Module","text":""},{"location":"packages/eb-adapters/api/catboost/#eb_adapters.catboost","title":"<code>eb_adapters.catboost</code>","text":""},{"location":"packages/eb-adapters/api/catboost/#eb_adapters.catboost.CatBoostAdapter","title":"<code>CatBoostAdapter</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for <code>catboost.CatBoostRegressor</code>.</p> <p>This adapter exposes a scikit-learn-like API and stores initialization parameters so the instance can be reconstructed by cloning utilities (for example, an internal <code>clone_model()</code> helper or <code>sklearn.base.clone</code>).</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Keyword arguments forwarded to <code>catboost.CatBoostRegressor</code>.</p> <code>{}</code> Notes <ul> <li><code>X</code> and <code>y</code> are treated as standard tabular regression inputs.</li> <li>If provided, <code>sample_weight</code> is passed through to CatBoost training.</li> <li>Training verbosity is disabled by default (<code>verbose=False</code>) unless the caller   supplies <code>verbose</code> explicitly.</li> <li>All initialization parameters are stored in <code>self.params</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = CatBoostAdapter(\n...     depth=4,\n...     learning_rate=0.1,\n...     iterations=200,\n...     loss_function=\"RMSE\",\n... )\n&gt;&gt;&gt; # X, y are numpy arrays (or array-like)\n&gt;&gt;&gt; # model.fit(X, y).predict(X)\n</code></pre>"},{"location":"packages/eb-adapters/api/catboost/#eb_adapters.catboost.CatBoostAdapter.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the underlying <code>catboost.CatBoostRegressor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target vector of shape (n_samples,).</p> required <code>sample_weight</code> <code>ndarray | None</code> <p>Optional per-sample weights of shape (n_samples,). If provided, this is forwarded to CatBoost training.</p> <code>None</code> <p>Returns:</p> Type Description <code>CatBoostAdapter</code> <p>The fitted adapter (self), allowing method chaining.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If CatBoost is not available or the internal model is not initialized.</p>"},{"location":"packages/eb-adapters/api/catboost/#eb_adapters.catboost.CatBoostAdapter.predict","title":"<code>predict(X)</code>","text":"<p>Predict using the fitted CatBoost model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values of shape (n_samples,).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the adapter has not been fit yet.</p>"},{"location":"packages/eb-adapters/api/catboost/#eb_adapters.catboost.CatBoostAdapter.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Return initialization parameters for cloning utilities.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>Included for scikit-learn compatibility. This adapter does not expose nested estimators, so the value does not change the output.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A shallow copy of the stored initialization parameters.</p>"},{"location":"packages/eb-adapters/api/catboost/#eb_adapters.catboost.CatBoostAdapter.set_params","title":"<code>set_params(**params)</code>","text":"<p>Update parameters and rebuild the underlying CatBoost model.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Keyword parameters to merge into the stored initialization parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CatBoostAdapter</code> <p>The updated adapter instance (self).</p> Notes <p>This method updates <code>self.params</code> and then re-instantiates <code>catboost.CatBoostRegressor</code> using the merged parameter set.</p>"},{"location":"packages/eb-adapters/api/lightgbm/","title":"LightGBM Adapter API","text":"<p>This section documents the LightGBM adapter used to integrate <code>lightgbm.LGBMRegressor</code> into the eb-adapters package.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-adapters/api/lightgbm/#lightgbm-adapter-module","title":"LightGBM Adapter Module","text":""},{"location":"packages/eb-adapters/api/lightgbm/#eb_adapters.lightgbm","title":"<code>eb_adapters.lightgbm</code>","text":""},{"location":"packages/eb-adapters/api/lightgbm/#eb_adapters.lightgbm.LightGBMRegressorAdapter","title":"<code>LightGBMRegressorAdapter</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for <code>lightgbm.LGBMRegressor</code>.</p> <p>This adapter exposes a scikit-learn-like API and stores initialization parameters so the instance can be reconstructed by cloning utilities (for example, an internal <code>clone_model()</code> helper or <code>sklearn.base.clone</code>).</p> <p>Parameters:</p> Name Type Description Default <code>**lgbm_params</code> <code>Any</code> <p>Keyword arguments forwarded to <code>lightgbm.LGBMRegressor</code>.</p> <code>{}</code> Notes <ul> <li><code>X</code> and <code>y</code> are treated as standard tabular regression inputs.</li> <li>If provided, <code>sample_weight</code> is passed through to LightGBM training.</li> <li>All initialization parameters are stored in <code>self.lgbm_params</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = LightGBMRegressorAdapter(\n...     n_estimators=200,\n...     learning_rate=0.05,\n...     max_depth=-1,\n... )\n&gt;&gt;&gt; # X, y are numpy arrays (or array-like)\n&gt;&gt;&gt; # model.fit(X, y).predict(X)\n</code></pre>"},{"location":"packages/eb-adapters/api/lightgbm/#eb_adapters.lightgbm.LightGBMRegressorAdapter.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the underlying <code>lightgbm.LGBMRegressor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target vector of shape (n_samples,).</p> required <code>sample_weight</code> <code>ndarray | None</code> <p>Optional per-sample weights of shape (n_samples,). If provided, this is forwarded to LightGBM training.</p> <code>None</code> <p>Returns:</p> Type Description <code>LightGBMRegressorAdapter</code> <p>The fitted adapter (self), allowing method chaining.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If LightGBM is not available or the internal model is not initialized.</p>"},{"location":"packages/eb-adapters/api/lightgbm/#eb_adapters.lightgbm.LightGBMRegressorAdapter.predict","title":"<code>predict(X)</code>","text":"<p>Predict using the fitted LightGBM model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values of shape (n_samples,).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the adapter has not been fit yet.</p>"},{"location":"packages/eb-adapters/api/lightgbm/#eb_adapters.lightgbm.LightGBMRegressorAdapter.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Return initialization parameters for cloning utilities.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>Included for scikit-learn compatibility. This adapter does not expose nested estimators, so the value does not change the output.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A shallow copy of the stored initialization parameters.</p>"},{"location":"packages/eb-adapters/api/lightgbm/#eb_adapters.lightgbm.LightGBMRegressorAdapter.set_params","title":"<code>set_params(**params)</code>","text":"<p>Update parameters and rebuild the underlying LightGBM model.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Keyword parameters to merge into the stored initialization parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LightGBMRegressorAdapter</code> <p>The updated adapter instance (self).</p> Notes <p>This method updates <code>self.lgbm_params</code> and then re-instantiates <code>lightgbm.LGBMRegressor</code> using the merged parameter set.</p>"},{"location":"packages/eb-adapters/api/prophet/","title":"Prophet Adapter API","text":"<p>This section documents the Prophet adapter used to integrate <code>prophet.Prophet</code> into the eb-adapters package.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-adapters/api/prophet/#prophet-adapter-module","title":"Prophet Adapter Module","text":""},{"location":"packages/eb-adapters/api/prophet/#eb_adapters.prophet","title":"<code>eb_adapters.prophet</code>","text":""},{"location":"packages/eb-adapters/api/prophet/#eb_adapters.prophet.ProphetAdapter","title":"<code>ProphetAdapter</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for <code>prophet.Prophet</code>.</p> <p>This adapter enables Prophet models to be used inside ElectricBarometer or other CWSL-based evaluation workflows by exposing a scikit-learn-like API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any | None</code> <p>Optional pre-configured <code>prophet.Prophet</code> instance. If None, a default Prophet model is constructed. If the <code>prophet</code> package is not installed, constructing a default model will raise <code>ImportError</code>.</p> <code>None</code> Notes <p>Input conventions:</p> <ul> <li><code>X</code> encodes the time index as either:</li> <li>shape (n_samples,) of datetime-like values, or</li> <li>shape (n_samples, n_features) where the first column is datetime-like</li> <li><code>y</code> is a one-dimensional array-like of numeric targets.</li> </ul> <p>At fit time, the adapter constructs a DataFrame with columns:</p> <ul> <li><code>ds</code>: timestamps parsed from <code>X</code></li> <li><code>y</code>: targets from <code>y</code></li> </ul> <p>and calls <code>Prophet.fit(df)</code>.</p> <p>At predict time, the adapter constructs a DataFrame with column <code>ds</code> and returns the <code>yhat</code> predictions as a one-dimensional numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prophet import Prophet\n&gt;&gt;&gt; base = Prophet()\n&gt;&gt;&gt; model = ProphetAdapter(model=base)\n&gt;&gt;&gt; # X contains datetimes, y contains numeric targets\n&gt;&gt;&gt; # model.fit(X, y).predict(X)\n</code></pre>"},{"location":"packages/eb-adapters/api/prophet/#eb_adapters.prophet.ProphetAdapter.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the underlying Prophet model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Time index values. Accepted forms are: - shape (n_samples,) of datetime-like values, or - shape (n_samples, n_features) where the first column is datetime-like</p> required <code>y</code> <code>ndarray</code> <p>Target vector of shape (n_samples,).</p> required <code>sample_weight</code> <code>ndarray | None</code> <p>Accepted for API compatibility but ignored by this adapter.</p> <code>None</code> <p>Returns:</p> Type Description <code>ProphetAdapter</code> <p>The fitted adapter (self), allowing method chaining.</p> Notes <p>This method imports pandas locally to avoid making pandas a hard dependency at module import time.</p>"},{"location":"packages/eb-adapters/api/prophet/#eb_adapters.prophet.ProphetAdapter.predict","title":"<code>predict(X)</code>","text":"<p>Predict using the fitted Prophet model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Time index values in the same format accepted by <code>fit</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values of shape (n_samples,), taken from Prophet's <code>yhat</code> output column.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the Prophet forecast output does not contain the <code>yhat</code> column.</p> Notes <p>This method imports pandas locally to avoid making pandas a hard dependency at module import time.</p>"},{"location":"packages/eb-adapters/api/statsmodels/","title":"Statsmodels Adapters API","text":"<p>This section documents adapters for integrating classic <code>statsmodels</code> time-series models into the eb-adapters package.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-adapters/api/statsmodels/#statsmodels-adapter-module","title":"Statsmodels Adapter Module","text":""},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels","title":"<code>eb_adapters.statsmodels</code>","text":""},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.SarimaxAdapter","title":"<code>SarimaxAdapter</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for <code>statsmodels</code> SARIMAX.</p> <p>This wrapper fits a univariate SARIMAX model on <code>y</code> and produces forecasts for <code>len(X)</code> steps ahead when <code>predict(X)</code> is called.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>tuple[int, int, int]</code> <p>ARIMA (p, d, q) order.</p> <code>(1, 0, 0)</code> <code>seasonal_order</code> <code>tuple[int, int, int, int]</code> <p>Seasonal (P, D, Q, s) order.</p> <code>(0, 0, 0, 0)</code> <code>trend</code> <code>str | None</code> <p>Trend specification forwarded to SARIMAX.</p> <code>None</code> <code>enforce_stationarity</code> <code>bool</code> <p>Whether to enforce stationarity in the SARIMAX model.</p> <code>True</code> <code>enforce_invertibility</code> <code>bool</code> <p>Whether to enforce invertibility in the SARIMAX model.</p> <code>True</code> Notes <ul> <li><code>X</code> is ignored during fitting. It is only used at prediction time to   determine the forecast horizon (<code>n_steps = len(X)</code>).</li> <li>This adapter stores initialization parameters via <code>get_params()</code> so cloning   utilities can reconstruct the adapter.</li> </ul>"},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.SarimaxAdapter.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit a univariate SARIMAX model on <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Ignored. Present for API compatibility.</p> required <code>y</code> <code>ndarray</code> <p>Target series of shape (n_samples,).</p> required <code>sample_weight</code> <code>ndarray | None</code> <p>Accepted for API compatibility but ignored by this adapter.</p> <code>None</code> <p>Returns:</p> Type Description <code>SarimaxAdapter</code> <p>The fitted adapter (self), allowing method chaining.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>statsmodels</code> is not installed.</p>"},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.SarimaxAdapter.predict","title":"<code>predict(X)</code>","text":"<p>Forecast <code>len(X)</code> steps ahead from the end of the training sample.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Array-like placeholder used to determine the forecast horizon.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Forecast values of shape (len(X),).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the adapter has not been fit yet.</p>"},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.SarimaxAdapter.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Return initialization parameters for cloning utilities.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>Included for scikit-learn compatibility.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Initialization parameters that can be passed back to <code>__init__</code>.</p>"},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.SarimaxAdapter.set_params","title":"<code>set_params(**params)</code>","text":"<p>Update adapter parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameters to set as attributes on the adapter instance.</p> <code>{}</code> <p>Returns:</p> Type Description <code>SarimaxAdapter</code> <p>The updated adapter instance (self).</p>"},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.ArimaAdapter","title":"<code>ArimaAdapter</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for <code>statsmodels</code> ARIMA.</p> <p>This wrapper fits a univariate ARIMA model on <code>y</code> and produces forecasts for <code>len(X)</code> steps ahead when <code>predict(X)</code> is called.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>tuple[int, int, int]</code> <p>ARIMA (p, d, q) order.</p> <code>(1, 0, 0)</code> <code>trend</code> <code>str | None</code> <p>Trend specification forwarded to <code>statsmodels.tsa.ARIMA</code>.</p> <code>None</code> Notes <ul> <li><code>X</code> is ignored during fitting. It is only used at prediction time to   determine the forecast horizon (<code>n_steps = len(X)</code>).</li> <li>This adapter stores initialization parameters via <code>get_params()</code> so cloning   utilities can reconstruct the adapter.</li> </ul>"},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.ArimaAdapter.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit a univariate ARIMA model on <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Ignored. Present for API compatibility.</p> required <code>y</code> <code>ndarray</code> <p>Target series of shape (n_samples,).</p> required <code>sample_weight</code> <code>ndarray | None</code> <p>Accepted for API compatibility but ignored by this adapter.</p> <code>None</code> <p>Returns:</p> Type Description <code>ArimaAdapter</code> <p>The fitted adapter (self), allowing method chaining.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>statsmodels</code> is not installed.</p>"},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.ArimaAdapter.predict","title":"<code>predict(X)</code>","text":"<p>Forecast <code>len(X)</code> steps ahead from the end of the training sample.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Array-like placeholder used to determine the forecast horizon.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Forecast values of shape (len(X),).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the adapter has not been fit yet.</p>"},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.ArimaAdapter.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Return initialization parameters for cloning utilities.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>Included for scikit-learn compatibility.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Initialization parameters that can be passed back to <code>__init__</code>.</p>"},{"location":"packages/eb-adapters/api/statsmodels/#eb_adapters.statsmodels.ArimaAdapter.set_params","title":"<code>set_params(**params)</code>","text":"<p>Update adapter parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameters to set as attributes on the adapter instance.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ArimaAdapter</code> <p>The updated adapter instance (self).</p>"},{"location":"packages/eb-contracts/","title":"eb-contracts","text":"<p><code>eb-contracts</code> defines the canonical data contracts, schemas, and structural interfaces used across the Electric Barometer ecosystem.</p> <p>This package establishes shared meaning and shape, ensuring that forecasts, evaluations, optimization outputs, and downstream systems interoperate consistently.</p>"},{"location":"packages/eb-contracts/#scope","title":"Scope","text":"<p>This package is responsible for:</p> <ul> <li>Defining canonical data structures and schemas</li> <li>Formalizing inputs and outputs exchanged between EB components</li> <li>Encoding shared assumptions about forecast results, costs, and runtime context</li> <li>Providing validation and migration utilities for contract evolution</li> </ul> <p>It intentionally avoids implementing metrics, models, optimization logic, or workflows.</p>"},{"location":"packages/eb-contracts/#contents","title":"Contents","text":"<ul> <li> <p>Core contracts   Canonical representations for forecasts, results, costs, and runtime context</p> </li> <li> <p>Validation utilities   Tools for checking contract conformance and structural integrity</p> </li> <li> <p>Migration support   Helpers for evolving contracts across versions</p> </li> </ul>"},{"location":"packages/eb-contracts/#api-reference","title":"API reference","text":"<ul> <li>Contract definitions</li> </ul>"},{"location":"packages/eb-contracts/api/definitions/","title":"Definitions","text":"<p>This section documents shared definitions and semantic primitives exposed by <code>eb-contracts</code>.</p> <p>These definitions provide canonical constants, enums, and structural building blocks used across Electric Barometer contracts and APIs to ensure consistent interpretation of values and fields.</p>"},{"location":"packages/eb-contracts/api/definitions/#eb_contracts.definitions","title":"<code>eb_contracts.definitions</code>","text":"<p>Canonical definitions for forecasting and panel-based evaluation.</p> <p>This package provides shared vocabulary, conventions, and semantic declarations used across contract schemas and evaluation workflows.</p> <p>It is intentionally lightweight and declarative: - No validation logic - No business rules - No framework- or industry-specific assumptions</p>"},{"location":"packages/eb-contracts/api/definitions/#eb_contracts.definitions.SemanticProfile","title":"<code>SemanticProfile</code>  <code>dataclass</code>","text":"<p>A named set of semantic expectations.</p> <p>Profiles are descriptive and may be used by validators to decide which checks to enforce. They do not perform validation themselves.</p>"},{"location":"packages/eb-contracts/api/definitions/#eb_contracts.definitions.UnitSpec","title":"<code>UnitSpec</code>  <code>dataclass</code>","text":"<p>Declarative unit specification for a set of values.</p> <p>Attributes:     units:         A string describing the measurement units (e.g., \"units\", \"dollars\", \"minutes\").     scale:         A multiplicative scale factor applied uniformly to values. Defaults to 1.0.     policy:         Whether units are expected to be declared (\"declared\") or may be unknown (\"unknown\").         This is descriptive; enforcement belongs in contract validators or higher layers.</p>"},{"location":"packages/eb-contracts/api/definitions/#eb_contracts.definitions.required_columns_point","title":"<code>required_columns_point()</code>","text":"<p>Required columns for the canonical panel point forecast representation.</p>"},{"location":"packages/eb-contracts/api/definitions/#eb_contracts.definitions.required_columns_quantile","title":"<code>required_columns_quantile()</code>","text":"<p>Required columns for the canonical panel quantile forecast representation.</p>"},{"location":"packages/eb-contracts/api/migrate_forecast/","title":"Forecast migration","text":"<p>This section documents utilities for migrating forecast data between contract versions in <code>eb-contracts</code>.</p> <p>These helpers support forward-compatible evolution of forecast schemas by transforming historical or legacy forecast structures into their current canonical representations.</p>"},{"location":"packages/eb-contracts/api/migrate_forecast/#eb_contracts.api.migrate_forecast","title":"<code>eb_contracts.api.migrate_forecast</code>","text":"<p>Forecast migration helpers.</p> <p>This module contains explicit utilities for adapting \"in the wild\" forecast frames into EB contract artifacts.</p> <p>Migration is intentionally explicit: - You provide column mappings. - The output is a validated contract artifact (unless validation mode is off).</p>"},{"location":"packages/eb-contracts/api/migrate_forecast/#eb_contracts.api.migrate_forecast.PanelPointColumns","title":"<code>PanelPointColumns</code>  <code>dataclass</code>","text":"<p>Column mapping for point forecasts.</p>"},{"location":"packages/eb-contracts/api/migrate_forecast/#eb_contracts.api.migrate_forecast.PanelQuantileColumns","title":"<code>PanelQuantileColumns</code>  <code>dataclass</code>","text":"<p>Column mapping for quantile forecasts.</p>"},{"location":"packages/eb-contracts/api/migrate_forecast/#eb_contracts.api.migrate_forecast.to_panel_point_v1","title":"<code>to_panel_point_v1(frame, *, columns)</code>","text":"<p>Adapt a frame into the PanelPointForecastV1 contract.</p>"},{"location":"packages/eb-contracts/api/migrate_forecast/#eb_contracts.api.migrate_forecast.to_panel_quantile_v1","title":"<code>to_panel_quantile_v1(frame, *, columns)</code>","text":"<p>Adapt a frame into the PanelQuantileForecastV1 contract.</p>"},{"location":"packages/eb-contracts/api/migrate_forecast/#eb_contracts.api.migrate_forecast.to_panel_point_result_v1","title":"<code>to_panel_point_result_v1(frame, *, columns)</code>","text":"<p>Adapt a frame into the PanelPointResultV1 contract.</p>"},{"location":"packages/eb-contracts/api/validate/","title":"Forecast migration","text":"<p>This section documents utilities for migrating forecast data between contract versions in <code>eb-contracts</code>.</p> <p>These helpers support forward-compatible evolution of forecast schemas by transforming historical or legacy forecast structures into their current canonical representations.</p>"},{"location":"packages/eb-contracts/api/validate/#eb_contracts.api.migrate_forecast","title":"<code>eb_contracts.api.migrate_forecast</code>","text":"<p>Forecast migration helpers.</p> <p>This module contains explicit utilities for adapting \"in the wild\" forecast frames into EB contract artifacts.</p> <p>Migration is intentionally explicit: - You provide column mappings. - The output is a validated contract artifact (unless validation mode is off).</p>"},{"location":"packages/eb-contracts/api/validate/#eb_contracts.api.migrate_forecast.PanelPointColumns","title":"<code>PanelPointColumns</code>  <code>dataclass</code>","text":"<p>Column mapping for point forecasts.</p>"},{"location":"packages/eb-contracts/api/validate/#eb_contracts.api.migrate_forecast.PanelQuantileColumns","title":"<code>PanelQuantileColumns</code>  <code>dataclass</code>","text":"<p>Column mapping for quantile forecasts.</p>"},{"location":"packages/eb-contracts/api/validate/#eb_contracts.api.migrate_forecast.to_panel_point_v1","title":"<code>to_panel_point_v1(frame, *, columns)</code>","text":"<p>Adapt a frame into the PanelPointForecastV1 contract.</p>"},{"location":"packages/eb-contracts/api/validate/#eb_contracts.api.migrate_forecast.to_panel_quantile_v1","title":"<code>to_panel_quantile_v1(frame, *, columns)</code>","text":"<p>Adapt a frame into the PanelQuantileForecastV1 contract.</p>"},{"location":"packages/eb-contracts/api/validate/#eb_contracts.api.migrate_forecast.to_panel_point_result_v1","title":"<code>to_panel_point_result_v1(frame, *, columns)</code>","text":"<p>Adapt a frame into the PanelPointResultV1 contract.</p>"},{"location":"packages/eb-evaluation/","title":"eb-evaluation","text":"<p><code>eb-evaluation</code> provides evaluation, diagnostics, and model-selection utilities used to assess forecast quality, readiness, and decision impact within the Electric Barometer ecosystem.</p> <p>This package focuses on post-forecast analysis: measuring performance, identifying risk, validating assumptions, and supporting model comparison and selection.</p>"},{"location":"packages/eb-evaluation/#scope","title":"Scope","text":"<p>This package is responsible for:</p> <ul> <li>Evaluating forecast outputs using cost-aware and readiness-oriented metrics</li> <li>Producing diagnostic signals for data quality, service risk, and governance</li> <li>Supporting model comparison and automated model selection</li> <li>Structuring evaluation results for downstream reporting and decision workflows</li> </ul> <p>It intentionally avoids model training, optimization policy definition, or data contract enforcement.</p>"},{"location":"packages/eb-evaluation/#contents","title":"Contents","text":"<ul> <li> <p>Evaluation utilities   Functions and helpers for computing evaluation metrics and summaries</p> </li> <li> <p>Diagnostics   Tools for assessing data quality, forecast plausibility, and governance signals</p> </li> <li> <p>Model selection   Utilities for comparing models and selecting candidates based on evaluation criteria</p> </li> </ul>"},{"location":"packages/eb-evaluation/#api-reference","title":"API reference","text":"<ul> <li>Evaluation APIs</li> </ul>"},{"location":"packages/eb-evaluation/api/","title":"API reference","text":"<p>This section documents the public evaluation APIs provided by <code>eb-evaluation</code>.</p> <p>The pages below expose evaluation, diagnostics, and model-selection utilities via rendered docstrings. These APIs operate on forecast outputs and related artifacts to assess performance, readiness, and decision impact.</p>"},{"location":"packages/eb-evaluation/api/#modules","title":"Modules","text":"<ul> <li>Adjustment</li> <li>DataFrame evaluators</li> <li>Diagnostics</li> <li>Model selection</li> <li>Utilities</li> </ul>"},{"location":"packages/eb-evaluation/api/adjustment/ral/","title":"Readiness adjustment evaluation","text":"<p>This section documents evaluation utilities related to the readiness adjustment layer (RAL) provided by <code>eb-evaluation</code>.</p> <p>These utilities assess the impact of readiness adjustments on forecast outputs and help quantify how RAL policies affect service risk and decision outcomes.</p>"},{"location":"packages/eb-evaluation/api/adjustment/ral/#eb_evaluation.adjustment.ral","title":"<code>eb_evaluation.adjustment.ral</code>","text":"<p>Readiness Adjustment Layer (RAL): deterministic fit + apply in eb-evaluation.</p> <p>This module implements a transparent post-processing step that converts a baseline forecast into an operationally conservative readiness forecast via a learned uplift.</p> Responsibilities <ul> <li>Fit a simple uplift policy via grid search that minimizes CWSL.</li> <li>Apply learned uplift factors to new data (global or segmented).</li> <li>Provide before/after diagnostics for auditability.</li> </ul>"},{"location":"packages/eb-evaluation/api/adjustment/ral/#eb_evaluation.adjustment.ral.ReadinessAdjustmentLayer","title":"<code>ReadinessAdjustmentLayer</code>","text":"<p>Readiness Adjustment Layer (RAL) for operational forecast uplift.</p>"},{"location":"packages/eb-evaluation/api/adjustment/ral/#eb_evaluation.adjustment.ral.ReadinessAdjustmentLayer.transform","title":"<code>transform(df, *, forecast_col, output_col='readiness_forecast', segment_cols=None)</code>","text":"<p>Apply learned uplift factors to produce readiness forecasts.</p> <p>Test expectation: - If called before explicit fit(), this should still work for global uplift by   implicitly fitting on the provided dataframe (requires an actual column), but only   when costs (cu/co) are set.</p>"},{"location":"packages/eb-evaluation/api/dataframe/entity/","title":"Entity-level evaluation","text":"<p>This section documents evaluation utilities for computing metrics and diagnostics at the individual entity level.</p> <p>Entity-level evaluation supports analysis of forecast performance and readiness for a single unit (e.g. store, item, or asset) over time.</p>"},{"location":"packages/eb-evaluation/api/dataframe/entity/#eb_evaluation.dataframe.entity","title":"<code>eb_evaluation.dataframe.entity</code>","text":"<p>Entity-level evaluation utilities (DataFrame helpers).</p> <p>This module contains DataFrame-oriented evaluation helpers that operate on panel data: entity-by-interval observations containing actuals and forecasts.</p> <p>The primary helper evaluates each entity using entity-specific cost asymmetry parameters (cost ratios) that are typically estimated upstream (for example, via a balance-based estimator). The result is one row per entity containing cost-weighted and service-oriented Electric Barometer metrics, plus familiar symmetric error metrics.</p>"},{"location":"packages/eb-evaluation/api/dataframe/entity/#eb_evaluation.dataframe.entity.evaluate_panel_with_entity_R","title":"<code>evaluate_panel_with_entity_R(df, entity_R, *, entity_col='entity', y_true_col='actual_qty', y_pred_col='forecast_qty', R_col='R', co_col='co', tau=2.0, sample_weight_col=None)</code>","text":"<p>Evaluate an entity-interval panel using entity-level cost ratios.</p> <p>This helper evaluates each entity slice of a panel using entity-specific cost asymmetry parameters. It is designed to pair naturally with a table that provides, for each entity, a cost ratio:</p> \\[     R_e = \\frac{c_{u,e}}{c_o} \\] <p>and an overbuild cost coefficient \\(c_o\\). For each entity \\(e\\), the implied shortfall (underbuild) cost coefficient is:</p> \\[     c_{u,e} = R_e \\cdot c_o \\] Evaluation flow <p>For each entity:</p> <ol> <li>Join the entity-level values \\((R_e, c_o)\\) onto all intervals for that entity.</li> <li>Construct per-row arrays \\(c_u\\) and \\(c_o\\) that are constant within the entity slice.</li> <li>Compute the EB metric suite (CWSL, NSL, UD, HR@\\(\\tau\\), FRS) using the entity-specific    cost parameters, plus common symmetric metrics.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Panel of interval-level data containing at least:</p> <ul> <li><code>entity_col</code> (entity identifier)</li> <li><code>y_true_col</code> (actuals)</li> <li><code>y_pred_col</code> (forecasts)</li> </ul> <p>If <code>sample_weight_col</code> is provided, it must also exist in <code>df</code>.</p> required <code>entity_R</code> <code>DataFrame</code> <p>Table with one row per entity containing at least:</p> <ul> <li><code>entity_col</code> (entity identifier)</li> <li><code>R_col</code> (cost ratio \\(R_e\\))</li> <li><code>co_col</code> (overbuild cost coefficient \\(c_o\\))</li> </ul> <p>Typically produced by an upstream calibration step (for example, an entity ratio estimator).</p> required <code>entity_col</code> <code>str</code> <p>Column identifying the entity (for example, <code>\"store_id\"</code>, <code>\"sku\"</code>, <code>\"location\"</code>).</p> <code>\"entity\"</code> <code>y_true_col</code> <code>str</code> <p>Column containing realized demand / actual values.</p> <code>\"actual_qty\"</code> <code>y_pred_col</code> <code>str</code> <p>Column containing baseline forecast values.</p> <code>\"forecast_qty\"</code> <code>R_col</code> <code>str</code> <p>Column in <code>entity_R</code> containing the cost ratio \\(R_e\\).</p> <code>\"R\"</code> <code>co_col</code> <code>str</code> <p>Column in <code>entity_R</code> containing the overbuild cost coefficient \\(c_o\\).</p> <code>\"co\"</code> <code>tau</code> <code>float</code> <p>Absolute-error tolerance parameter for the hit-rate metric HR@\\(\\tau\\).</p> <code>2.0</code> <code>sample_weight_col</code> <code>str | None</code> <p>Optional column in <code>df</code> of non-negative sample weights.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>One row per entity with columns:</p> <ul> <li><code>entity_col</code> : entity identifier</li> <li><code>R</code> : entity ratio \\(R_e\\)</li> <li><code>cu</code> : implied underbuild cost coefficient \\(c_{u,e} = R_e \\cdot c_o\\)</li> <li><code>co</code> : overbuild cost coefficient \\(c_o\\)</li> <li><code>CWSL</code> : cost-weighted service loss</li> <li><code>NSL</code> : no-shortfall level (service-oriented)</li> <li><code>UD</code> : underbuild depth</li> <li><code>wMAPE</code> : weighted mean absolute percentage error (per eb_metrics definition)</li> <li><code>HR@tau</code> : hit rate within tolerance \\(\\tau\\)</li> <li><code>FRS</code> : forecast readiness score</li> <li><code>MAE</code> : mean absolute error</li> <li><code>RMSE</code> : root mean squared error</li> <li><code>MAPE</code> : mean absolute percentage error</li> </ul> <p>If a metric is undefined for a given entity slice (for example, due to a metric-specific validation failure), that metric value is returned as NaN for that entity.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required columns are missing from <code>df</code> or <code>entity_R</code>.</p> <code>ValueError</code> <p>If the merge between <code>df</code> and <code>entity_R</code> produces no rows (no overlapping entities).</p> Notes <ul> <li>The join uses an inner merge on <code>entity_col</code>. Entities present in <code>df</code> but missing   from <code>entity_R</code> are dropped. This is intentional: evaluation requires cost parameters.</li> <li>Cost arrays are constructed per entity as constants, enabling vectorized evaluation calls.</li> <li>Some metrics in :mod:<code>eb_metrics.metrics</code> may not accept sample weights; this function   calls those metrics unweighted to match their signatures.</li> </ul>"},{"location":"packages/eb-evaluation/api/dataframe/group/","title":"Group-level evaluation","text":"<p>This section documents evaluation utilities for computing metrics and diagnostics across grouped entities.</p> <p>Group-level evaluation supports aggregation and comparison across cohorts, segments, or experimental groups.</p>"},{"location":"packages/eb-evaluation/api/dataframe/group/#eb_evaluation.dataframe.group","title":"<code>eb_evaluation.dataframe.group</code>","text":"<p>Group-level evaluation (DataFrame utilities).</p> <p>This module provides helpers for evaluating forecasts on grouped subsets of a DataFrame (e.g., by store, item, daypart, region). It orchestrates grouping, parameter handling, and tabular output while delegating metric definitions to <code>eb_metrics.metrics</code>.</p> <p>The primary entry point is <code>evaluate_groups_df</code>, which computes the Electric Barometer metric suite (CWSL, NSL, UD, HR@tau, FRS) plus common symmetric diagnostics (wMAPE, MAE, RMSE, MAPE) for each group.</p>"},{"location":"packages/eb-evaluation/api/dataframe/group/#eb_evaluation.dataframe.group.evaluate_groups_df","title":"<code>evaluate_groups_df(df, group_cols, *, actual_col='actual_qty', forecast_col='forecast_qty', cu=2.0, co=1.0, tau=2.0, sample_weight_col=None)</code>","text":"<p>Evaluate core EB metrics per group from a DataFrame.</p> <p>For each group defined by <code>group_cols</code>, this helper computes:</p> <ul> <li>CWSL</li> <li>NSL</li> <li>UD</li> <li>wMAPE</li> <li>HR@tau</li> <li>FRS</li> <li>MAE</li> <li>RMSE</li> <li>MAPE</li> </ul> <p>Cost parameters can be provided either globally (scalar) or per-row (column name).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input data containing actuals, forecasts, and grouping columns.</p> required <code>group_cols</code> <code>list[str]</code> <p>Column names used to define groups (e.g., <code>[\"store_id\", \"item_id\"]</code>).</p> required <code>actual_col</code> <code>str</code> <p>Name of the column containing actual demand values.</p> <code>\"actual_qty\"</code> <code>forecast_col</code> <code>str</code> <p>Name of the column containing forecast values.</p> <code>\"forecast_qty\"</code> <code>cu</code> <code>float | str</code> <p>Underbuild (shortfall) cost coefficient.</p> <ul> <li>If <code>float</code>: scalar cost applied uniformly across all rows/groups.</li> <li>If <code>str</code>: name of a column in <code>df</code> containing per-row underbuild costs.</li> </ul> <code>2.0</code> <code>co</code> <code>float | str</code> <p>Overbuild (excess) cost coefficient.</p> <ul> <li>If <code>float</code>: scalar cost applied uniformly across all rows/groups.</li> <li>If <code>str</code>: name of a column in <code>df</code> containing per-row overbuild costs.</li> </ul> <code>1.0</code> <code>tau</code> <code>float</code> <p>Absolute-error tolerance parameter for the hit-rate metric HR@tau.</p> <code>2.0</code> <code>sample_weight_col</code> <code>str | None</code> <p>Optional column name containing non-negative sample weights per row. If provided, weights are passed into metrics that accept a <code>sample_weight</code> argument.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with one row per group and columns::</p> <pre><code>group_cols + [\"CWSL\", \"NSL\", \"UD\", \"wMAPE\", \"HR@tau\", \"FRS\", \"MAE\", \"RMSE\", \"MAPE\"].\n</code></pre> <p>If a metric is undefined for a particular group (e.g., invalid values for that group), the corresponding value is returned as NaN rather than raising an error for the entire evaluation.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required columns are missing from <code>df</code>.</p> <code>ValueError</code> <p>If <code>df</code> is empty, or if <code>group_cols</code> is empty.</p> Notes <ul> <li><code>wmape</code> in <code>eb_metrics.metrics</code> does not take <code>sample_weight</code>, so it is   computed unweighted here.</li> <li>Symmetric diagnostics (MAE, RMSE, MAPE) are computed unweighted to match the   current <code>eb_metrics</code> signatures.</li> <li>Metrics are evaluated group-by-group; a failure in one group does not prevent   evaluation of other groups.</li> </ul>"},{"location":"packages/eb-evaluation/api/dataframe/hierarchy/","title":"Hierarchical evaluation","text":"<p>This section documents evaluation utilities for hierarchical data structures.</p> <p>Hierarchical evaluation supports rollups and drill-downs across multi-level structures such as region \u2192 market \u2192 entity.</p>"},{"location":"packages/eb-evaluation/api/dataframe/hierarchy/#eb_evaluation.dataframe.hierarchy","title":"<code>eb_evaluation.dataframe.hierarchy</code>","text":"<p>Hierarchy-level evaluation (DataFrame utilities).</p> <p>This module provides a convenience helper for evaluating forecasts at multiple levels of a grouping hierarchy (e.g., overall, by store, by item, by store x item).</p> <p>It returns a dictionary mapping each hierarchy level name to a DataFrame of metrics for that level. Metric definitions are delegated to <code>eb_metrics.metrics</code>; this module focuses on grouping orchestration and tabular output suitable for reporting.</p> <p>The EB metric suite here includes CWSL and related service/readiness diagnostics (NSL, UD, HR@tau, FRS) as well as wMAPE.</p>"},{"location":"packages/eb-evaluation/api/dataframe/hierarchy/#eb_evaluation.dataframe.hierarchy.evaluate_hierarchy_df","title":"<code>evaluate_hierarchy_df(df, levels, actual_col, forecast_col, cu, co, tau=None)</code>","text":"<p>Evaluate EB metrics at multiple hierarchy levels.</p> <p>This helper evaluates forecast performance across several grouping levels, each defined by a list of column names. For each level, it computes:</p> <ul> <li>CWSL</li> <li>NSL</li> <li>UD</li> <li>wMAPE</li> <li>HR@tau (optional)</li> <li>FRS</li> </ul> <p>where each metric is computed over the subset (group) implied by that level.</p> <p>The <code>levels</code> mapping accepts an empty list to represent the overall aggregate, e.g. <code>{\"overall\": []}</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing at minimum <code>actual_col</code> and <code>forecast_col</code> plus any grouping columns referenced by <code>levels</code>.</p> required <code>levels</code> <code>dict[str, Sequence[str]]</code> <p>Mapping from level name to the column names used to group at that level.</p> <p>Example:</p> <p>levels = { ...     \"overall\": [], ...     \"by_store\": [\"store_id\"], ...     \"by_item\": [\"item_id\"], ...     \"by_store_item\": [\"store_id\", \"item_id\"], ... }</p> <p>An empty sequence means evaluate the entire DataFrame as a single group.</p> required <code>actual_col</code> <code>str</code> <p>Column name for actual demand / realized values.</p> required <code>forecast_col</code> <code>str</code> <p>Column name for forecast values.</p> required <code>cu</code> <p>Underbuild (shortfall) cost coefficient passed through to <code>eb_metrics.metrics.cwsl</code> and <code>eb_metrics.metrics.frs</code>.</p> required <code>co</code> <p>Overbuild (excess) cost coefficient passed through to <code>eb_metrics.metrics.cwsl</code> and <code>eb_metrics.metrics.frs</code>.</p> required <code>tau</code> <code>float | None</code> <p>Tolerance parameter for HR@tau. If <code>None</code>, HR@tau is omitted from outputs.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>Dictionary mapping level name to a DataFrame of metrics for that level.</p> <p>Each DataFrame includes:</p> <ul> <li>the level's grouping columns (if any), first</li> <li><code>n_intervals</code> : number of rows evaluated in that group</li> <li><code>total_demand</code> : sum of <code>actual_col</code> for that group</li> <li><code>cwsl</code> : cost-weighted service loss</li> <li><code>nsl</code> : no-shortage level</li> <li><code>ud</code> : underbuild deviation</li> <li><code>wmape</code> : weighted mean absolute percentage error (per eb_metrics definition)</li> <li><code>hr_at_tau</code> : hit rate within tolerance tau (only if <code>tau</code> is provided)</li> <li><code>frs</code> : forecast readiness score</li> </ul> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required columns are missing from <code>df</code> (actual/forecast and any columns referenced in <code>levels</code>).</p> <code>ValueError</code> <p>If <code>df</code> is empty, or if <code>levels</code> is empty.</p> Notes <ul> <li>This function does not catch per-group metric exceptions. If eb_metrics raises a   <code>ValueError</code> for a specific group (e.g., invalid inputs), that error will propagate.   If you want best-effort reporting (NaN on failure), wrap metric calls similarly to   <code>evaluate_groups_df</code>.</li> <li><code>groupby(..., dropna=False)</code> is used so that missing values in grouping keys form explicit   groups, which is often desirable in operational reporting.</li> </ul>"},{"location":"packages/eb-evaluation/api/dataframe/panel/","title":"Panel evaluation","text":"<p>This section documents evaluation utilities for panel-style data.</p> <p>Panel evaluation supports analysis of entity-by-time datasets commonly produced by forecasting and readiness pipelines.</p>"},{"location":"packages/eb-evaluation/api/dataframe/panel/#eb_evaluation.dataframe.panel","title":"<code>eb_evaluation.dataframe.panel</code>","text":"<p>Panel-style evaluation output (DataFrame utilities).</p> <p>This module provides a convenience wrapper that evaluates a DataFrame at multiple hierarchy levels and returns a long-form (tidy) panel suitable for reporting, plotting, and downstream aggregation.</p> <p>The implementation delegates the core computation to <code>eb_evaluation.dataframe.hierarchy.evaluate_hierarchy_df</code> and then reshapes the wide per-level outputs into a single stacked table with:</p> <ul> <li>a <code>level</code> column (which hierarchy level produced the row)</li> <li>optional grouping key columns (depending on the level)</li> <li><code>metric</code> / <code>value</code> columns for tidy analysis</li> </ul>"},{"location":"packages/eb-evaluation/api/dataframe/panel/#eb_evaluation.dataframe.panel.evaluate_panel_df","title":"<code>evaluate_panel_df(df, levels, actual_col, forecast_col, cu, co, tau=None)</code>","text":"<p>Evaluate metrics at multiple levels and return a long-form panel DataFrame.</p> <p>This is a convenience wrapper around <code>eb_evaluation.dataframe.hierarchy.evaluate_hierarchy_df</code> that:</p> <ol> <li>Computes a wide metrics DataFrame per hierarchy level.</li> <li>Stacks them into a single table with a <code>level</code> column.</li> <li>Melts metrics into <code>metric</code> / <code>value</code> pairs.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing at least <code>actual_col</code> and <code>forecast_col</code> plus any grouping columns referenced in <code>levels</code>.</p> required <code>levels</code> <code>dict[str, Sequence[str]]</code> <p>Mapping of level name to the column names used to group at that level.</p> <p>Example:</p> <p>levels = { ...     \"overall\": [], ...     \"by_store\": [\"store_id\"], ...     \"by_item\": [\"item_id\"], ...     \"by_store_item\": [\"store_id\", \"item_id\"], ... }</p> required <code>actual_col</code> <code>str</code> <p>Column name for actual demand / realized values.</p> required <code>forecast_col</code> <code>str</code> <p>Column name for forecast values.</p> required <code>cu</code> <p>Underbuild (shortfall) cost coefficient passed through to CWSL/FRS evaluations.</p> required <code>co</code> <p>Overbuild (excess) cost coefficient passed through to CWSL/FRS evaluations.</p> required <code>tau</code> <code>float | None</code> <p>Tolerance parameter for HR@tau. If <code>None</code>, HR@tau is omitted.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Long-form (tidy) panel with columns:</p> <ul> <li><code>level</code> : hierarchy level name</li> <li><code>&lt;group cols&gt;</code> : the grouping keys for that level (may be empty for overall)</li> <li><code>metric</code> : metric name</li> <li><code>value</code> : metric value</li> </ul> <p>Each row corresponds to a single metric evaluated at a specific level/group.</p> Notes <ul> <li>The set of metric columns is derived from the outputs of   <code>eb_evaluation.dataframe.hierarchy.evaluate_hierarchy_df</code>. Only metrics present in   the combined wide table are melted.</li> <li>Grouping key columns vary by level. The returned panel includes the union of all   grouping key columns across levels; levels that do not use a given key will have NaN   in that column.</li> </ul>"},{"location":"packages/eb-evaluation/api/dataframe/single/","title":"Single-series evaluation","text":"<p>This section documents evaluation utilities for single time series.</p> <p>Single-series evaluation supports focused analysis of forecast accuracy and diagnostics for an individual series without cross-entity aggregation.</p>"},{"location":"packages/eb-evaluation/api/dataframe/single/#eb_evaluation.dataframe.single","title":"<code>eb_evaluation.dataframe.single</code>","text":"<p>Single-slice CWSL evaluation (DataFrame utilities).</p> <p>This module provides a lightweight DataFrame wrapper around <code>eb_metrics.metrics.cwsl</code> for computing Cost-Weighted Service Loss (CWSL) on a single slice of data (i.e., the entire DataFrame provided).</p> <p>It supports both scalar costs and per-row cost columns for asymmetric cost evaluation.</p>"},{"location":"packages/eb-evaluation/api/dataframe/single/#eb_evaluation.dataframe.single.compute_cwsl_df","title":"<code>compute_cwsl_df(df, y_true_col, y_pred_col, cu, co, sample_weight_col=None)</code>","text":"<p>Compute CWSL from a DataFrame.</p> <p>This is a convenience wrapper around <code>eb_metrics.metrics.cwsl</code> that accepts a pandas DataFrame and column names.</p> <p>Costs can be specified either as scalars or as per-row columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input table containing at least the actual and forecast columns, and optionally cost/weight columns.</p> required <code>y_true_col</code> <code>str</code> <p>Name of the column containing actual demand values.</p> required <code>y_pred_col</code> <code>str</code> <p>Name of the column containing forecast values.</p> required <code>cu</code> <code>float | str</code> <p>Underbuild (shortfall) cost coefficient.</p> <ul> <li>If <code>float</code>: scalar cost applied uniformly across all rows.</li> <li>If <code>str</code>: name of a column in <code>df</code> containing per-row underbuild costs.</li> </ul> required <code>co</code> <code>float | str</code> <p>Overbuild (excess) cost coefficient.</p> <ul> <li>If <code>float</code>: scalar cost applied uniformly across all rows.</li> <li>If <code>str</code>: name of a column in <code>df</code> containing per-row overbuild costs.</li> </ul> required <code>sample_weight_col</code> <code>str | None</code> <p>Optional column name containing non-negative sample weights per row. If <code>None</code>, all rows are weighted equally.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The Cost-Weighted Service Loss (CWSL) value for the provided DataFrame slice.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If any required columns are missing.</p> <code>ValueError</code> <p>If the underlying <code>eb_metrics.metrics.cwsl</code> raises due to invalid values.</p> Notes <p>This function performs minimal validation and delegates metric validation to <code>eb_metrics.metrics.cwsl</code>.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/dqc/","title":"Single-series evaluation","text":"<p>This section documents evaluation utilities for single time series.</p> <p>Single-series evaluation supports focused analysis of forecast accuracy and diagnostics for an individual series without cross-entity aggregation.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/dqc/#eb_evaluation.dataframe.single","title":"<code>eb_evaluation.dataframe.single</code>","text":"<p>Single-slice CWSL evaluation (DataFrame utilities).</p> <p>This module provides a lightweight DataFrame wrapper around <code>eb_metrics.metrics.cwsl</code> for computing Cost-Weighted Service Loss (CWSL) on a single slice of data (i.e., the entire DataFrame provided).</p> <p>It supports both scalar costs and per-row cost columns for asymmetric cost evaluation.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/dqc/#eb_evaluation.dataframe.single.compute_cwsl_df","title":"<code>compute_cwsl_df(df, y_true_col, y_pred_col, cu, co, sample_weight_col=None)</code>","text":"<p>Compute CWSL from a DataFrame.</p> <p>This is a convenience wrapper around <code>eb_metrics.metrics.cwsl</code> that accepts a pandas DataFrame and column names.</p> <p>Costs can be specified either as scalars or as per-row columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input table containing at least the actual and forecast columns, and optionally cost/weight columns.</p> required <code>y_true_col</code> <code>str</code> <p>Name of the column containing actual demand values.</p> required <code>y_pred_col</code> <code>str</code> <p>Name of the column containing forecast values.</p> required <code>cu</code> <code>float | str</code> <p>Underbuild (shortfall) cost coefficient.</p> <ul> <li>If <code>float</code>: scalar cost applied uniformly across all rows.</li> <li>If <code>str</code>: name of a column in <code>df</code> containing per-row underbuild costs.</li> </ul> required <code>co</code> <code>float | str</code> <p>Overbuild (excess) cost coefficient.</p> <ul> <li>If <code>float</code>: scalar cost applied uniformly across all rows.</li> <li>If <code>str</code>: name of a column in <code>df</code> containing per-row overbuild costs.</li> </ul> required <code>sample_weight_col</code> <code>str | None</code> <p>Optional column name containing non-negative sample weights per row. If <code>None</code>, all rows are weighted equally.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The Cost-Weighted Service Loss (CWSL) value for the provided DataFrame slice.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If any required columns are missing.</p> <code>ValueError</code> <p>If the underlying <code>eb_metrics.metrics.cwsl</code> raises due to invalid values.</p> Notes <p>This function performs minimal validation and delegates metric validation to <code>eb_metrics.metrics.cwsl</code>.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/fpc/","title":"Forecast plausibility checks (FPC)","text":"<p>This section documents forecast plausibility check utilities provided by <code>eb-evaluation</code>.</p> <p>FPC utilities evaluate whether forecast outputs fall within reasonable and operationally plausible bounds.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/fpc/#eb_evaluation.diagnostics.fpc","title":"<code>eb_evaluation.diagnostics.fpc</code>","text":"<p>Forecast Primitive Compatibility (FPC) diagnostics.</p> <p>This module defines a derived, auditable classification that diagnoses whether a given forecast primitive (typically point forecasts with scale-based adjustment) is structurally compatible with the observed demand process.</p> <p>FPC is not a performance metric and is not intended as an optimization objective. It consumes FRF diagnostics (e.g., NSL, UD, HR@\u03c4, CWSL response) and produces a small set of interpretable compatibility states used for governance and policy gating.</p> <p>Design goals: - Deterministic and auditable (explicit signals in / signals out) - Metric-agnostic (does not require a specific modeling approach) - Lightweight dependencies (NumPy optional; Pandas optional)</p>"},{"location":"packages/eb-evaluation/api/diagnostics/fpc/#eb_evaluation.diagnostics.fpc.FPCClass","title":"<code>FPCClass</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Compatibility taxonomy for forecast primitives.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/fpc/#eb_evaluation.diagnostics.fpc.FPCSignals","title":"<code>FPCSignals</code>  <code>dataclass</code>","text":"<p>Observable signals used to classify forecast primitive compatibility.</p> <p>All fields are intended to be computed from evaluation data and are interpretable on their own. None of these values should be \"fitted\".</p> <p>Notes: - nsl_ are fractions in [0, 1] - hr_ are fractions in [0, 1] - ud is a magnitude in units of y (same units as actual demand) - cwsl_* are dimensionless ratios (normalized by demand)</p>"},{"location":"packages/eb-evaluation/api/diagnostics/fpc/#eb_evaluation.diagnostics.fpc.FPCThresholds","title":"<code>FPCThresholds</code>  <code>dataclass</code>","text":"<p>Thresholds for FPC classification.</p> <p>These are intentionally simple and interpretable. You can tune these to your operational context, but treat them as governance parameters (not a model).</p>"},{"location":"packages/eb-evaluation/api/diagnostics/fpc/#eb_evaluation.diagnostics.fpc.FPCResult","title":"<code>FPCResult</code>  <code>dataclass</code>","text":"<p>Result container for classification + rationale.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/fpc/#eb_evaluation.diagnostics.fpc.classify_fpc","title":"<code>classify_fpc(signals, thresholds=None)</code>","text":"<p>Classify forecast primitive compatibility from observable signals.</p> <p>Strategy: - INCOMPATIBLE when we see a strong mismatch signature (very low coverage AND   tiny response), reinforced by low tolerance hit-rate and/or high UD. - COMPATIBLE when coverage is meaningfully above zero AND responds to RAL in a   material way. - Otherwise MARGINAL.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/","title":"Governance diagnostics","text":"<p>This section documents governance-related diagnostics provided by <code>eb-evaluation</code>.</p> <p>Governance diagnostics surface signals related to policy compliance, readiness thresholds, and decision accountability.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance","title":"<code>eb_evaluation.diagnostics.governance</code>","text":"<p>Forecast governance entrypoints for Electric Barometer diagnostics.</p> <p>This module unifies governance-oriented diagnostics into a single, stable decision surface:</p> <ul> <li>Demand Quantization Compatibility (DQC) diagnoses whether realized demand is   continuous-like or strongly quantized/packed (piecewise items, pack sizes).</li> <li>Forecast Primitive Compatibility (FPC) diagnoses whether a scale-based   readiness adjustment (e.g., RAL) is a structurally valid control lever for the   demand process at the evaluation resolution.</li> </ul> <p>The resulting GovernanceDecision is an auditable, deterministic artifact used to drive: - snapping requirements (raw units vs grid units), - tolerance policy interpretation (\u03c4 in raw units vs \u03c4 in grid units), - readiness adjustment policy (allow / caution / disallow), - reporting and downstream policy gating.</p> Decision contract (authoritative) <p>Inputs: - y (realized demand series) is used ONLY for DQC. - fpc_signals_raw is REQUIRED and represents FPC signals computed on raw units. - fpc_signals_snapped is OPTIONAL and represents FPC signals computed after   snapping forecasts to the demand grid. If omitted, snapped == raw.</p> <p>Outputs: - snap_required:     True iff DQC class \u2208 {quantized, piecewise_packed}. - snap_unit:     DQC granularity when snap_required else None. - tau_policy:     grid_units when snap_required else raw_units. - ral_policy &amp; status:     Determined from FPC on:       * snapped FPC when snap_required       * raw FPC when continuous-like</p> <p>Policy presets: - conservative / balanced / aggressive provide small, stable presets for   governance thresholds. Explicit threshold overrides always win.</p> Notes <p>This module is a governance layer. It is not a performance metric and is not an optimization objective.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance.GovernancePreset","title":"<code>GovernancePreset</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Small, stable governance presets.</p> <p>These presets tune only thresholds (not algorithms). They are intended as governance defaults that are easy to communicate and keep stable over time.</p> <ul> <li>conservative: harder to declare \"compatible\"</li> <li>balanced: current default behavior (close to upstream defaults)</li> <li>aggressive: easier to declare \"compatible\" (still deterministic/auditable)</li> </ul>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance.GovernanceStatus","title":"<code>GovernanceStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Traffic-light status for downstream gating.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance.TauPolicy","title":"<code>TauPolicy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>How to interpret tolerance \u03c4 downstream.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance.RALPolicy","title":"<code>RALPolicy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Whether readiness adjustment is allowed downstream.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance.GovernanceDecision","title":"<code>GovernanceDecision</code>  <code>dataclass</code>","text":"<p>Authoritative governance decision.</p> <p>Fields are designed to be stable and auditable. Downstream systems should use these values directly rather than re-implementing policy logic.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance.snap_to_grid","title":"<code>snap_to_grid(values, unit, *, mode='ceil')</code>","text":"<p>Snap values to the detected demand grid.</p> <p>We default to ceil snapping because readiness is a \"build to cover\" control in most operational settings (i.e., avoid underbuild). Downstream systems may choose alternate snapping (round/floor) but governance should be conservative.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Sequence[float]</code> <p>Forecast values to snap.</p> required <code>unit</code> <code>float</code> <p>Grid unit (granularity) to snap to. Must be &gt; 0.</p> required <code>mode</code> <code>str</code> <p>One of {\"ceil\", \"round\", \"floor\"}.</p> <code>'ceil'</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>Snapped values.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance.build_fpc_signals","title":"<code>build_fpc_signals(*, nsl_base, nsl_ral, hr_base_tau, hr_ral_tau, ud, cwsl_base=None, cwsl_ral=None, intervals=None, shortfall_intervals=None)</code>","text":"<p>Convenience builder for FPCSignals with derived deltas.</p> <p>This keeps the public governance entrypoint stable even if the FPCSignals dataclass grows fields later.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance.preset_thresholds","title":"<code>preset_thresholds(preset)</code>","text":"<p>Return (DQCThresholds, FPCThresholds) for a governance preset.</p> <p>Explicit thresholds passed to decide_governance override these defaults.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/governance/#eb_evaluation.diagnostics.governance.decide_governance","title":"<code>decide_governance(*, y, fpc_signals_raw, fpc_signals_snapped=None, dqc_thresholds=None, fpc_thresholds=None, preset=GovernancePreset.BALANCED)</code>","text":"<p>Produce an authoritative governance decision for a single realized series.</p> Inputs <p>y:     Realized demand series (used for DQC only). fpc_signals_raw:     FPC signals computed in raw units. fpc_signals_snapped:     Optional FPC signals computed after snapping forecasts to the detected     demand grid. If not provided, the snapped decision is treated as equal     to the raw decision. dqc_thresholds:     Optional thresholds for DQC. Overrides preset thresholds. fpc_thresholds:     Optional thresholds for FPC. Overrides preset thresholds. preset:     GovernancePreset determining default thresholds when explicit thresholds     are not provided.</p> <p>Returns:</p> Type Description <code>GovernanceDecision</code> <p>Deterministic policy artifact.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/presets/","title":"Diagnostic presets","text":"<p>This section documents predefined diagnostic configurations provided by <code>eb-evaluation</code>.</p> <p>Presets bundle common combinations of diagnostics for standardized evaluation workflows.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/presets/#eb_evaluation.diagnostics.presets","title":"<code>eb_evaluation.diagnostics.presets</code>","text":"<p>Governance presets for Electric Barometer diagnostics.</p> <p>This module defines small, named bundles of governance thresholds that represent policy stances (not model tuning). Presets are intended to be:</p> <ul> <li>stable and versionable (e.g., referenced in notebooks, configs, and reports),</li> <li>auditable (explicit thresholds, no hidden behavior),</li> <li>lightweight (pure configuration; no computation).</li> </ul> <p>Presets are consumed by stable entrypoints such as validate_governance to standardize downstream behavior without requiring callers to hand-wire thresholds on every invocation.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/presets/#eb_evaluation.diagnostics.presets.GovernancePreset","title":"<code>GovernancePreset</code>  <code>dataclass</code>","text":"<p>Named bundle of governance thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Stable preset identifier (e.g., \"conservative\", \"balanced\", \"aggressive\").</p> required <code>description</code> <code>str</code> <p>Short human-readable summary of the policy stance.</p> required <code>dqc</code> <code>DQCThresholds</code> <p>Thresholds governing Demand Quantization Compatibility (DQC).</p> required <code>fpc</code> <code>FPCThresholds</code> <p>Thresholds governing Forecast Primitive Compatibility (FPC).</p> required"},{"location":"packages/eb-evaluation/api/diagnostics/presets/#eb_evaluation.diagnostics.presets.get_governance_preset","title":"<code>get_governance_preset(name)</code>","text":"<p>Retrieve a governance preset by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Preset name. One of {\"conservative\", \"balanced\", \"aggressive\"}.</p> required <p>Returns:</p> Type Description <code>GovernancePreset</code> <p>The corresponding preset.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the preset name is unknown.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/presets/#eb_evaluation.diagnostics.presets.preset_thresholds","title":"<code>preset_thresholds(preset)</code>","text":"<p>Resolve a preset into (DQCThresholds, FPCThresholds).</p> <p>Parameters:</p> Name Type Description Default <code>preset</code> <code>str | GovernancePreset</code> <p>Either a preset name (\"conservative\" | \"balanced\" | \"aggressive\") or an explicit GovernancePreset instance.</p> required <p>Returns:</p> Type Description <code>(DQCThresholds, FPCThresholds)</code> <p>Threshold objects suitable for passing to validate_governance / decide_governance.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>preset</code> is not a <code>str</code> or <code>GovernancePreset</code>.</p> <code>ValueError</code> <p>If <code>preset</code> is a <code>str</code> but not a known preset name.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/run/","title":"Diagnostic execution","text":"<p>This section documents utilities for executing diagnostic routines in <code>eb-evaluation</code>.</p> <p>These utilities coordinate the execution of multiple diagnostics over forecast outputs and input data.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/run/#eb_evaluation.diagnostics.run","title":"<code>eb_evaluation.diagnostics.run</code>","text":"<p>Run-level orchestration for governance-oriented readiness evaluation.</p> <p>This module is the \"wiring layer\" between: - raw evaluation series (y, yhat_base, yhat_ral), - diagnostic computations (DQC, FPC), - governance decision surface (DQC x FPC), - and a minimal recommended routing mode for downstream reporting.</p> Design goals <ul> <li>Keep diagnostic modules pure (no orchestration inside dqc.py / fpc.py).</li> <li>Keep validation entrypoints stable (validate.py remains a thin wrapper layer).</li> <li>Provide a single, auditable choke point to compute + gate readiness evaluation.</li> </ul> <p>This module does NOT persist artifacts. Persistence/adapters should live in a separate \"artifacts\" layer if/when needed.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/run/#eb_evaluation.diagnostics.run.GateResult","title":"<code>GateResult</code>  <code>dataclass</code>","text":"<p>Combined governance artifact (run-level).</p> <p>This is intentionally small and \"portable\" as an in-memory object. If you later decide to persist it across systems, define a contract in eb-contracts.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/run/#eb_evaluation.diagnostics.run.run_governance_gate","title":"<code>run_governance_gate(*, y, yhat_base, yhat_ral, tau, cwsl_r=None, dqc_thresholds=None, fpc_thresholds=None, preset=None, snap_mode='ceil')</code>","text":"<p>Run the minimal governance gate and return a recommended evaluation mode.</p> Routing semantics <ul> <li>\"reroute_discrete\":     When FPC is INCOMPATIBLE for the applicable space (snapped if required,     raw otherwise). This indicates scale-based readiness adjustment is not a     valid control lever and discrete decision modeling should be used.</li> <li>\"pack_aware\":     When snapping is required by DQC and FPC is not incompatible. Indicates     downstream evaluation should interpret \u03c4 in grid units and (if using RAL)     apply snap-to-grid to adjusted forecasts before scoring.</li> <li>\"continuous\":     When demand is continuous-like and FPC is not incompatible.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Sequence[float]</code> <p>Realized demand series.</p> required <code>yhat_base</code> <code>Sequence[float]</code> <p>Baseline forecasts (raw units).</p> required <code>yhat_ral</code> <code>Sequence[float]</code> <p>Readiness-adjusted forecasts (raw units).</p> required <code>tau</code> <code>float</code> <p>Tolerance band for HR@\u03c4 (raw units). If snapping is required, governance indicates \u03c4 should be interpreted in grid units downstream.</p> required <code>cwsl_r</code> <code>float | None</code> <p>Optional CWSL under/over cost ratio r (&gt;= 1 typically). When provided, CWSL response is included in signals and may influence classification.</p> <code>None</code> <code>dqc_thresholds</code> <code>DQCThresholds | None</code> <p>Optional explicit thresholds. If <code>preset</code> is provided, explicit thresholds are not allowed.</p> <code>None</code> <code>fpc_thresholds</code> <code>DQCThresholds | None</code> <p>Optional explicit thresholds. If <code>preset</code> is provided, explicit thresholds are not allowed.</p> <code>None</code> <code>preset</code> <code>GovernancePreset | str | None</code> <p>Optional governance preset name/enum; determines default thresholds.</p> <code>None</code> <code>snap_mode</code> <code>Literal['ceil', 'round', 'floor']</code> <p>Snapping mode used when snapping is required.</p> <code>'ceil'</code> <p>Returns:</p> Type Description <code>GateResult</code> <p>Combined diagnostic results + governance decision + recommended routing.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If series lengths mismatch, or if <code>preset</code> is mixed with explicit thresholds.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/validate/","title":"Diagnostic validation","text":"<p>This section documents validation utilities for diagnostics in <code>eb-evaluation</code>.</p> <p>Diagnostic validation utilities check diagnostic outputs for structural correctness and consistency.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/validate/#eb_evaluation.diagnostics.validate","title":"<code>eb_evaluation.diagnostics.validate</code>","text":"<p>Public validation entrypoints for diagnostic artifacts.</p> <p>This module defines stable, versioned entrypoints for running Electric Barometer diagnostics. Consumers should prefer these functions over importing diagnostic modules directly.</p> <p>The goal is API stability: diagnostic implementations may evolve, but these entrypoints remain stable and auditable.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/validate/#eb_evaluation.diagnostics.validate.GateResult","title":"<code>GateResult</code>  <code>dataclass</code>","text":"<p>Result of running the governance gate from common raw inputs.</p> <p>This is a convenience orchestrator that: - computes DQC from realized demand y, - computes raw FPC signals from (y, yhat_base, yhat_ral, tau, cwsl_r), - optionally computes snapped FPC signals when DQC requires snapping, - runs the governance decision contract, - returns an explicit recommended evaluation mode for downstream routing.</p> <p>The authoritative decision is always <code>decision</code> (GovernanceDecision).</p>"},{"location":"packages/eb-evaluation/api/diagnostics/validate/#eb_evaluation.diagnostics.validate.validate_fpc","title":"<code>validate_fpc(*, signals, thresholds=None)</code>","text":"<p>Run Forecast Primitive Compatibility (FPC) classification.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>FPCSignals</code> <p>Precomputed observable signals used by FPC.</p> required <code>thresholds</code> <code>FPCThresholds | None</code> <p>Optional governance thresholds. If not provided, defaults are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>FPCResult</code> <p>Compatibility class + signals + rationale.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/validate/#eb_evaluation.diagnostics.validate.validate_dqc","title":"<code>validate_dqc(*, y, thresholds=None)</code>","text":"<p>Run Demand Quantization Compatibility (DQC) classification.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Sequence[float]</code> <p>Realized demand/usage series for a single entity.</p> required <code>thresholds</code> <code>DQCThresholds | None</code> <p>Optional governance thresholds. If not provided, defaults are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>DQCResult</code> <p>Quantization class + signals + rationale.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/validate/#eb_evaluation.diagnostics.validate.validate_governance","title":"<code>validate_governance(*, y, fpc_signals_raw, fpc_signals_snapped=None, dqc_thresholds=None, fpc_thresholds=None, preset=None)</code>","text":"<p>Run the governance decision contract (DQC x FPC) for a single entity.</p> <p>This is the stable public entrypoint for governance-oriented evaluation. It combines: - Demand Quantization Compatibility (DQC): whether snapping to a demand grid is required - Forecast Primitive Compatibility (FPC): whether scale-based readiness adjustment (e.g., RAL)   is structurally valid, evaluated in raw space and (when required) snapped space.</p> Presets <p>If <code>preset</code> is provided, callers MUST NOT also pass explicit <code>dqc_thresholds</code> and/or <code>fpc_thresholds</code>. Mixing a preset with explicit thresholds is ambiguous and is rejected.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Sequence[float]</code> <p>Realized demand/usage series for a single entity. This should be the raw realized series (not pre-snapped).</p> required <code>fpc_signals_raw</code> <code>FPCSignals</code> <p>Observable FPC signals computed in raw units (no snapping).</p> required <code>fpc_signals_snapped</code> <code>FPCSignals | None</code> <p>Observable FPC signals computed after applying the snap-to-grid policy. Required when DQC indicates snapping is required; optional otherwise.</p> <code>None</code> <code>dqc_thresholds</code> <code>DQCThresholds | None</code> <p>Optional thresholds for DQC classification.</p> <code>None</code> <code>fpc_thresholds</code> <code>FPCThresholds | None</code> <p>Optional thresholds for FPC classification (applied to both raw and snapped signals).</p> <code>None</code> <code>preset</code> <code>str | GovernancePreset | None</code> <p>Optional governance preset. Provide either: - preset name: {\"conservative\", \"balanced\", \"aggressive\"}, or - a GovernancePreset instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>GovernanceDecision</code> <p>A deterministic, auditable decision artifact containing: - DQC result (structure / snap requirement) - FPC results (raw + snapped) - snapping and \u03c4 interpretation policy - readiness adjustment allowability policy (traffic-light status)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>preset</code> is provided together with explicit thresholds.</p> <code>TypeError</code> <p>If <code>preset</code> is not a str or GovernancePreset.</p> <code>ValueError</code> <p>If <code>preset</code> is a str but not a known preset name.</p>"},{"location":"packages/eb-evaluation/api/diagnostics/validate/#eb_evaluation.diagnostics.validate.run_governance_gate","title":"<code>run_governance_gate(*, y, yhat_base, yhat_ral, tau, cwsl_r=None, dqc_thresholds=None, fpc_thresholds=None, preset=None)</code>","text":"<p>Orchestrate governance from common raw inputs.</p> <p>This helper computes FPC signals directly from series inputs and produces an explicit recommended evaluation mode:</p> <ul> <li>\"continuous\": demand is continuous-like; evaluate in raw units.</li> <li>\"pack_aware\": demand is quantized/packed; \u03c4 and adjustments should be interpreted   in grid units; forecasts/adjustments should be snapped as required.</li> <li>\"reroute_discrete\": forecast primitive is incompatible for readiness control; consider   rerouting to a discrete/event/state decision model.</li> </ul> <p>Preset rules match validate_governance: do not mix preset with explicit thresholds.</p>"},{"location":"packages/eb-evaluation/api/model_selection/auto_engine/","title":"Automated model selection","text":"<p>This section documents automated model selection utilities provided by <code>eb-evaluation</code>.</p> <p>These utilities support automated evaluation and selection of candidate models based on performance metrics, diagnostics, and decision-oriented criteria.</p>"},{"location":"packages/eb-evaluation/api/model_selection/auto_engine/#eb_evaluation.model_selection.auto_engine","title":"<code>eb_evaluation.model_selection.auto_engine</code>","text":"<p>Auto model-zoo builder for Electric Barometer selection.</p> <p>This module defines :class:<code>~eb_evaluation.model_selection.auto_engine.AutoEngine</code>, a convenience factory that constructs an unfitted :class:<code>~eb_evaluation.model_selection.electric_barometer.ElectricBarometer</code> with a curated set of candidate regressors (a \"model zoo\") and asymmetric cost parameters.</p> <p>The intent is to provide a batteries-included entry point:</p> <ul> <li>choose asymmetric costs (cu, co)</li> <li>choose a speed preset (fast, balanced, slow)</li> <li>optionally include additional engines (XGBoost, LightGBM, CatBoost) when installed</li> <li>get back an unfitted selector ready for cost-aware model selection</li> </ul> <p>Model selection is performed using cost-aware criteria (e.g., CWSL) rather than symmetric error alone, enabling operationally aligned choices.</p>"},{"location":"packages/eb-evaluation/api/model_selection/auto_engine/#eb_evaluation.model_selection.auto_engine.AutoEngine","title":"<code>AutoEngine</code>","text":"<p>Convenience factory for :class:<code>~eb_evaluation.model_selection.electric_barometer.ElectricBarometer</code>.</p> <p>AutoEngine builds an ElectricBarometer with a curated set of candidate models chosen by a speed preset:</p> <ul> <li><code>speed=\"fast\"</code>: small, inexpensive zoo; suitable for quick experiments and CI.</li> <li><code>speed=\"balanced\"</code> (default): trade-off between runtime and modeling power.</li> <li><code>speed=\"slow\"</code>: heavier ensembles/boosting; use when wall-clock time is acceptable.</li> </ul> <p>Asymmetric costs define the primary selection objective via the cost ratio <code>R = cu / co</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cu</code> <code>float</code> <p>Underbuild (shortfall) cost per unit. Must be strictly positive.</p> <code>2.0</code> <code>co</code> <code>float</code> <p>Overbuild (excess) cost per unit. Must be strictly positive.</p> <code>1.0</code> <code>tau</code> <code>float</code> <p>Tolerance parameter forwarded to ElectricBarometer for optional diagnostics (e.g., HR@tau).</p> <code>2.0</code> <code>selection_mode</code> <code>str</code> <p>Selection strategy used by ElectricBarometer. Must be <code>\"holdout\"</code> or <code>\"cv\"</code>.</p> <code>'holdout'</code> <code>cv</code> <code>int</code> <p>Number of folds when <code>selection_mode=\"cv\"</code>.</p> <code>3</code> <code>random_state</code> <code>int | None</code> <p>Seed used for stochastic models and (when applicable) cross-validation.</p> <code>None</code> <code>speed</code> <code>SpeedType</code> <p>Controls which models are included and their approximate complexity.</p> <code>'balanced'</code> Notes <p>Optional engines are included only when their packages are installed: <code>xgboost</code>, <code>lightgbm</code>, <code>catboost</code>.</p>"},{"location":"packages/eb-evaluation/api/model_selection/auto_engine/#eb_evaluation.model_selection.auto_engine.AutoEngine.available_models","title":"<code>available_models()</code>","text":"<p>List the model names that would be included in the zoo for this AutoEngine.</p> <p>This reflects: - the selected <code>speed</code> preset, and - which optional packages are available in the current environment.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Model names in deterministic insertion order.</p>"},{"location":"packages/eb-evaluation/api/model_selection/auto_engine/#eb_evaluation.model_selection.auto_engine.AutoEngine.build_zoo","title":"<code>build_zoo()</code>","text":"<p>Build and return the unfitted model zoo.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Mapping of <code>{name: estimator}</code> for candidate regressors.</p> Notes <p>The returned dict is a shallow copy so callers may filter/mutate it without affecting future calls.</p>"},{"location":"packages/eb-evaluation/api/model_selection/auto_engine/#eb_evaluation.model_selection.auto_engine.AutoEngine.build_selector","title":"<code>build_selector(X, y, *, include=None, exclude=None, metric='cwsl', error_policy='warn_skip', time_budget_s=None, per_model_time_budget_s=None, refit_on_full=False)</code>","text":"<p>Build an unfitted ElectricBarometer configured with the default model zoo.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix. Currently unused by the builder (reserved for future heuristics).</p> required <code>y</code> <code>ndarray</code> <p>Target vector. Currently unused by the builder (reserved for future heuristics).</p> required <code>include</code> <code>set[str] | None</code> <p>Optional allowlist of model names to include from the zoo.</p> <code>None</code> <code>exclude</code> <code>set[str] | None</code> <p>Optional blocklist of model names to exclude from the zoo.</p> <code>None</code> <code>metric</code> <code>_MetricName</code> <p>Selection objective used by ElectricBarometer to choose the winning model.</p> <code>'cwsl'</code> <code>error_policy</code> <code>_ErrorPolicy</code> <p>Behavior when a candidate model fails to fit/predict or otherwise errors.</p> <code>'warn_skip'</code> <code>time_budget_s</code> <code>float | None</code> <p>Optional wall-clock time budget (seconds) for the full selection run.</p> <code>None</code> <code>per_model_time_budget_s</code> <code>float | None</code> <p>Optional wall-clock time budget (seconds) per candidate model.</p> <code>None</code> <code>refit_on_full</code> <code>bool</code> <p>Forwarded to ElectricBarometer; controls whether the winning model is refit on train+validation in holdout mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>ElectricBarometer</code> <p>Unfitted selector instance.</p>"},{"location":"packages/eb-evaluation/api/model_selection/compare/","title":"Model comparison","text":"<p>This section documents model comparison utilities provided by <code>eb-evaluation</code>.</p> <p>Model comparison utilities support side-by-side evaluation of multiple models across metrics, diagnostics, and operational outcomes.</p>"},{"location":"packages/eb-evaluation/api/model_selection/compare/#eb_evaluation.model_selection.compare","title":"<code>eb_evaluation.model_selection.compare</code>","text":"<p>Forecast comparison and cost-aware model selection helpers.</p> <p>This module provides evaluation-oriented utilities built on top of <code>eb_metrics.metrics</code>:</p> <ul> <li><code>compare_forecasts</code> computes CWSL and related diagnostics for multiple forecast   vectors against a common target series.</li> <li><code>select_model_by_cwsl</code> fits candidate estimators (using their native training   objective) and selects the model with the lowest validation CWSL.</li> <li><code>select_model_by_cwsl_cv</code> performs K-fold cross-validation, selecting the model   with the lowest mean CWSL and refitting it on the full dataset.</li> </ul> <p>CWSL is evaluated with asymmetric costs for underbuild and overbuild, typically summarized by a cost ratio:</p> \\[     R = \\frac{c_u}{c_o} \\] <p>where \\(c_u\\) is the cost per unit of shortfall and \\(c_o\\) is the cost per unit of excess.</p>"},{"location":"packages/eb-evaluation/api/model_selection/compare/#eb_evaluation.model_selection.compare.compare_forecasts","title":"<code>compare_forecasts(y_true, forecasts, cu, co, sample_weight=None, tau=2.0)</code>","text":"<p>Compare multiple forecast models on the same target series.</p> <p>For each forecast vector, compute CWSL and a standard set of diagnostics:</p> <ul> <li>CWSL</li> <li>NSL</li> <li>UD</li> <li>wMAPE</li> <li>HR@tau</li> <li>FRS</li> <li>MAE</li> <li>RMSE</li> <li>MAPE</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Actual (ground-truth) values.</p> required <code>forecasts</code> <code>Mapping[str, array - like]</code> <p>Mapping from model name to forecast vector. Each forecast must be shape <code>(n_samples,)</code>.</p> required <code>cu</code> <code>float or array-like of shape (n_samples,)</code> <p>Underbuild (shortfall) cost per unit.</p> required <code>co</code> <code>float or array-like of shape (n_samples,)</code> <p>Overbuild (excess) cost per unit.</p> required <code>sample_weight</code> <code>array-like of shape (n_samples,)</code> <p>Optional non-negative weights per interval. Passed to metrics that support <code>sample_weight</code> (CWSL, NSL, UD, HR@tau, FRS). Metrics that are currently unweighted in <code>eb_metrics</code> (e.g., wMAPE, MAE, RMSE, MAPE) are computed without weights.</p> <code>None</code> <code>tau</code> <code>float or array - like</code> <p>Tolerance parameter for HR@tau. May be scalar or per-interval.</p> <code>2.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame indexed by model name with columns: <code>[\"CWSL\", \"NSL\", \"UD\", \"wMAPE\", \"HR@tau\", \"FRS\", \"MAE\", \"RMSE\", \"MAPE\"]</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_true</code> is not 1D, if <code>forecasts</code> is empty, or if any forecast length is incompatible with <code>y_true</code>.</p>"},{"location":"packages/eb-evaluation/api/model_selection/compare/#eb_evaluation.model_selection.compare.select_model_by_cwsl","title":"<code>select_model_by_cwsl(models, X_train, y_train, X_val, y_val, *, cu, co, sample_weight_val=None)</code>","text":"<p>Fit multiple models, then select the best by validation CWSL.</p> <p>Each estimator is fit on <code>(X_train, y_train)</code> using its native objective (typically MSE/RMSE), then evaluated on the validation set via CWSL:</p> \\[     \\text{CWSL} = \\mathrm{cwsl}(y_{\\mathrm{val}}, \\hat{y}_{\\mathrm{val}}; c_u, c_o) \\] <p>The model with the lowest CWSL is returned, along with a compact results table.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>dict[str, Any]</code> <p>Mapping from model name to an unfitted estimator implementing:</p> <ul> <li><code>fit(X, y)</code></li> <li><code>predict(X)</code></li> </ul> required <code>X_train</code> <p>Training data used to fit each model.</p> required <code>y_train</code> <p>Training data used to fit each model.</p> required <code>X_val</code> <p>Validation data used only for evaluation.</p> required <code>y_val</code> <p>Validation data used only for evaluation.</p> required <code>cu</code> <code>float</code> <p>Underbuild (shortfall) cost per unit for CWSL.</p> required <code>co</code> <code>float</code> <p>Overbuild (excess) cost per unit for CWSL.</p> required <code>sample_weight_val</code> <code>array - like or None</code> <p>Optional per-interval weights for the validation set, passed to CWSL.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_name</code> <code>str</code> <p>Name of the model with the lowest CWSL on the validation set.</p> <code>best_model</code> <code>Any</code> <p>Fitted estimator corresponding to <code>best_name</code>.</p> <code>results</code> <code>DataFrame</code> <p>DataFrame indexed by model name with columns <code>[\"CWSL\", \"RMSE\", \"wMAPE\"]</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no models are evaluated.</p> Notes <ul> <li>RMSE and wMAPE are computed unweighted (consistent with current eb_metrics behavior).</li> <li>This function is intentionally simple and does not handle time-series splitting; callers   should ensure the split is appropriate.</li> </ul>"},{"location":"packages/eb-evaluation/api/model_selection/compare/#eb_evaluation.model_selection.compare.select_model_by_cwsl_cv","title":"<code>select_model_by_cwsl_cv(models, X, y, *, cu, co, cv=5, sample_weight=None)</code>","text":"<p>Select a model by cross-validated CWSL and refit on the full dataset.</p> <p>This is a simple K-fold cross-validation loop:</p> <ol> <li>Split indices into <code>cv</code> folds.</li> <li>For each model and fold:</li> <li>fit on (cv - 1) folds</li> <li>evaluate on the held-out fold using CWSL, RMSE, and wMAPE</li> <li>Aggregate metrics across folds for each model.</li> <li>Choose the model with the lowest mean CWSL.</li> <li>Refit the chosen model once on all data <code>(X, y)</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>dict[str, Any]</code> <p>Mapping from model name to an unfitted estimator implementing <code>fit</code> and <code>predict</code>.</p> required <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Feature matrix.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target vector.</p> required <code>cu</code> <code>float</code> <p>Underbuild (shortfall) cost per unit for CWSL.</p> required <code>co</code> <code>float</code> <p>Overbuild (excess) cost per unit for CWSL.</p> required <code>cv</code> <code>int</code> <p>Number of folds. Must be &gt;= 2.</p> <code>5</code> <code>sample_weight</code> <code>numpy.ndarray of shape (n_samples,)</code> <p>Optional per-sample weights used only for CWSL metric calculation. RMSE and wMAPE remain unweighted.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_name</code> <code>str</code> <p>Model name with the lowest mean CWSL across folds.</p> <code>best_model</code> <code>Any</code> <p>The chosen estimator refit on all data.</p> <code>results</code> <code>DataFrame</code> <p>DataFrame indexed by model name with columns:</p> <ul> <li><code>CWSL_mean</code>, <code>CWSL_std</code></li> <li><code>RMSE_mean</code>, <code>RMSE_std</code></li> <li><code>wMAPE_mean</code>, <code>wMAPE_std</code></li> <li><code>n_folds</code></li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X/y dimensions mismatch, <code>cv &lt; 2</code>, sample_weight length mismatch, or no models are evaluated.</p> Notes <p>This function uses a naive split of indices into contiguous folds via <code>numpy.array_split</code>. For time-series problems, callers should prefer time-aware splitting outside this helper.</p>"},{"location":"packages/eb-evaluation/api/model_selection/cwsl_regressor/","title":"Cost-weighted service loss regressor","text":"<p>This section documents the cost-weighted service loss (CWSL) regressor used in model selection workflows.</p> <p>The CWSL regressor supports learning and ranking models based on asymmetric service risk preferences.</p>"},{"location":"packages/eb-evaluation/api/model_selection/cwsl_regressor/#eb_evaluation.model_selection.cwsl_regressor","title":"<code>eb_evaluation.model_selection.cwsl_regressor</code>","text":"<p>scikit-learn-style estimator wrapper for cost-aware model selection.</p> <p>This module defines <code>CWSLRegressor</code>, a lightweight wrapper around <code>ElectricBarometer</code> that exposes a familiar <code>fit</code> / <code>predict</code> / <code>score</code> API.</p> <p>The estimator selects among a set of candidate models using Cost-Weighted Service Loss (CWSL) as the selection criterion. Candidate models are trained using their native objectives (e.g., squared error), but selection is performed using asymmetric operational costs.</p> <p>The primary cost preference can be summarized by the cost ratio:</p> \\[     R = \\frac{c_u}{c_o} \\] <p>where \\(c_u\\) is the underbuild (shortfall) cost per unit and \\(c_o\\) is the overbuild (excess) cost per unit.</p>"},{"location":"packages/eb-evaluation/api/model_selection/cwsl_regressor/#eb_evaluation.model_selection.cwsl_regressor.CWSLRegressor","title":"<code>CWSLRegressor</code>","text":"<p>scikit-learn-style estimator that selects among candidate models using CWSL.</p> <p>This class wraps <code>ElectricBarometer</code> and exposes:</p> <ul> <li><code>fit(X, y)</code>: perform cost-aware model selection</li> <li><code>predict(X)</code>: predict using the selected best estimator</li> <li><code>score(X, y)</code>: return a sklearn-style score based on negative CWSL</li> </ul> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>dict[str, Any]</code> <p>Mapping of candidate model name to an unfitted estimator implementing:</p> <ul> <li><code>fit(X, y)</code></li> <li><code>predict(X)</code></li> </ul> <p>Models may be scikit-learn regressors, pipelines, or EB adapters.</p> required <code>cu</code> <code>float</code> <p>Underbuild (shortfall) cost per unit. Must be strictly positive.</p> <code>2.0</code> <code>co</code> <code>float</code> <p>Overbuild (excess) cost per unit. Must be strictly positive.</p> <code>1.0</code> <code>tau</code> <code>float</code> <p>Tolerance parameter forwarded to ElectricBarometer for optional diagnostics (e.g., HR@tau).</p> <code>2.0</code> <code>training_mode</code> <code>selection_only</code> <p>Training behavior; currently ElectricBarometer supports selection-only mode.</p> <code>\"selection_only\"</code> <code>refit_on_full</code> <code>bool</code> <p>Refit behavior after selection.</p> <ul> <li>In holdout mode: if True, refit the winning model on train + validation.</li> <li>In CV mode: the winning model is refit on the full dataset.</li> </ul> <code>True</code> <code>selection_mode</code> <code>(holdout, cv)</code> <p>How selection is performed:</p> <ul> <li><code>\"holdout\"</code>: split internally using <code>validation_fraction</code>.</li> <li><code>\"cv\"</code>: perform K-fold selection inside ElectricBarometer.</li> </ul> <code>\"holdout\"</code> <code>cv</code> <code>int</code> <p>Number of folds when <code>selection_mode=\"cv\"</code>.</p> <code>3</code> <code>validation_fraction</code> <code>float</code> <p>Fraction of samples used for validation when <code>selection_mode=\"holdout\"</code>. Must be in (0, 1).</p> <code>0.2</code> <code>random_state</code> <code>int | None</code> <p>Seed used for internal shuffling and CV splitting.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>selector_</code> <code>ElectricBarometer | None</code> <p>Underlying selector instance used in the most recent fit.</p> <code>best_name_</code> <code>str | None</code> <p>Name of the winning model.</p> <code>best_estimator_</code> <code>Any | None</code> <p>Fitted winning estimator.</p> <code>results_</code> <code>Any</code> <p>Comparison table produced by ElectricBarometer (typically a pandas DataFrame).</p> <code>validation_cwsl_</code> <code>float | None</code> <p>CWSL score of the winning model on validation (holdout) or mean CV.</p> <code>validation_rmse_</code> <code>float | None</code> <p>RMSE score of the winning model on validation or mean CV.</p> <code>validation_wmape_</code> <code>float | None</code> <p>wMAPE score of the winning model on validation or mean CV.</p> <code>n_features_in_</code> <code>int | None</code> <p>Number of features observed during fit.</p> Notes <p><code>score</code> returns negative CWSL to align with sklearn conventions (higher is better).</p>"},{"location":"packages/eb-evaluation/api/model_selection/cwsl_regressor/#eb_evaluation.model_selection.cwsl_regressor.CWSLRegressor.r_","title":"<code>r_</code>  <code>property</code>","text":"<p>Cost ratio.</p> <p>Returns:</p> Type Description <code>float</code> <p>The ratio \\(R = c_u / c_o\\).</p>"},{"location":"packages/eb-evaluation/api/model_selection/cwsl_regressor/#eb_evaluation.model_selection.cwsl_regressor.CWSLRegressor.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit CWSLRegressor on (X, y) by delegating selection to ElectricBarometer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Feature matrix.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target vector.</p> required <code>sample_weight</code> <code>numpy.ndarray of shape (n_samples,)</code> <p>Optional per-sample weights. In CV mode these are passed through to ElectricBarometer so validation folds are cost-weighted. In holdout mode, they are passed through to ElectricBarometer (behavior depends on EB).</p> <code>None</code> <p>Returns:</p> Type Description <code>CWSLRegressor</code> <p>Fitted instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X/y shapes are incompatible or sample_weight length mismatches.</p>"},{"location":"packages/eb-evaluation/api/model_selection/cwsl_regressor/#eb_evaluation.model_selection.cwsl_regressor.CWSLRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Generate predictions from the selected best estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> required <p>Returns:</p> Type Description <code>numpy.ndarray of shape (n_samples,)</code> <p>Predicted values.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called before <code>fit</code>.</p>"},{"location":"packages/eb-evaluation/api/model_selection/cwsl_regressor/#eb_evaluation.model_selection.cwsl_regressor.CWSLRegressor.score","title":"<code>score(X, y, sample_weight=None)</code>","text":"<p>Compute a sklearn-style score using negative CWSL.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Feature matrix.</p> required <code>y</code> <code>array-like of shape (n_samples,)</code> <p>Target vector.</p> required <code>sample_weight</code> <code>numpy.ndarray of shape (n_samples,)</code> <p>Optional per-sample weights passed to CWSL.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Negative CWSL on the provided data (higher is better).</p>"},{"location":"packages/eb-evaluation/api/model_selection/cwsl_regressor/#eb_evaluation.model_selection.cwsl_regressor.CWSLRegressor.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Minimal sklearn-compatible <code>get_params</code> implementation.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>Included for sklearn compatibility. Nested parameter extraction is not performed in this lightweight implementation.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Parameter mapping.</p>"},{"location":"packages/eb-evaluation/api/model_selection/cwsl_regressor/#eb_evaluation.model_selection.cwsl_regressor.CWSLRegressor.set_params","title":"<code>set_params(**params)</code>","text":"<p>Minimal sklearn-compatible <code>set_params</code> implementation.</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <p>Parameters to set on this instance.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CWSLRegressor</code> <p>Updated instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid parameter name is provided.</p>"},{"location":"packages/eb-evaluation/api/model_selection/electric_barometer/","title":"Electric Barometer model selection","text":"<p>This section documents Electric Barometer\u2013specific model selection utilities.</p> <p>These utilities integrate evaluation metrics, diagnostics, and readiness signals to support holistic model ranking and selection.</p>"},{"location":"packages/eb-evaluation/api/model_selection/electric_barometer/#eb_evaluation.model_selection.electric_barometer","title":"<code>eb_evaluation.model_selection.electric_barometer</code>","text":"<p>Cost-aware model selection using the Electric Barometer workflow.</p> <p>This module defines <code>ElectricBarometer</code>, a lightweight selector that evaluates a set of candidate regressors using Cost-Weighted Service Loss (CWSL) as the primary objective and selects the model that minimizes expected operational cost.</p> <p>Selection preference is governed by asymmetric unit costs:</p> <ul> <li><code>cu</code>: underbuild (shortfall) cost per unit</li> <li><code>co</code>: overbuild (excess) cost per unit</li> </ul> <p>A convenient summary is the cost ratio:</p> \\[     R = \\frac{c_u}{c_o} \\] Notes <p>ElectricBarometer is intentionally a selector (not a trainer that optimizes CWSL directly). Candidate models are trained using their native objectives (e.g., squared error) and are selected using a chosen selection objective on validation data (holdout) or across folds (CV).</p>"},{"location":"packages/eb-evaluation/api/model_selection/electric_barometer/#eb_evaluation.model_selection.electric_barometer.ElectricBarometer","title":"<code>ElectricBarometer</code>","text":"<p>Cost-aware selector that chooses the best model by minimizing a selection objective.</p> <p>ElectricBarometer evaluates each candidate model on either:</p> <ul> <li>a provided train/validation split (<code>selection_mode=\"holdout\"</code>), or</li> <li>K-fold cross-validation on the provided dataset (<code>selection_mode=\"cv\"</code>),</li> </ul> <p>and selects the model with the best (lowest) score under the chosen selection objective. For interpretability, it also reports reference diagnostics (CWSL, RMSE, wMAPE).</p> <p>Operational preference is captured by asymmetric costs and the induced ratio:</p> <p>$$</p> <pre><code>R = \\frac{c_u}{c_o}\n</code></pre> <p>$$</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>dict[str, Any]</code> <p>Mapping of candidate model name to an unfitted estimator implementing:</p> <ul> <li><code>fit(X, y)</code></li> <li><code>predict(X)</code></li> </ul> <p>Models can be scikit-learn regressors/pipelines or EB adapters implementing the same interface.</p> required <code>include</code> <code>set[str] | None</code> <p>Optional allowlist of model names to include from <code>models</code>. If provided, only these names are retained (after validation).</p> <code>None</code> <code>exclude</code> <code>set[str] | None</code> <p>Optional blocklist of model names to exclude from <code>models</code> (after validation). Applied after <code>include</code> filtering.</p> <code>None</code> <code>metric</code> <code>('cwsl', 'rmse', 'wmape')</code> <p>Selection objective used to choose the winning model. All metrics are computed and reported; this parameter determines which column is optimized.</p> <code>\"cwsl\"</code> <code>tie_tol</code> <code>float</code> <p>Absolute tolerance applied to the selection metric when determining ties. Any model with score &lt;= (best_score + tie_tol) is considered tied.</p> <code>0.0</code> <code>tie_breaker</code> <code>('metric', 'simpler', 'name')</code> <p>How to break ties among models within <code>tie_tol</code> of the best score.</p> <ul> <li><code>\"metric\"</code>: choose the tied model with the lowest metric   (deterministic by insertion/index order)</li> <li><code>\"simpler\"</code>: prefer a \"simpler\" model based on a lightweight heuristic</li> <li><code>\"name\"</code>: choose lexicographically smallest model name</li> </ul> <code>\"metric\"</code> <code>validate_inputs</code> <code>('strict', 'coerce', 'off')</code> <p>Input validation level.</p> <ul> <li><code>\"strict\"</code>: require numeric arrays and error on NaN/inf</li> <li><code>\"coerce\"</code>: coerce to float and error on NaN/inf</li> <li><code>\"off\"</code>: minimal validation (legacy behavior)</li> </ul> <code>\"strict\"</code> <code>error_policy</code> <code>('raise', 'skip', 'warn_skip')</code> <p>Behavior when a candidate model fails to fit/predict or otherwise errors.</p> <ul> <li><code>\"raise\"</code>: raise immediately</li> <li><code>\"skip\"</code>: skip failing models silently (recorded in <code>failures_</code>)</li> <li><code>\"warn_skip\"</code>: warn and skip (recorded in <code>failures_</code>)</li> </ul> <code>\"raise\"</code> <code>time_budget_s</code> <code>float | None</code> <p>Optional wall-clock time budget (seconds) for the full selection run. If exceeded, remaining models are not evaluated. Note: this cannot forcibly interrupt a model already running; it gates starting new candidates and can mark a candidate as timed out if it exceeds budgets.</p> <code>None</code> <code>per_model_time_budget_s</code> <code>float | None</code> <p>Optional wall-clock time budget (seconds) per candidate model (across folds in CV). If exceeded, that model is marked as timed out and skipped (or raises under <code>error_policy=\"raise\"</code>).</p> <code>None</code> <code>cu</code> <code>float</code> <p>Underbuild (shortfall) cost per unit. Must be strictly positive.</p> <code>2.0</code> <code>co</code> <code>float</code> <p>Overbuild (excess) cost per unit. Must be strictly positive.</p> <code>1.0</code> <code>tau</code> <code>float</code> <p>Reserved for downstream diagnostics (e.g., HR@\u03c4) that may be integrated into selection reporting. Currently not used in the selection criterion.</p> <code>2.0</code> <code>training_mode</code> <code>'selection_only'</code> <p>Training behavior. In the current implementation, candidate models are trained using their native objectives and only selection is external.</p> <code>\"selection_only\"</code> <code>refit_on_full</code> <code>bool</code> <p>Refit behavior in holdout mode:</p> <ul> <li>If True, after selecting the best model by the chosen metric on validation data,   refit a fresh clone of the winning model on train and validation.</li> <li>If False, keep the fitted winning model as trained on the training split   (and selected on the validation split).</li> </ul> <p>In CV mode, the selected model is always refit on the full dataset provided to <code>fit</code> (i.e., <code>X_train, y_train</code>).</p> <code>False</code> <code>selection_mode</code> <code>('holdout', 'cv')</code> <p>Selection strategy:</p> <ul> <li><code>\"holdout\"</code>: use the provided <code>(X_train, y_train, X_val, y_val)</code>.</li> <li><code>\"cv\"</code>: ignore <code>X_val, y_val</code> and run K-fold selection on <code>X_train, y_train</code>.</li> </ul> <code>\"holdout\"</code> <code>cv</code> <code>int</code> <p>Number of folds when <code>selection_mode=\"cv\"</code>. Must be at least 2.</p> <code>3</code> <code>random_state</code> <code>int | None</code> <p>Seed used for CV shuffling/splitting.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>best_name_</code> <code>str | None</code> <p>Name of the winning model after calling <code>fit</code>.</p> <code>best_model_</code> <code>Any | None</code> <p>Fitted estimator corresponding to <code>best_name_</code>.</p> <code>results_</code> <code>DataFrame | None</code> <p>Per-model comparison table.</p> <ul> <li>In holdout mode: one row per model with columns <code>[\"CWSL\", \"RMSE\", \"wMAPE\"]</code></li> <li>In CV mode: mean scores across folds with the same columns</li> </ul> <code>failures_</code> <code>dict[str, str]</code> <p>Mapping of model name to a failure reason for models that errored or timed out.</p> <code>validation_cwsl_</code> <code>float | None</code> <p>CWSL of the winning model on validation (holdout) or mean across folds (CV).</p> <code>validation_rmse_</code> <code>float | None</code> <p>RMSE of the winning model on validation (holdout) or mean across folds (CV).</p> <code>validation_wmape_</code> <code>float | None</code> <p>wMAPE of the winning model on validation (holdout) or mean across folds (CV).</p> <code>candidate_names_</code> <code>list[str]</code> <p>Names of candidate models remaining after include/exclude filtering.</p> <code>evaluated_names_</code> <code>list[str]</code> <p>Names of models that were actually attempted during the most recent <code>fit</code>.</p> <code>stopped_early_</code> <code>bool</code> <p>Whether evaluation stopped early due to the global time budget.</p> <code>stop_reason_</code> <code>str | None</code> <p>If <code>stopped_early_</code> is True, a human-readable reason string.</p>"},{"location":"packages/eb-evaluation/api/model_selection/electric_barometer/#eb_evaluation.model_selection.electric_barometer.ElectricBarometer.r_","title":"<code>r_</code>  <code>property</code>","text":"<p>Cost ratio.</p> <p>Returns:</p> Type Description <code>float</code> <p>The ratio:</p> <p>$$</p> <pre><code>R = \\frac{c_u}{c_o}\n</code></pre> <p>$$</p>"},{"location":"packages/eb-evaluation/api/model_selection/electric_barometer/#eb_evaluation.model_selection.electric_barometer.ElectricBarometer.fit","title":"<code>fit(X_train, y_train, X_val, y_val, sample_weight_train=None, sample_weight_val=None, refit_on_full=None)</code>","text":"<p>Fit candidate models and select the best one by minimizing the chosen metric.</p>"},{"location":"packages/eb-evaluation/api/model_selection/electric_barometer/#eb_evaluation.model_selection.electric_barometer.ElectricBarometer.predict","title":"<code>predict(X)</code>","text":"<p>Predict using the selected best model.</p>"},{"location":"packages/eb-evaluation/api/model_selection/electric_barometer/#eb_evaluation.model_selection.electric_barometer.ElectricBarometer.cwsl_score","title":"<code>cwsl_score(y_true, y_pred, sample_weight=None, cu=None, co=None)</code>","text":"<p>Compute CWSL using this selector's costs (or overrides).</p>"},{"location":"packages/eb-evaluation/api/utils/validation/","title":"Validation utilities","text":"<p>This section documents validation utilities provided by <code>eb-evaluation</code>.</p> <p>These utilities perform structural and semantic validation of inputs and outputs used during evaluation workflows, ensuring consistency and correctness before metrics, diagnostics, or model-selection logic is applied.</p>"},{"location":"packages/eb-evaluation/api/utils/validation/#eb_evaluation.utils.validation","title":"<code>eb_evaluation.utils.validation</code>","text":"<p>Lightweight DataFrame validation utilities.</p> <p>This module provides small, explicit validation helpers for pandas DataFrames used throughout the Electric Barometer evaluation and model-selection stack.</p> <p>The intent is to:</p> <ul> <li>fail fast with clear error messages</li> <li>keep validation logic centralized and reusable</li> <li>distinguish data validation errors from other ValueError instances</li> </ul> <p>These helpers are intentionally minimal and do not attempt schema inference or coercion; they only assert required structural properties.</p>"},{"location":"packages/eb-evaluation/api/utils/validation/#eb_evaluation.utils.validation.DataFrameValidationError","title":"<code>DataFrameValidationError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when a pandas DataFrame fails a validation check.</p> <p>This is a thin subclass of <code>ValueError</code> that allows callers to explicitly catch DataFrame-related validation issues and distinguish them from other value errors (e.g., numerical domain errors).</p>"},{"location":"packages/eb-evaluation/api/utils/validation/#eb_evaluation.utils.validation.ensure_columns_present","title":"<code>ensure_columns_present(df, required, *, context=None)</code>","text":"<p>Ensure that all required columns are present in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to validate.</p> required <code>required</code> <code>Sequence[str]</code> <p>Column names that must be present in <code>df</code>.</p> required <code>context</code> <code>str | None</code> <p>Optional context string (e.g., function or module name) included in the error message to aid debugging.</p> <code>None</code> <p>Raises:</p> Type Description <code>DataFrameValidationError</code> <p>If one or more required columns are missing.</p> Notes <p>This function performs a presence-only check. It does not validate column dtypes or contents.</p>"},{"location":"packages/eb-evaluation/api/utils/validation/#eb_evaluation.utils.validation.ensure_non_empty","title":"<code>ensure_non_empty(df, *, context=None)</code>","text":"<p>Ensure that a DataFrame is not empty.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to validate.</p> required <code>context</code> <code>str | None</code> <p>Optional context string (e.g., function or module name) included in the error message to aid debugging.</p> <code>None</code> <p>Raises:</p> Type Description <code>DataFrameValidationError</code> <p>If the DataFrame has zero rows.</p> Notes <p>This check is commonly used after filtering or grouping operations to ensure downstream computations have at least one observation.</p>"},{"location":"packages/eb-features/","title":"eb-features","text":"<p><code>eb-features</code> provides feature engineering utilities used to construct model-ready inputs from raw or intermediate Electric Barometer data.</p> <p>This package focuses on feature definition and transformation, not model training, evaluation, or optimization policy logic.</p>"},{"location":"packages/eb-features/#scope","title":"Scope","text":"<p>This package is responsible for:</p> <ul> <li>Defining reusable feature transformations</li> <li>Encoding temporal, contextual, and structural signals</li> <li>Producing standardized feature sets for downstream modeling</li> <li>Supporting consistent feature semantics across the EB ecosystem</li> </ul> <p>It intentionally avoids evaluation logic, optimization policies, or runtime orchestration.</p>"},{"location":"packages/eb-features/#contents","title":"Contents","text":"<ul> <li> <p>Feature builders   Utilities for constructing derived features from raw inputs</p> </li> <li> <p>Feature transforms   Reusable transformations applied across modeling workflows</p> </li> </ul>"},{"location":"packages/eb-features/#api-reference","title":"API reference","text":"<ul> <li>Feature APIs</li> </ul>"},{"location":"packages/eb-features/api/calendar/","title":"Calendar Features","text":"<p>This section documents calendar-based feature extraction utilities provided by eb-features.</p> <p>Calendar features are derived from timestamp columns and may include cyclical encodings for periodic components.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-features/api/calendar/#calendar-feature-api","title":"Calendar Feature API","text":""},{"location":"packages/eb-features/api/calendar/#eb_features.panel.calendar","title":"<code>eb_features.panel.calendar</code>","text":"<p>Calendar and time-derived features for panel time series.</p> <p>This module provides utilities to derive calendar/time features from a timestamp column. It is designed for panel time-series data (entity-by-timestamp) and is typically used as part of a broader feature engineering pipeline.</p> Supported base calendar features <p>Given a timestamp column <code>timestamp_col</code>:</p> <ul> <li><code>\"hour\"</code>: Hour of day in <code>[0, 23]</code></li> <li><code>\"dow\"</code>: Day of week in <code>[0, 6]</code> where Monday=0 (pandas convention)</li> <li><code>\"dom\"</code>: Day of month in <code>[1, 31]</code></li> <li><code>\"month\"</code>: Month in <code>[1, 12]</code></li> <li><code>\"is_weekend\"</code>: Weekend indicator (Saturday/Sunday) as <code>0/1</code></li> </ul> Optional cyclical encodings <p>Certain calendar attributes are periodic and can be represented with sine/cosine pairs:</p> <ul> <li>hour (period 24):</li> </ul> \\[ \\sin\\left(2\\pi \\frac{\\mathrm{hour}}{24}\\right),\\quad \\cos\\left(2\\pi \\frac{\\mathrm{hour}}{24}\\right) \\] <ul> <li>day of week (period 7):</li> </ul> \\[ \\sin\\left(2\\pi \\frac{\\mathrm{dow}}{7}\\right),\\quad \\cos\\left(2\\pi \\frac{\\mathrm{dow}}{7}\\right) \\] <p>These encodings are added only when: - the corresponding base feature (hour or dow) is present, and - <code>use_cyclical_time=True</code>.</p> Notes <ul> <li>Feature construction is stateless: functions operate only on the provided DataFrame.</li> <li>Time features are derived from <code>pandas.to_datetime</code> conversion of <code>timestamp_col</code>.   Timezone-aware timestamps are supported; feature values reflect the timestamp's local   representation as stored in the column.</li> </ul>"},{"location":"packages/eb-features/api/calendar/#eb_features.panel.calendar.add_calendar_features","title":"<code>add_calendar_features(df, *, timestamp_col, calendar_features, use_cyclical_time=True)</code>","text":"<p>Add calendar/time-derived features to a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing a timestamp column.</p> required <code>timestamp_col</code> <code>str</code> <p>Name of the timestamp column.</p> required <code>calendar_features</code> <code>Sequence[str]</code> <p>Base calendar features to derive. Allowed values are: <code>{\"hour\", \"dow\", \"dom\", \"month\", \"is_weekend\"}</code>.</p> required <code>use_cyclical_time</code> <code>bool</code> <p>If True, add sine/cosine encodings for hour and/or day-of-week when those base features are included.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_out</code> <code>DataFrame</code> <p>Copy of <code>df</code> with the requested calendar features added as columns.</p> <code>feature_cols</code> <code>list[str]</code> <p>Names of all features added by this call, including cyclical encodings (if any).</p> <code>calendar_cols</code> <code>list[str]</code> <p>Names of base calendar feature columns added (excludes cyclical encodings).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>timestamp_col</code> is not present in <code>df</code>.</p> <code>ValueError</code> <p>If an unsupported calendar feature is requested.</p> Notes <p>The derived columns are currently named:</p> <ul> <li><code>hour</code></li> <li><code>dayofweek</code> (for requested <code>\"dow\"</code>)</li> <li><code>dayofmonth</code> (for requested <code>\"dom\"</code>)</li> <li><code>month</code></li> <li><code>is_weekend</code></li> </ul> <p>This naming is stable and intended to be referenced by downstream steps.</p>"},{"location":"packages/eb-features/api/config/","title":"Feature Configuration","text":"<p>This section documents configuration objects used to control panel feature engineering behavior in eb-features.</p> <p>Configuration objects define which features are built, how leakage is handled, and how missing history is treated.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-features/api/config/#configuration-api","title":"Configuration API","text":""},{"location":"packages/eb-features/api/config/#eb_features.panel.config","title":"<code>eb_features.panel.config</code>","text":"<p>Configuration objects for panel time-series feature engineering.</p> <p>This module defines declarative configuration used by the panel feature engineering pipeline. The configuration is designed to be:</p> <ul> <li>frequency-agnostic (lags/windows are expressed in index steps, not wall-clock time)</li> <li>stateless (config describes what to compute; no fitted state is stored)</li> <li>explicit (feature families are enabled/disabled via lists/flags)</li> </ul> Notes <p>This configuration is consumed by <code>FeatureEngineer</code> and related helper modules (lags, rolling, calendar, encoders).</p>"},{"location":"packages/eb-features/api/config/#eb_features.panel.config.FeatureConfig","title":"<code>FeatureConfig</code>  <code>dataclass</code>","text":"<p>Configuration for panel time-series feature engineering.</p> <p>The feature engineering pipeline assumes a long-form panel DataFrame with an entity identifier column, a timestamp column, and a numeric target column. This configuration describes which feature families to generate and which passthrough columns to include.</p> Notes <ul> <li>Lag steps and rolling windows are expressed in index steps (rows) at the input   frequency, not in wall-clock units.</li> <li>Lags and rolling windows are computed within each entity.</li> <li>If <code>dropna=True</code>, rows lacking sufficient lag/rolling history are dropped after   feature construction.</li> </ul> <p>Attributes:</p> Name Type Description <code>lag_steps</code> <code>Sequence[int] | None</code> <p>Positive lag offsets (in steps) applied to the target. For each <code>k</code> in <code>lag_steps</code>, the feature <code>lag_{k}</code> is added.</p> <code>rolling_windows</code> <code>Sequence[int] | None</code> <p>Positive rolling window lengths (in steps) applied to the target. For each <code>w</code> in <code>rolling_windows</code> and each stat in <code>rolling_stats</code>, the feature <code>roll_{w}_{stat}</code> is added.</p> <code>rolling_stats</code> <code>Sequence[str]</code> <p>Rolling statistics to compute. Allowed values are: <code>{\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\"}</code>.</p> <code>calendar_features</code> <code>Sequence[str]</code> <p>Calendar features derived from the timestamp column. Allowed values are: <code>{\"hour\", \"dow\", \"dom\", \"month\", \"is_weekend\"}</code>.</p> <code>use_cyclical_time</code> <code>bool</code> <p>If True, add sine/cosine encodings for hour and day-of-week when those base calendar columns are present.</p> <code>regressor_cols</code> <code>Sequence[str] | None</code> <p>Numeric external regressors to pass through. If None, numeric columns may be auto-detected by the calling pipeline (excluding entity/timestamp/target and <code>static_cols</code>).</p> <code>static_cols</code> <code>Sequence[str] | None</code> <p>Entity-level metadata columns already present on the input DataFrame. These are passed through directly as features.</p> <code>dropna</code> <code>bool</code> <p>If True, drop rows with NaNs in any engineered feature columns (typically caused by lags and rolling windows).</p>"},{"location":"packages/eb-features/api/constants/","title":"Constants","text":"<p>This section documents shared constants used throughout eb-features to define allowed configuration values and feature semantics.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-features/api/constants/#constants-api","title":"Constants API","text":""},{"location":"packages/eb-features/api/constants/#eb_features.panel.constants","title":"<code>eb_features.panel.constants</code>","text":"<p>Shared constants for panel feature engineering.</p> <p>This module centralizes small, stable configuration values used across the <code>eb_features.panel</code> subpackage. Keeping these definitions in one place prevents validation drift between modules and provides a single reference point for both implementation and documentation.</p> Notes <ul> <li>These constants are intentionally minimal and low-churn.</li> <li>They define allowed values, default configurations, and calendar parameters   used consistently across feature builders.</li> </ul>"},{"location":"packages/eb-features/api/constants/#eb_features.panel.constants.ALLOWED_ROLLING_STATS","title":"<code>ALLOWED_ROLLING_STATS = frozenset({'mean', 'std', 'min', 'max', 'sum', 'median'})</code>  <code>module-attribute</code>","text":"<p>Allowed rolling-window summary statistics.</p> <p>Each statistic corresponds to a feature name of the form:</p> \\[ \\mathrm{roll\\_{w}\\_{stat}}(t) \\] <p>where <code>w</code> is the window length (in index steps) and <code>stat</code> is one of the allowed values.</p>"},{"location":"packages/eb-features/api/constants/#eb_features.panel.constants.ALLOWED_CALENDAR_FEATURES","title":"<code>ALLOWED_CALENDAR_FEATURES = frozenset({'hour', 'dow', 'dom', 'month', 'is_weekend'})</code>  <code>module-attribute</code>","text":"<p>Allowed calendar features derived from the timestamp column.</p> <p>Calendar features are added as integer-valued columns and may optionally be accompanied by cyclical encodings (sine/cosine) for periodic components.</p>"},{"location":"packages/eb-features/api/constants/#eb_features.panel.constants.HOUR_PERIOD","title":"<code>HOUR_PERIOD = 24</code>  <code>module-attribute</code>","text":"<p>Period used for cyclical hour-of-day encodings.</p>"},{"location":"packages/eb-features/api/constants/#eb_features.panel.constants.DOW_PERIOD","title":"<code>DOW_PERIOD = 7</code>  <code>module-attribute</code>","text":"<p>Period used for cyclical day-of-week encodings.</p>"},{"location":"packages/eb-features/api/constants/#eb_features.panel.constants.WEEKEND_DAYS","title":"<code>WEEKEND_DAYS = (5, 6)</code>  <code>module-attribute</code>","text":"<p>Day-of-week values corresponding to Saturday and Sunday.</p>"},{"location":"packages/eb-features/api/encoders/","title":"Feature Encoding","text":"<p>This section documents encoding utilities provided by eb-features for transforming non-numeric features into numeric representations.</p> <p>These encoders are intentionally stateless and operate only on the provided dataset.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-features/api/encoders/#encoding-api","title":"Encoding API","text":""},{"location":"packages/eb-features/api/encoders/#eb_features.panel.encoders","title":"<code>eb_features.panel.encoders</code>","text":"<p>Encoding utilities for panel feature engineering.</p> <p>This module provides small, stateless helpers to make feature matrices numeric.</p> Current scope <p>The panel feature engineering pipeline produces a feature frame that may contain a mixture of numeric and non-numeric columns (e.g., entity metadata strings). Many downstream estimators expect purely numeric arrays. The helper in this module encodes non-numeric columns using pandas categorical codes.</p> Important <p>Categorical codes are stable only within the provided DataFrame. Because this module is intentionally stateless (no fitted mapping is persisted), codes may differ between training and inference if category sets or ordering differ.</p> <p>For production modeling pipelines that require consistent encodings across datasets, consider: - pre-encoding categoricals upstream (one-hot, target encoding, etc.), or - introducing a fitted encoder with persisted category mappings.</p>"},{"location":"packages/eb-features/api/encoders/#eb_features.panel.encoders.encode_non_numeric_as_category_codes","title":"<code>encode_non_numeric_as_category_codes(feature_frame, *, columns=None, dtype='int32')</code>","text":"<p>Encode non-numeric feature columns as categorical codes.</p> <p>Parameters:</p> Name Type Description Default <code>feature_frame</code> <code>DataFrame</code> <p>Feature DataFrame whose columns will be encoded as needed.</p> required <code>columns</code> <code>Iterable[str] | None</code> <p>Columns to consider for encoding. If None, all columns are considered.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Output dtype for encoded columns. (Booleans are converted to 0/1 and cast to this dtype; categorical codes are integers cast to this dtype.)</p> <code>\"int32\"</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Copy of <code>feature_frame</code> where: - boolean columns are converted to {0, 1} - non-numeric, non-boolean columns are replaced by categorical integer codes</p> Notes <ul> <li>Missing values in non-numeric columns are assigned the code <code>-1</code> by pandas.</li> <li>Category ordering is made deterministic by sorting observed values by their   string representation before assigning codes.</li> </ul>"},{"location":"packages/eb-features/api/engineering/","title":"Feature Engineering","text":"<p>This section documents the panel feature engineering orchestration utilities provided by eb-features.</p> <p>These tools transform entity \u00d7 timestamp panel data into model-ready feature matrices while enforcing leakage safety, monotonic time ordering, and deterministic feature construction.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-features/api/engineering/#feature-engineering-api","title":"Feature Engineering API","text":""},{"location":"packages/eb-features/api/engineering/#eb_features.panel.engineering","title":"<code>eb_features.panel.engineering</code>","text":"<p>Panel feature engineering orchestrator.</p> <p>This module defines a lightweight, frequency-agnostic feature engineering utility for panel time-series data (entity-by-timestamp). The implementation is intentionally stateless: each call constructs features from the provided input DataFrame and configuration.</p> <p>The output is designed for classical supervised learning pipelines that expect a fixed-width design matrix <code>X</code> and target vector <code>y</code>.</p> Features <p>Given an entity identifier column and a target series <code>y_t</code> (per entity), the feature pipeline can construct:</p> <p>1) Lag features:</p> \\[ \\mathrm{lag}_k(t) = y_{t-k} \\] <p>2) Rolling window statistics over the last <code>w</code> observations (leakage-safe by default):</p> \\[ \\mathrm{roll\\_mean}_w(t) = \\frac{1}{w}\\sum_{j=1}^{w} y_{t-j} \\] <p>3) Calendar features derived from timestamp: hour, day-of-week, day-of-month, month, and weekend indicator.</p> <p>4) Optional cyclical encodings for periodic calendar features:</p> \\[ \\sin\\left(2\\pi \\frac{\\mathrm{hour}}{24}\\right), \\quad \\cos\\left(2\\pi \\frac{\\mathrm{hour}}{24}\\right) \\] <p>and similarly for day-of-week with period 7.</p> <p>5) Optional passthrough features: numeric regressors and static metadata columns.</p> Notes <ul> <li>Lags and rolling windows are expressed in index steps (rows) at the input frequency.</li> <li>All time-dependent features are computed strictly within each entity.</li> <li>Passthrough non-numeric columns are encoded using stable integer category codes for the   values present in the provided DataFrame.</li> </ul>"},{"location":"packages/eb-features/api/engineering/#eb_features.panel.engineering.FeatureConfig","title":"<code>FeatureConfig</code>  <code>dataclass</code>","text":"<p>Configuration for panel time-series feature engineering.</p>"},{"location":"packages/eb-features/api/engineering/#eb_features.panel.engineering.FeatureEngineer","title":"<code>FeatureEngineer</code>","text":"<p>Transform panel time-series data into a model-ready <code>(X, y, feature_names)</code> triple.</p>"},{"location":"packages/eb-features/api/engineering/#eb_features.panel.engineering.FeatureEngineer.transform","title":"<code>transform(df, config)</code>","text":"<p>Transform a panel DataFrame into <code>(X, y, feature_names)</code>.</p>"},{"location":"packages/eb-features/api/lags/","title":"Lag Features","text":"<p>This section documents lag-based feature construction utilities provided by eb-features.</p> <p>Lag features are computed strictly within entity boundaries and expressed in index steps relative to the input frequency.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-features/api/lags/#lag-feature-api","title":"Lag Feature API","text":""},{"location":"packages/eb-features/api/lags/#eb_features.panel.lags","title":"<code>eb_features.panel.lags</code>","text":"<p>Lag feature construction for panel time series.</p> <p>This module provides stateless utilities to construct lagged versions of a target series within each entity of a panel (entity-by-timestamp) dataset.</p> <p>Lag features are expressed in index steps (rows) at the input frequency rather than wall-clock units. This makes the transformation frequency-agnostic (5-min, 30-min, hourly, daily, etc.), assuming the panel is sorted by timestamp within each entity.</p> Definition <p>For a target series <code>y_t</code> and lag step <code>k</code>:</p> \\[ \\mathrm{lag}_k(t) = y_{t-k} \\] <p>The resulting feature column is named <code>lag_{k}</code>.</p> Notes <ul> <li>Lag features are computed strictly within each entity using grouped shifts.</li> <li>The calling pipeline is responsible for handling missing values introduced by lagging   (e.g., dropping rows or applying imputation).</li> </ul>"},{"location":"packages/eb-features/api/lags/#eb_features.panel.lags.add_lag_features","title":"<code>add_lag_features(df, *, entity_col, target_col, lag_steps)</code>","text":"<p>Add target lag features to a panel DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing at least <code>entity_col</code> and <code>target_col</code>.</p> required <code>entity_col</code> <code>str</code> <p>Name of the entity identifier column.</p> required <code>target_col</code> <code>str</code> <p>Name of the numeric target column to be lagged.</p> required <code>lag_steps</code> <code>Sequence[int] | None</code> <p>Positive lag offsets (in steps). For each <code>k</code> in <code>lag_steps</code>, the feature <code>lag_{k}</code> is added. If None or empty, no lag features are added.</p> required <p>Returns:</p> Name Type Description <code>df_out</code> <code>DataFrame</code> <p>Copy of <code>df</code> with lag feature columns added.</p> <code>feature_cols</code> <code>list[str]</code> <p>Names of the lag feature columns added.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>entity_col</code> or <code>target_col</code> is missing from <code>df</code>.</p> <code>ValueError</code> <p>If any lag step is non-positive.</p> Notes <p>Lagging introduces missing values for the first <code>k</code> observations of each entity. These are typically removed downstream when <code>dropna=True</code> or handled via imputation.</p>"},{"location":"packages/eb-features/api/rolling/","title":"Rolling Window Features","text":"<p>This section documents rolling-window feature construction utilities provided by eb-features.</p> <p>Rolling features support multiple statistics and optional leakage-safe computation.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-features/api/rolling/#rolling-feature-api","title":"Rolling Feature API","text":""},{"location":"packages/eb-features/api/rolling/#eb_features.panel.rolling","title":"<code>eb_features.panel.rolling</code>","text":"<p>Rolling window feature construction for panel time series.</p> <p>This module provides stateless utilities to compute rolling window statistics of a target series within each entity of a panel (entity-by-timestamp) dataset.</p> <p>Rolling windows are expressed in index steps (rows) at the input frequency rather than wall-clock units. This keeps the transformation frequency-agnostic.</p> Definitions <p>For a target series <code>y_t</code> and a window length <code>w</code>, the rolling mean feature is:</p> \\[ \\mathrm{roll\\_mean}_w(t) = \\frac{1}{w}\\sum_{j=1}^{w} y_{t-j} \\] <p>This formulation explicitly uses past values only (<code>y_{t-1}</code> through <code>y_{t-w}</code>), which avoids target leakage when predicting at time <code>t</code>.</p> <p>The resulting feature columns are named <code>roll_{w}_{stat}</code>, e.g., <code>roll_24_mean</code>.</p> Notes <ul> <li>Rolling features are computed strictly within each entity.</li> <li>By default this module computes rolling statistics on <code>target.shift(1)</code> within each   entity, so that the current target value <code>y_t</code> is not included in the window.</li> <li>The calling pipeline is responsible for sorting data by <code>(entity, timestamp)</code> and for   deciding how to handle NaNs introduced by rolling windows (e.g., dropping rows).</li> </ul>"},{"location":"packages/eb-features/api/rolling/#eb_features.panel.rolling.add_rolling_features","title":"<code>add_rolling_features(df, *, entity_col, target_col, rolling_windows, rolling_stats, min_periods=None, leakage_safe=True)</code>","text":"<p>Add rolling window statistics features to a panel DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing at least <code>entity_col</code> and <code>target_col</code>.</p> required <code>entity_col</code> <code>str</code> <p>Name of the entity identifier column.</p> required <code>target_col</code> <code>str</code> <p>Name of the numeric target column used to compute rolling statistics.</p> required <code>rolling_windows</code> <code>Sequence[int] | None</code> <p>Positive rolling window lengths (in steps). For each <code>w</code> in <code>rolling_windows</code> and each stat in <code>rolling_stats</code>, the feature <code>roll_{w}_{stat}</code> is added. If None or empty, no rolling features are added.</p> required <code>rolling_stats</code> <code>Sequence[str]</code> <p>Rolling statistics to compute. Allowed values are: <code>{\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\"}</code>.</p> required <code>min_periods</code> <code>int | None</code> <p>Minimum number of observations in the window required to produce a value. If None, defaults to <code>w</code> (full-window requirement).</p> <code>None</code> <code>leakage_safe</code> <code>bool</code> <p>If True, compute rolling statistics on the lagged target (<code>target.shift(1)</code> within each entity), so that the current <code>y_t</code> is excluded from the window.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_out</code> <code>DataFrame</code> <p>Copy of <code>df</code> with rolling feature columns added.</p> <code>feature_cols</code> <code>list[str]</code> <p>Names of the rolling feature columns added.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>entity_col</code> or <code>target_col</code> is missing from <code>df</code>.</p> <code>ValueError</code> <p>If any rolling window is non-positive, if <code>min_periods</code> is invalid, or if an unsupported stat is requested.</p> Notes <p>Rolling features introduce missing values at the beginning of each entity's series. These are typically removed downstream when <code>dropna=True</code> or handled via imputation.</p>"},{"location":"packages/eb-features/api/validation/","title":"Input Validation","text":"<p>This section documents validation utilities provided by eb-features to enforce structural assumptions on panel time-series data.</p> <p>These helpers ensure column presence, monotonic timestamps, and feature integrity.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-features/api/validation/#validation-api","title":"Validation API","text":""},{"location":"packages/eb-features/api/validation/#eb_features.panel.validation","title":"<code>eb_features.panel.validation</code>","text":"<p>Validation utilities for panel feature engineering.</p> <p>This module centralizes lightweight input validation helpers used throughout the panel feature engineering subpackage.</p> Key invariants <ul> <li>Required columns must exist.</li> <li>Within each entity, timestamps must be strictly increasing in the given row order.   (No sorting is performed inside validation; callers may sort afterward for deterministic   computation, but validation should catch out-of-order input.)</li> </ul>"},{"location":"packages/eb-features/api/validation/#eb_features.panel.validation.validate_required_columns","title":"<code>validate_required_columns(df, *, required_cols)</code>","text":"<p>Validate that required columns exist on a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>required_cols</code> <code>Sequence[str]</code> <p>Columns that must be present.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any required columns are missing.</p>"},{"location":"packages/eb-features/api/validation/#eb_features.panel.validation.ensure_columns_present","title":"<code>ensure_columns_present(df, *, columns, label)</code>","text":"<p>Ensure that a set of configured columns exists on a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>columns</code> <code>Iterable[str]</code> <p>Columns that must be present.</p> required <code>label</code> <code>str</code> <p>Human-readable label used in error messages (e.g., \"Static\", \"Regressor\").</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any specified columns are missing.</p>"},{"location":"packages/eb-features/api/validation/#eb_features.panel.validation.validate_monotonic_timestamps","title":"<code>validate_monotonic_timestamps(df, *, entity_col, timestamp_col)</code>","text":"<p>Validate that timestamps are strictly increasing within each entity in row order.</p> <p>This function does not sort. It checks the DataFrame exactly as provided.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input panel DataFrame.</p> required <code>entity_col</code> <code>str</code> <p>Entity identifier column.</p> required <code>timestamp_col</code> <code>str</code> <p>Timestamp column.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If required columns are missing.</p> <code>ValueError</code> <p>If any entity has non-strictly increasing timestamps (ties or out-of-order), or if timestamps cannot be parsed.</p>"},{"location":"packages/eb-metrics/","title":"eb-metrics","text":"<p>eb-metrics is the core metric library of the Electric Barometer ecosystem.</p> <p>It provides a principled set of error metrics and evaluation utilities designed for operational forecasting environments\u2014contexts where directional error, asymmetric cost, and service reliability matter more than symmetric accuracy alone.</p> <p>Unlike generic regression metrics, <code>eb-metrics</code> is built to evaluate forecasts in settings where underprediction and overprediction have materially different consequences, such as quick-service restaurants (QSR), retail operations, logistics, inventory planning, and other service-constrained systems.</p>"},{"location":"packages/eb-metrics/#what-this-package-provides","title":"What this package provides","text":""},{"location":"packages/eb-metrics/#asymmetric-cost-aware-loss-metrics","title":"Asymmetric, cost-aware loss metrics","text":"<p>Metrics that explicitly encode operational cost asymmetry between shortfall and overbuild.</p> <ul> <li>Cost-Weighted Service Loss (CWSL)   A demand-normalized, directionally-aware loss that generalizes weighted MAPE   by assigning explicit costs to underbuild and overbuild.</li> </ul>"},{"location":"packages/eb-metrics/#service-level-and-readiness-diagnostics","title":"Service-level and readiness diagnostics","text":"<p>Metrics that evaluate forecast behavior from a service reliability and operational readiness perspective.</p> <ul> <li>No Shortfall Level (NSL) \u2014 frequency of avoiding shortfall  </li> <li>Underbuild Depth (UD) \u2014 severity of shortfalls when they occur  </li> <li>Hit Rate within Tolerance (HR@\u03c4) \u2014 accuracy within operational bounds  </li> <li>Forecast Readiness Score (FRS) \u2014 composite readiness metric combining NSL and CWSL  </li> </ul>"},{"location":"packages/eb-metrics/#classical-regression-metrics","title":"Classical regression metrics","text":"<p>Standard symmetric error metrics included for baseline comparison and diagnostic validation.</p> <ul> <li>MAE, MSE, RMSE  </li> <li>MAPE, WMAPE, sMAPE  </li> <li>MedAE, MASE, MSLE, RMSLE  </li> </ul>"},{"location":"packages/eb-metrics/#framework-integrations","title":"Framework integrations","text":"<p>Adapters that allow Electric Barometer metrics to integrate cleanly into common machine-learning workflows.</p> <ul> <li>scikit-learn scorers (e.g., for <code>GridSearchCV</code>, <code>cross_val_score</code>)</li> <li>Keras / TensorFlow loss functions for cost-aware model training</li> </ul>"},{"location":"packages/eb-metrics/#documentation-structure","title":"Documentation structure","text":"<ul> <li>API Reference   All metric and framework documentation is generated automatically from   NumPy-style docstrings in the source code using <code>mkdocstrings</code>.</li> </ul> <p>Conceptual motivation, formal definitions, and interpretive guidance for these metrics are documented in the companion research repository eb-papers.</p>"},{"location":"packages/eb-metrics/#intended-audience","title":"Intended audience","text":"<p>This documentation is intended for:</p> <ul> <li>data scientists and applied ML practitioners</li> <li>forecasting and demand-planning teams</li> <li>operations and service analytics leaders</li> <li>researchers working in cost-sensitive or service-constrained environments</li> </ul> <p>The focus throughout is on decision-relevant evaluation, not abstract statistical accuracy.</p>"},{"location":"packages/eb-metrics/#relationship-to-the-electric-barometer-framework","title":"Relationship to the Electric Barometer framework","text":"<p><code>eb-metrics</code> provides the metric layer of the Electric Barometer ecosystem. It is designed to be used alongside:</p> <ul> <li>eb-evaluation \u2014 structured forecast evaluation workflows</li> <li>eb-adapters \u2014 integrations with external forecasting systems</li> <li>eb-papers \u2014 formal definitions, theory, and technical notes</li> </ul> <p>Together, these components support a unified approach to measuring forecast readiness, not just forecast error.</p>"},{"location":"packages/eb-metrics/api/frameworks/","title":"Framework Integrations","text":"<p>This section documents framework-specific integration utilities provided by eb-metrics, enabling Electric Barometer metrics to be used directly in model training and selection workflows.</p> <p>All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-metrics/api/frameworks/#framework-utilities","title":"Framework Utilities","text":""},{"location":"packages/eb-metrics/api/frameworks/#eb_metrics.frameworks","title":"<code>eb_metrics.frameworks</code>","text":"<p>Framework integrations for Electric Barometer metrics.</p> <p>The <code>eb_metrics.frameworks</code> package provides optional adapters that allow Electric Barometer metrics to plug into common machine-learning and forecasting workflows (training, model selection, and evaluation) without redefining metric semantics.</p> <p>Currently supported integrations include:</p> <ul> <li>scikit-learn scorers (for model selection utilities such as <code>GridSearchCV</code>)</li> <li>Keras / TensorFlow loss functions (for training deep learning models)</li> </ul> Notes <ul> <li>These integrations are thin wrappers around core Electric Barometer metrics.   Metric definitions live in :mod:<code>eb_metrics.metrics</code>.</li> <li>Some integrations rely on optional third-party dependencies and may import   those dependencies lazily (e.g., TensorFlow).</li> </ul> <p>Conceptual definitions and interpretation are documented in the companion research repository (<code>eb-papers</code>).</p>"},{"location":"packages/eb-metrics/api/frameworks/#eb_metrics.frameworks.make_cwsl_keras_loss","title":"<code>make_cwsl_keras_loss(cu, co)</code>","text":"<p>Create a Keras-compatible loss implementing Cost-Weighted Service Loss (CWSL).</p> <p>This factory returns a TensorFlow/Keras loss function that mirrors the NumPy-based CWSL metric but is shaped to work in deep learning workflows.</p> <p>The returned loss function has signature:</p> <p><code>loss(y_true, y_pred) -&gt; tf.Tensor</code></p> <p>and returns a tensor of shape <code>(batch_size,)</code> (one loss value per example), which Keras will then reduce according to the configured reduction mode.</p> <p>For each sample, CWSL is computed as:</p> \\[ \\begin{aligned} s &amp;= \\max(0, y - \\hat{y}) \\\\ o &amp;= \\max(0, \\hat{y} - y) \\\\ \\mathrm{CWSL} &amp;= \\frac{c_u \\sum s + c_o \\sum o}{\\sum y} \\end{aligned} \\] <p>where the summations reduce over the final axis (e.g., forecast horizon).</p> <p>Parameters:</p> Name Type Description Default <code>cu</code> <code>float</code> <p>Per-unit cost of underbuild (shortfall). Must be strictly positive.</p> required <code>co</code> <code>float</code> <p>Per-unit cost of overbuild (excess). Must be strictly positive.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A Keras-compatible loss function <code>loss(y_true, y_pred)</code> returning a per-sample CWSL tensor of shape <code>(batch_size,)</code>.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If TensorFlow is not installed.</p> <code>ValueError</code> <p>If <code>cu</code> or <code>co</code> are not strictly positive.</p> Notes <ul> <li>Inputs are cast to <code>tf.float32</code>.</li> <li>The reduction is performed over the last axis. This supports both:</li> <li><code>(batch_size, horizon)</code> tensors (typical forecasting setup), and</li> <li><code>(batch_size,)</code> tensors (single-step forecasting), where reduction over     the last axis still behaves correctly.</li> <li>Division by zero is avoided by clamping the per-sample demand denominator   with Keras epsilon.</li> </ul> References <p>Electric Barometer Technical Note: Cost-Weighted Service Loss (CWSL).</p>"},{"location":"packages/eb-metrics/api/frameworks/#eb_metrics.frameworks.cwsl_loss","title":"<code>cwsl_loss(y_true, y_pred, *, cu, co, sample_weight=None)</code>","text":"<p>Compute the (positive) Cost-Weighted Service Loss (CWSL) for scikit-learn usage.</p> <p>This helper is a thin wrapper around :func:<code>eb_metrics.metrics.cwsl</code> that:</p> <ul> <li>enforces strict positivity of <code>cu</code> and <code>co</code> (to align with typical   scikit-learn scorer usage), and</li> <li>converts inputs to <code>numpy.ndarray</code> prior to evaluation.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Realized demand values.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Forecast values.</p> required <code>cu</code> <code>float</code> <p>Per-unit cost of underbuild (shortfall). Must be strictly positive.</p> required <code>co</code> <code>float</code> <p>Per-unit cost of overbuild (excess). Must be strictly positive.</p> required <code>sample_weight</code> <code>array-like of shape (n_samples,)</code> <p>Optional per-sample weights passed through to CWSL.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Positive CWSL value (lower is better).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>cu</code> or <code>co</code> are not strictly positive, or if the underlying CWSL computation is undefined for the given inputs.</p> Notes <p>CWSL is defined as a demand-normalized asymmetric cost:</p> \\[ \\mathrm{CWSL} = \\frac{\\sum_i w_i \\left(c_u s_i + c_o o_i\\right)}{\\sum_i w_i y_i}, \\quad s_i = \\max(0, y_i-\\hat{y}_i), \\quad o_i = \\max(0, \\hat{y}_i-y_i) \\] References <p>Electric Barometer Technical Note: Cost-Weighted Service Loss (CWSL).</p>"},{"location":"packages/eb-metrics/api/frameworks/#eb_metrics.frameworks.cwsl_scorer","title":"<code>cwsl_scorer(cu, co)</code>","text":"<p>Build a scikit-learn scorer based on Cost-Weighted Service Loss (CWSL).</p> <p>The returned object can be used wherever scikit-learn expects a scorer, for example:</p> <ul> <li><code>GridSearchCV(..., scoring=cwsl_scorer(cu=2.0, co=1.0))</code></li> <li><code>cross_val_score(..., scoring=cwsl_scorer(cu=2.0, co=1.0))</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>cu</code> <code>float</code> <p>Per-unit cost of underbuild (shortfall). Must be strictly positive.</p> required <code>co</code> <code>float</code> <p>Per-unit cost of overbuild (excess). Must be strictly positive.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A scikit-learn scorer that returns negative CWSL (higher is better).</p> Notes <p>scikit-learn assumes scores are maximized. Because CWSL is a loss (lower is better), this scorer is configured with <code>greater_is_better=False</code> so scikit-learn negates the value internally.</p> <p>In other words, the score returned by scikit-learn is:</p> \\[ \\text{score} = -\\,\\mathrm{CWSL} \\] References <p>Electric Barometer Technical Note: Cost-Weighted Service Loss (CWSL).</p>"},{"location":"packages/eb-metrics/api/metrics/","title":"Metrics API","text":"<p>This section documents the core metric implementations provided by eb-metrics. All content below is generated automatically from NumPy-style docstrings in the source code.</p>"},{"location":"packages/eb-metrics/api/metrics/#metrics-package","title":"Metrics Package","text":""},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics","title":"<code>eb_metrics.metrics</code>","text":"<p>Public metric API for the Electric Barometer ecosystem.</p> <p>The <code>eb_metrics.metrics</code> package provides a curated, stable import surface for Electric Barometer evaluation metrics.</p> <p>This package groups metrics into four main categories:</p> <ul> <li> <p>Asymmetric loss metrics (<code>loss</code>)   Cost-aware losses that encode directional operational asymmetry.</p> </li> <li> <p>Service and readiness diagnostics (<code>service</code>)   Metrics that quantify shortfall avoidance, tolerance coverage, and readiness.</p> </li> <li> <p>Classical regression metrics (<code>regression</code>)   Standard symmetric error metrics used for baseline comparison.</p> </li> <li> <p>Cost-ratio utilities (<code>cost_ratio</code>)   Helpers for selecting and analyzing the asymmetric cost ratio   :math:<code>R = c_u / c_o</code>.</p> </li> </ul> <p>Conceptual definitions and interpretation of Electric Barometer metrics are documented in the companion research repository (<code>eb-papers</code>). This package is the executable reference implementation.</p> Notes <p>Users are encouraged to import from <code>eb_metrics.metrics</code> or from the relevant submodule (e.g., <code>eb_metrics.metrics.service</code>) rather than internal helpers.</p> <p>Examples:</p> <p>Import from the package surface:</p> <pre><code>&gt;&gt;&gt; from eb_metrics.metrics import cwsl, nsl, frs\n</code></pre> <p>Or import from a submodule:</p> <pre><code>&gt;&gt;&gt; from eb_metrics.metrics.loss import cwsl\n&gt;&gt;&gt; from eb_metrics.metrics.service import nsl\n</code></pre>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.estimate_R_cost_balance","title":"<code>estimate_R_cost_balance(y_true, y_pred, R_grid=(0.5, 1.0, 2.0, 3.0), co=1.0, sample_weight=None)</code>","text":"<p>Estimate a global cost ratio \\(R = c_u / c_o\\) via cost balance.</p> <p>This routine selects a single, global cost ratio \\(R\\) by searching a candidate grid and choosing the value where the total weighted underbuild cost is closest to the total weighted overbuild cost.</p> <p>For each candidate \\(R\\) in <code>R_grid</code>:</p> \\[ \\begin{aligned} c_{u,i} &amp;= R \\cdot c_{o,i} \\\\ s_i &amp;= \\max(0, y_i - \\hat{y}_i) \\\\ e_i &amp;= \\max(0, \\hat{y}_i - y_i) \\\\ C_u(R) &amp;= \\sum_i w_i \\; c_{u,i} \\; s_i \\\\ C_o(R) &amp;= \\sum_i w_i \\; c_{o,i} \\; e_i \\end{aligned} \\] <p>and the selected value is:</p> \\[ R^* = \\arg\\min_R \\; \\left| C_u(R) - C_o(R) \\right|. \\] <p>The returned \\(R^*\\) can be used as: - a reasonable default global cost ratio for evaluation, and/or - the center of a sensitivity sweep (e.g., <code>{R*/2, R*, 2*R*}</code>).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Realized demand (non-negative).</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Forecast demand (non-negative). Must have the same shape as <code>y_true</code>.</p> required <code>R_grid</code> <code>sequence of float</code> <p>Candidate cost ratios \\(R\\) to search over. Only strictly positive values are considered.</p> <code>(0.5, 1.0, 2.0, 3.0)</code> <code>co</code> <code>float or array-like of shape (n_samples,)</code> <p>Overbuild cost \\(c_o\\) per unit. Can be:</p> <ul> <li>scalar: same overbuild cost for all intervals</li> <li>1D array: per-interval overbuild cost</li> </ul> <p>For each \\(R\\), the implied underbuild cost is \\(c_{u,i} = R \\cdot c_{o,i}\\).</p> <code>1.0</code> <code>sample_weight</code> <code>float or array-like of shape (n_samples,)</code> <p>Optional non-negative weights per interval used to weight the cost aggregation. If <code>None</code>, all intervals receive weight <code>1.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The value in <code>R_grid</code> that minimizes \\(\\left| C_u(R) - C_o(R) \\right|\\).</p> <p>If multiple values yield the same minimal gap, the first such value in the (filtered) grid is returned, except in the degenerate perfect forecast case (zero error everywhere), where the candidate closest to <code>1.0</code> is returned.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid (e.g., negative <code>y_true</code> or <code>y_pred</code>), if <code>R_grid</code> is empty, or if it contains no positive values.</p> Notes <p>This helper is intentionally simple: it does not infer cost structure from business inputs, nor does it estimate per-item costs. It provides a reproducible, data-driven way to select a reasonable global \\(R\\) given realized outcomes and forecast behavior.</p> References <p>Electric Barometer Technical Note: Cost Ratio Estimation (Choosing \\(R\\)).</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.cwsl","title":"<code>cwsl(y_true, y_pred, cu, co, sample_weight=None)</code>","text":"<p>Compute Cost-Weighted Service Loss (CWSL).</p> <p>CWSL is a demand-normalized, directionally-aware loss that penalizes shortfalls and overbuilds using explicit per-unit costs.</p> <p>For each interval \\(i\\):</p> \\[ \\begin{aligned} s_i &amp;= \\max(0, y_i - \\hat{y}_i) \\\\ o_i &amp;= \\max(0, \\hat{y}_i - y_i) \\\\ \\text{cost}_i &amp;= c_{u,i} \\; s_i + c_{o,i} \\; o_i \\end{aligned} \\] <p>and the aggregated metric is:</p> \\[ \\mathrm{CWSL} = \\frac{\\sum_i w_i \\; \\text{cost}_i}{\\sum_i w_i \\; y_i} \\] <p>where \\(w_i\\) are optional sample weights (default \\(w_i = 1\\)). Lower values indicate better performance.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Realized demand \\(y\\). Must be non-negative.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Forecast demand \\(\\hat{y}\\). Must be non-negative and have the same shape as <code>y_true</code>.</p> required <code>cu</code> <code>float or array-like of shape (n_samples,)</code> <p>Per-unit shortfall cost \\(c_u\\). Can be a scalar (global cost) or a 1D array specifying per-interval costs. Must be non-negative.</p> required <code>co</code> <code>float or array-like of shape (n_samples,)</code> <p>Per-unit overbuild cost \\(c_o\\). Can be a scalar (global cost) or a 1D array specifying per-interval costs. Must be non-negative.</p> required <code>sample_weight</code> <code>float or array-like of shape (n_samples,)</code> <p>Optional non-negative weights per interval. If <code>None</code>, all intervals receive weight <code>1.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The CWSL value. Lower is better.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_true</code> and <code>y_pred</code> have different shapes, if any demand or forecast values are negative, if any costs are negative, or if the metric is undefined due to zero total (weighted) demand with positive total (weighted) cost.</p> Notes <ul> <li>When <code>cu == co</code> (up to a constant scaling), CWSL behaves similarly to a   demand-normalized absolute error (wMAPE-like), but retains explicit cost   semantics.</li> <li>If total (weighted) demand is zero and total (weighted) cost is zero,   this implementation returns <code>0.0</code>.</li> <li>If total (weighted) demand is zero but total (weighted) cost is positive,   the metric is undefined under this formulation and a <code>ValueError</code> is   raised.</li> </ul> References <p>Electric Barometer Technical Note: Cost-Weighted Service Loss (CWSL).</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.mae","title":"<code>mae(y_true, y_pred)</code>","text":"<p>Mean Absolute Error (MAE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean absolute error. Lower is better.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.mase","title":"<code>mase(y_true, y_pred, y_naive)</code>","text":"<p>Mean Absolute Scaled Error (MASE).</p> <p>MASE scales the model MAE by the MAE of a naive forecast:</p> \\[ \\mathrm{MASE} = \\frac{\\mathrm{MAE}(y,\\hat{y})}{\\mathrm{MAE}(y, y^{\\text{naive}})} \\] <p>where <code>y_naive</code> is typically a naive baseline such as \\(y_{t-1}\\).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <code>y_naive</code> <code>array-like of shape (n_samples,)</code> <p>Naive forecast values aligned to <code>y_true</code>.</p> required <p>Returns:</p> Type Description <code>float</code> <p>MASE value. Lower is better.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the naive MAE is zero (MASE undefined).</p> Notes <p>MASE is scale-free and can be compared across series with different magnitudes, assuming the naive baseline is meaningful.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.medae","title":"<code>medae(y_true, y_pred)</code>","text":"<p>Median Absolute Error (MedAE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Median absolute error. Lower is better.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.mape","title":"<code>mape(y_true, y_pred)</code>","text":"<p>Mean Absolute Percentage Error (MAPE).</p> <p>MAPE is computed over samples where <code>y_true != 0</code>:</p> \\[ \\mathrm{MAPE} = 100 \\cdot \\mathrm{mean}\\left(\\left|\\frac{y-\\hat{y}}{y}\\right|\\right) \\] <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean absolute percentage error in percent. Lower is better.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If all values of <code>y_true</code> are zero (MAPE undefined).</p> Notes <p>MAPE can be unstable when <code>y_true</code> is near zero. Consider WMAPE or a domain-specific metric (e.g., CWSL) when percentage behavior is undesirable.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.mse","title":"<code>mse(y_true, y_pred)</code>","text":"<p>Mean Squared Error (MSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean squared error. Lower is better.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.msle","title":"<code>msle(y_true, y_pred)</code>","text":"<p>Mean Squared Log Error (MSLE).</p> <p>MSLE is defined as:</p> \\[ \\mathrm{MSLE} = \\mathrm{mean}\\left((\\log(1+y) - \\log(1+\\hat{y}))^2\\right) \\] <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values. Must be non-negative.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values. Must be non-negative.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean squared log error. Lower is better.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any value in <code>y_true</code> or <code>y_pred</code> is negative.</p> Notes <p>MSLE down-weights large absolute errors at high magnitudes and is commonly used when relative error is more meaningful than absolute error.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.rmse","title":"<code>rmse(y_true, y_pred)</code>","text":"<p>Root Mean Squared Error (RMSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Root mean squared error. Lower is better.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.rmsle","title":"<code>rmsle(y_true, y_pred)</code>","text":"<p>Root Mean Squared Log Error (RMSLE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values. Must be non-negative.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values. Must be non-negative.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Root mean squared log error. Lower is better.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.smape","title":"<code>smape(y_true, y_pred)</code>","text":"<p>Symmetric Mean Absolute Percentage Error (sMAPE).</p> <p>This implementation follows a common competition definition:</p> \\[ \\mathrm{sMAPE} = 200 \\cdot \\mathrm{mean}\\left(\\frac{|y-\\hat{y}|}{|y| + |\\hat{y}|}\\right) \\] <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>sMAPE in percent. Lower is better.</p> Notes <p>If <code>|y| + |y_pred| == 0</code> for all samples, this function returns <code>0.0</code>.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.wmape","title":"<code>wmape(y_true, y_pred)</code>","text":"<p>Weighted Mean Absolute Percentage Error (WMAPE).</p> <p>WMAPE is also commonly described as demand-normalized absolute error:</p> \\[ \\mathrm{WMAPE} = 100 \\cdot \\frac{\\sum_i |y_i-\\hat{y}_i|}{\\sum_i |y_i|} \\] <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Ground-truth values.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>WMAPE in percent. Lower is better.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>sum(|y_true|) == 0</code> (WMAPE undefined).</p> Notes <p>WMAPE is symmetric: it does not distinguish underprediction from overprediction. Use CWSL when directional cost asymmetry matters.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.cwsl_sensitivity","title":"<code>cwsl_sensitivity(y_true, y_pred, R_list=(0.5, 1.0, 2.0, 3.0), co=1.0, sample_weight=None)</code>","text":"<p>Evaluate CWSL across a grid of cost ratios (cost sensitivity analysis).</p> <p>This helper computes Cost-Weighted Service Loss (CWSL) for each candidate cost ratio:</p> \\[ R = c_u / c_o \\] <p>holding <code>co</code> fixed and setting:</p> \\[ c_u = R \\cdot c_o \\] <p>for each value in <code>R_list</code>.</p> <p>This provides a simple way to assess how model ranking or absolute loss changes under alternative assumptions about shortfall vs. overbuild cost.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Realized demand. Must be non-negative.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Forecast demand. Must be non-negative and have the same shape as <code>y_true</code>.</p> required <code>R_list</code> <code>sequence of float</code> <p>Candidate cost ratios to evaluate. Only strictly positive values are used.</p> <code>(0.5, 1.0, 2.0, 3.0)</code> <code>co</code> <code>float or array-like of shape (n_samples,)</code> <p>Overbuild cost \\(c_o\\). Can be scalar or per-interval.</p> <code>1.0</code> <code>sample_weight</code> <code>float or array-like of shape (n_samples,)</code> <p>Optional non-negative weights per interval, passed through to CWSL.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[float, float]</code> <p>Mapping <code>{R: cwsl_value}</code> for each valid <code>R</code> in <code>R_list</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>R_list</code> contains no positive values, or if inputs are invalid or CWSL is undefined for the given data slice.</p> Notes <ul> <li>This is a pure evaluation utility; it does not attempt to infer the   \u201ccorrect\u201d cost ratio. For that, see cost-ratio estimation utilities.</li> </ul> References <p>Electric Barometer Technical Note: Cost Sensitivity Utilities for CWSL.</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.frs","title":"<code>frs(y_true, y_pred, cu, co, sample_weight=None)</code>","text":"<p>Compute Forecast Readiness Score (FRS).</p> <p>FRS is a simple composite score defined as:</p> \\[ \\mathrm{FRS} = \\mathrm{NSL} - \\mathrm{CWSL} \\] <p>where: - NSL measures the frequency of avoiding shortfall (higher is better) - CWSL measures asymmetric, demand-normalized cost (lower is better)</p> <p>This construction rewards forecasts that simultaneously: - maintain high service reliability (high NSL), and - avoid costly asymmetric error (low CWSL).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Realized demand. Must be non-negative.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Forecast demand. Must be non-negative and have the same shape as <code>y_true</code>.</p> required <code>cu</code> <code>float or array-like of shape (n_samples,)</code> <p>Per-unit shortfall cost passed through to CWSL.</p> required <code>co</code> <code>float or array-like of shape (n_samples,)</code> <p>Per-unit overbuild cost passed through to CWSL.</p> required <code>sample_weight</code> <code>float or array-like of shape (n_samples,)</code> <p>Optional non-negative weights per interval, applied consistently to NSL and CWSL.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Forecast Readiness Score. Higher indicates better readiness. Values are typically bounded above by 1, but can be negative depending on cost and forecast error.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid or CWSL is undefined for the given data slice.</p> Notes <p>This metric is intentionally simple and should be interpreted as a readiness-oriented summary rather than a standalone loss function.</p> References <p>Electric Barometer Technical Note: Forecast Readiness Score (FRS).</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.hr_at_tau","title":"<code>hr_at_tau(y_true, y_pred, tau, sample_weight=None)</code>","text":"<p>Compute Hit Rate within Tolerance (HR@\u03c4).</p> <p>HR@\u03c4 measures the (optionally weighted) fraction of intervals whose absolute error falls within a tolerance band \\(\\tau\\).</p> <p>Define absolute error and hit indicator:</p> \\[ \\begin{aligned} e_i &amp;= |y_i - \\hat{y}_i| \\\\ h_i &amp;= \\mathbb{1}[e_i \\le \\tau_i] \\end{aligned} \\] <p>Then:</p> \\[ \\mathrm{HR@\\tau} = \\frac{\\sum_i w_i \\; h_i}{\\sum_i w_i} \\] <p>Higher values are better, with \\(\\mathrm{HR@\\tau} \\in [0, 1]\\).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Realized demand. Must be non-negative.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Forecast demand. Must be non-negative and have the same shape as <code>y_true</code>.</p> required <code>tau</code> <code>float or array-like of shape (n_samples,)</code> <p>Non-negative absolute error tolerance. Can be: - scalar: same tolerance for all intervals - 1D array: per-interval tolerance</p> required <code>sample_weight</code> <code>float or array-like of shape (n_samples,)</code> <p>Optional non-negative weights per interval. If provided, HR@\u03c4 is computed as a weighted fraction. If total weight is zero, HR@\u03c4 is undefined and a <code>ValueError</code> is raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>HR@\u03c4 value in [0, 1]. Higher indicates more intervals within tolerance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid (shape mismatch, negative values), if <code>tau</code> is negative anywhere, or if total sample weight is zero.</p> Notes <ul> <li>HR@\u03c4 is a symmetric tolerance measure; it treats underbuild and overbuild   equally within the tolerance band.</li> <li>Use HR@\u03c4 alongside asymmetric metrics (e.g., CWSL) when operational costs   differ by direction.</li> </ul> References <p>Electric Barometer Technical Note: HR@\u03c4 (Hit Rate within Tolerance).</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.nsl","title":"<code>nsl(y_true, y_pred, sample_weight=None)</code>","text":"<p>Compute No-Shortfall Level (NSL).</p> <p>NSL is the (optionally weighted) fraction of evaluation intervals in which the forecast does not underpredict realized demand.</p> <p>For each interval \\(i\\), define a hit indicator:</p> \\[ h_i = \\mathbb{1}[\\hat{y}_i \\ge y_i] \\] <p>Then:</p> \\[ \\mathrm{NSL} = \\frac{\\sum_i w_i \\; h_i}{\\sum_i w_i} \\] <p>where \\(w_i\\) are optional sample weights (default \\(w_i = 1\\)). Higher values are better, with \\(\\mathrm{NSL} \\in [0, 1]\\).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Realized demand. Must be non-negative.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Forecast demand. Must be non-negative and have the same shape as <code>y_true</code>.</p> required <code>sample_weight</code> <code>float or array-like of shape (n_samples,)</code> <p>Optional non-negative weights per interval. If provided, NSL is computed as a weighted fraction. If total weight is zero, NSL is undefined and a <code>ValueError</code> is raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>NSL value in [0, 1]. Higher indicates better shortfall avoidance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid (shape mismatch, negative values), or if total sample weight is zero.</p> Notes <ul> <li>NSL is a service reliability measure: it does not quantify how large a   shortfall is\u2014only whether a shortfall occurred.</li> <li>UD complements NSL by measuring shortfall magnitude when shortfalls occur.</li> </ul> References <p>Electric Barometer Technical Note: No Shortfall Level (NSL).</p>"},{"location":"packages/eb-metrics/api/metrics/#eb_metrics.metrics.ud","title":"<code>ud(y_true, y_pred, sample_weight=None)</code>","text":"<p>Compute Underbuild Depth (UD).</p> <p>UD measures the (optionally weighted) average magnitude of shortfall. Unlike NSL (which counts shortfalls), UD quantifies how severe shortfalls are when they occur.</p> <p>Define per-interval shortfall:</p> \\[ s_i = \\max(0, y_i - \\hat{y}_i) \\] <p>Then:</p> \\[ \\mathrm{UD} = \\frac{\\sum_i w_i \\; s_i}{\\sum_i w_i} \\] <p>Higher values indicate deeper average shortfall; lower is better.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like of shape (n_samples,)</code> <p>Realized demand. Must be non-negative.</p> required <code>y_pred</code> <code>array-like of shape (n_samples,)</code> <p>Forecast demand. Must be non-negative and have the same shape as <code>y_true</code>.</p> required <code>sample_weight</code> <code>float or array-like of shape (n_samples,)</code> <p>Optional non-negative weights per interval. If provided, UD is computed as a weighted average. If total weight is zero, UD is undefined and a <code>ValueError</code> is raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>UD value (units match <code>y_true</code>/<code>y_pred</code>). Lower indicates better shortfall control.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid (shape mismatch, negative values), or if total sample weight is zero.</p> Notes <ul> <li>UD ignores overbuild entirely; it is a pure shortfall severity measure.</li> <li>UD is often interpreted alongside NSL:   high NSL + low UD indicates strong service consistency.</li> </ul> References <p>Electric Barometer Technical Note: Underbuild Depth (UD).</p>"},{"location":"packages/eb-optimization/","title":"eb-optimization","text":"<p><code>eb-optimization</code> provides optimization, tuning, and search utilities used to transform forecast outputs into actionable decisions within the Electric Barometer ecosystem.</p> <p>This package focuses on policy application, parameter tuning, and search strategies, rather than model training or metric definition.</p>"},{"location":"packages/eb-optimization/#scope","title":"Scope","text":"<p>This package is responsible for:</p> <ul> <li>Applying readiness and cost-ratio policies to forecast outputs</li> <li>Estimating and tuning cost-ratio and readiness parameters</li> <li>Supporting grid-based and kernel-based search strategies</li> <li>Providing reusable optimization primitives for downstream systems</li> </ul> <p>It intentionally avoids defining metrics, data contracts, or evaluation workflows.</p>"},{"location":"packages/eb-optimization/#contents","title":"Contents","text":"<ul> <li> <p>Policies   Readiness and cost-ratio policies that govern decision adjustments</p> </li> <li> <p>Search utilities   Tools for exploring parameter spaces and optimization surfaces</p> </li> <li> <p>Tuning helpers   Utilities for estimating and calibrating policy parameters</p> </li> </ul>"},{"location":"packages/eb-optimization/#api-reference","title":"API reference","text":"<ul> <li>Core module</li> <li>Policies</li> <li>Search</li> <li>Tuning</li> </ul>"},{"location":"packages/eb-optimization/api/","title":"API reference","text":"<p>This section documents the public API surface of <code>eb-optimization</code>.</p> <p>The pages below expose optimization policies, search utilities, and tuning helpers via rendered docstrings.</p>"},{"location":"packages/eb-optimization/api/#modules","title":"Modules","text":"<ul> <li>Core module</li> <li>Policies</li> <li>Search</li> <li>Tuning</li> </ul>"},{"location":"packages/eb-optimization/api/eb_optimization/","title":"Core module","text":"<p>This section documents the public entry points exposed by the <code>eb-optimization</code> package.</p> <p>The core module provides top-level constants, policy helpers, and convenience functions that are commonly used when applying optimization logic within the Electric Barometer ecosystem.</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization","title":"<code>eb_optimization</code>","text":"<p><code>eb_optimization</code> \u2014 optimization and tuning layer for the Electric Barometer ecosystem.</p> <p>This package contains the optimization layer of Electric Barometer:</p> <ul> <li>search: generic, reusable search mechanics (grids, tie-breaking kernels)</li> <li>tuning: calibration and selection utilities (e.g., cost-ratio tuning, sensitivity sweeps)</li> <li>policies: frozen, declarative policy artifacts for downstream execution</li> </ul> <p>It intentionally does not define metric primitives or evaluation math. Those live in <code>eb-metrics</code> (and orchestration lives in <code>eb-evaluation</code>).</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization.RALDeltas","title":"<code>RALDeltas</code>  <code>dataclass</code>","text":"<p>Two-band additive deltas for a two-band RAL policy.</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization.RALTwoBandPolicy","title":"<code>RALTwoBandPolicy</code>  <code>dataclass</code>","text":"<p>Portable policy artifact for two-band additive RAL.</p> <p>This policy encodes the exact \"two-band\" additive RAL used in the ISO-NE example notebook:</p> <ul> <li>If baseline forecast \\(\\hat{y}\\) is in the mid-risk band:   $$ \\hat{y}^{(r)} = \\hat{y} + d_{\\text{mid}} $$</li> <li>If baseline forecast \\(\\hat{y}\\) is in the high-risk band:   $$ \\hat{y}^{(r)} = \\hat{y} + d_{\\text{high}} $$</li> </ul> <p>Deltas can be:</p> <ul> <li>global (fallback) via <code>global_deltas</code>, and/or</li> <li>per-key overrides via <code>per_key_deltas</code>, keyed by a segment key column   (e.g., <code>interface</code>).</li> </ul> Notes <p>This class is intentionally a policy artifact (parameters + deterministic application). It does not learn deltas; it only stores and applies them.</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization.RALTwoBandPolicy.get_deltas","title":"<code>get_deltas(key=None)</code>","text":"<p>Return deltas for a key (or the global deltas if none/unknown).</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization.RALTwoBandPolicy.adjust_forecast","title":"<code>adjust_forecast(df, forecast_col, *, key_col=None)</code>","text":"<p>Apply the two-band additive RAL policy to a forecast column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the forecast to adjust.</p> required <code>forecast_col</code> <code>str</code> <p>Column name containing baseline forecast values.</p> required <code>key_col</code> <code>str</code> <p>Column name containing keys for per-key deltas (e.g., \"interface\"). If omitted, the global deltas are applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Adjusted forecast values as a series named \"readiness_forecast\".</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization.RALTwoBandPolicy.transform","title":"<code>transform(df, forecast_col, *, key_col=None)</code>","text":"<p>Transform the input DataFrame by applying the forecast adjustment.</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization.RALTwoBandPolicy.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize to a JSON-friendly dict.</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization.RALTwoBandPolicy.from_dict","title":"<code>from_dict(d)</code>  <code>classmethod</code>","text":"<p>Deserialize from a dict produced by <code>to_dict()</code>.</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization.EntityCostRatioEstimate","title":"<code>EntityCostRatioEstimate</code>  <code>dataclass</code>","text":"<p>Entity-level cost ratio calibration artifact.</p> <p>This container is designed to be persistable and ergonomic:</p> <ul> <li><code>table</code> can be saved to Parquet/CSV without DataFrame-in-cell object columns.</li> <li><code>curves</code> retains the full per-entity sensitivity curves for audit/plotting.</li> <li>Shared metadata captures the governance context of the run.</li> </ul> <p>Attributes:</p> Name Type Description <code>entity_col</code> <code>str</code> <p>Name of the entity identifier column used in <code>table</code>.</p> <code>method</code> <code>str</code> <p>Method identifier used to produce the estimate (e.g., \"cost_balance\").</p> <code>grid</code> <code>ndarray</code> <p>The filtered candidate grid actually searched (strictly positive values). The order of this grid defines tie-breaking for each entity.</p> <code>selection</code> <code>str</code> <p>Selection strategy used once each curve is computed (\"curve\" or \"kernel\").</p> <code>tie_break</code> <code>str</code> <p>Tie-breaking rule applied when multiple candidates achieve the same objective. For this routine it is always \"first\".</p> <code>table</code> <code>DataFrame</code> <p>One row per entity with scalar results and summary diagnostics. Columns include: - entity_col - R_star - n - under_cost - over_cost - gap - diagnostics  (dict; intended to be JSON-serializable)</p> <code>curves</code> <code>dict[Any, DataFrame]</code> <p>Mapping of entity_id -&gt; sensitivity curve DataFrame for that entity. Each curve has columns: [R, under_cost, over_cost, gap].</p>"},{"location":"packages/eb-optimization/api/eb_optimization/#eb_optimization.apply_ral_policy","title":"<code>apply_ral_policy(df, forecast_col, policy=DEFAULT_RAL_POLICY)</code>","text":"<p>Convenience functional wrapper to apply a RALPolicy.</p>"},{"location":"packages/eb-optimization/api/policies/cost_ratio_policy/","title":"Cost ratio policy","text":"<p>This section documents the cost ratio policy implementations provided by <code>eb-optimization</code>.</p> <p>Cost ratio policies define how asymmetric over- and under-prediction costs are applied to forecast outputs in order to adjust decisions according to business risk preferences.</p>"},{"location":"packages/eb-optimization/api/policies/cost_ratio_policy/#eb_optimization.policies.cost_ratio_policy","title":"<code>eb_optimization.policies.cost_ratio_policy</code>","text":"<p>Cost-ratio (R = c_u / c_o) policy artifacts for eb-optimization.</p> <p>This module defines frozen governance for selecting and applying a cost ratio <code>R</code> (and derived underbuild cost <code>c_u</code>) used by asymmetric cost metrics like CWSL.</p> Layering &amp; responsibilities <ul> <li><code>tuning/cost_ratio.py</code>:     Calibration logic (estimating R from residuals / cost balance).</li> <li><code>policies/cost_ratio_policy.py</code>:     Frozen configuration + deterministic application wrappers.</li> </ul>"},{"location":"packages/eb-optimization/api/policies/cost_ratio_policy/#eb_optimization.policies.cost_ratio_policy.CostRatioPolicy","title":"<code>CostRatioPolicy</code>  <code>dataclass</code>","text":"<p>Frozen cost-ratio (R) policy configuration.</p> <p>Attributes:</p> Name Type Description <code>R_grid</code> <code>Sequence[float]</code> <p>Candidate ratios to search. Only strictly positive values are considered.</p> <code>co</code> <code>float</code> <p>Default overbuild cost coefficient used for entity-level estimation.</p> <code>min_n</code> <code>int</code> <p>Minimum number of observations required to estimate an entity-level R.</p>"},{"location":"packages/eb-optimization/api/policies/cost_ratio_policy/#eb_optimization.policies.cost_ratio_policy.apply_cost_ratio_policy","title":"<code>apply_cost_ratio_policy(y_true, y_pred, *, policy=DEFAULT_COST_RATIO_POLICY, co=None, sample_weight=None, gate='warn', identifiability_override_reason=None)</code>","text":"<p>Apply a frozen cost-ratio policy to estimate a global R.</p> Notes <p>This policy boundary surfaces identifiability / stability diagnostics when available. It does NOT change the selection behavior; it only enriches the returned diagnostics.</p> Gating <p><code>gate</code> controls what happens when tuning reports <code>is_identifiable=False</code>:</p> <ul> <li>gate=\"off\"  : no action (still reports diagnostics)</li> <li>gate=\"warn\" : emit a RuntimeWarning (default)</li> <li>gate=\"raise\": raise ValueError</li> </ul> Overrides <p>If <code>identifiability_override_reason</code> is provided, the gate will not warn/raise, and the reason is recorded in diagnostics for auditability.</p>"},{"location":"packages/eb-optimization/api/policies/cost_ratio_policy/#eb_optimization.policies.cost_ratio_policy.apply_entity_cost_ratio_policy","title":"<code>apply_entity_cost_ratio_policy(df, *, entity_col, y_true_col, y_pred_col, policy=DEFAULT_COST_RATIO_POLICY, co=None, sample_weight_col=None, include_diagnostics=True, gate='warn', identifiability_override_reason=None)</code>","text":"<p>Apply a frozen cost-ratio policy per entity.</p> Notes <p>This policy boundary surfaces per-entity calibration diagnostics (in the <code>diagnostics</code> column) for eligible entities when <code>include_diagnostics=True</code>.</p> Entity-level identifiability <p>The tuning artifact returns per-entity <code>diagnostics</code> dicts. If those dicts contain an <code>is_identifiable</code> field, this function will:   - surface a convenience <code>is_identifiable</code> column, and   - optionally warn/raise based on <code>gate</code>.</p> <p>If no such field exists (older tuning versions), gating is a no-op.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/","title":"Readiness adjustment policy","text":"<p>This section documents readiness adjustment layer (RAL) policy implementations provided by <code>eb-optimization</code>.</p> <p>RAL policies define how forecast outputs are adjusted based on readiness signals and operational thresholds to balance service risk and cost efficiency.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy","title":"<code>eb_optimization.policies.ral_policy</code>","text":"<p>Policy artifacts for the Readiness Adjustment Layer (RAL).</p> <p>This module defines portable, immutable policy objects produced by offline optimization and consumed by deterministic evaluation and production workflows.</p> <p>Responsibilities: - Represent learned RAL parameters (global and optional segment-level uplifts) - Provide a stable, serializable contract between optimization and evaluation - Support audit and governance workflows</p> <p>Non-responsibilities: - Learning or tuning parameters - Applying policies to data - Defining metric or loss functions</p> <p>Design philosophy: Policies are artifacts, not algorithms. They encode decisions derived from optimization, not the optimization process itself.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALPolicy","title":"<code>RALPolicy</code>  <code>dataclass</code>","text":"<p>Portable policy artifact for the Readiness Adjustment Layer (RAL).</p> <p>A :class:<code>~eb_optimization.policies.ral_policy.RALPolicy</code> is the output of an offline tuning process (e.g., grid search or evolutionary optimization) and the input to deterministic evaluation / production application.</p> <p>Conceptually, RAL applies a multiplicative uplift to a baseline forecast:</p> \\[ \\hat{y}^{(r)} = u \\cdot \\hat{y} \\] <p>where <code>u</code> can be either:</p> <ul> <li>a global uplift (<code>global_uplift</code>), applied to all rows, and/or</li> <li>segment-level uplifts stored in <code>uplift_table</code>, keyed by <code>segment_cols</code></li> </ul> <p>Segment-level uplifts must fall back to the global uplift for unseen segment combinations at application time.</p> <p>Attributes:</p> Name Type Description <code>global_uplift</code> <code>float</code> <p>The global multiplicative uplift used as a fallback and baseline readiness adjustment.</p> <code>segment_cols</code> <code>list[str]</code> <p>The segmentation columns used to key <code>uplift_table</code>. Empty means \"global-only\".</p> <code>uplift_table</code> <code>DataFrame | None</code> <p>Optional DataFrame with columns <code>[*segment_cols, \"uplift\"]</code> containing segment-level uplifts. If <code>None</code> or empty, the policy is global-only.</p> Notes <p>This dataclass is intentionally simple and serializable. It is meant to be:</p> <ul> <li>produced offline in <code>eb-optimization</code></li> <li>applied deterministically in <code>eb-evaluation</code></li> <li>loggable/auditable as part of operational governance</li> </ul> <p>The policy does not encode metric definitions or optimization state\u2014only the artifacts needed to execute the adjustment.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALPolicy.is_segmented","title":"<code>is_segmented()</code>","text":"<p>Return True if the policy contains segment-level uplifts.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALPolicy.adjust_forecast","title":"<code>adjust_forecast(df, forecast_col)</code>","text":"<p>Apply the RAL policy to adjust the forecast values.</p> <p>This method applies the global uplift to all rows, and applies segment-level uplifts if the policy is segmented and matching segments exist in the <code>uplift_table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the forecast to adjust.</p> required <code>forecast_col</code> <code>str</code> <p>The name of the column in <code>df</code> containing the forecast values to adjust.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series with the adjusted forecast values.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALPolicy.transform","title":"<code>transform(df, forecast_col)</code>","text":"<p>Transform the input DataFrame by applying the forecast adjustment.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALBands","title":"<code>RALBands</code>  <code>dataclass</code>","text":"<p>Risk-region thresholds for a two-band additive RAL policy.</p> <p>mid     Lower bound for the mid-risk region (inclusive). high     Lower bound for the high-risk region (inclusive).</p> <p>The two-band transform is:   - add d_high when yhat &gt;= high   - add d_mid  when mid &lt;= yhat &lt; high</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALBandThresholds","title":"<code>RALBandThresholds</code>  <code>dataclass</code>","text":"<p>Canonical two-band thresholds artifact.</p> <p>This is the same concept as :class:<code>~eb_optimization.policies.ral_policy.RALBands</code>, but exposed as a named artifact for the canonical \"learn thresholds + deltas\" RAL approach (Option E).</p> <p>mid     Lower bound for the mid-risk region (inclusive). high     Lower bound for the high-risk region (inclusive).</p> Notes <ul> <li><code>high</code> must be &gt;= <code>mid</code>.</li> <li>Thresholds are assumed to be non-negative. (Many domains normalize to [0, 1],   but we do not hard-cap at 1.0 to allow safe usage when values can exceed 1.)</li> </ul>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALDeltas","title":"<code>RALDeltas</code>  <code>dataclass</code>","text":"<p>Two-band additive deltas for a two-band RAL policy.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALTwoBandPolicy","title":"<code>RALTwoBandPolicy</code>  <code>dataclass</code>","text":"<p>Portable policy artifact for two-band additive RAL.</p> <p>This policy encodes the exact \"two-band\" additive RAL used in the ISO-NE example notebook:</p> <ul> <li>If baseline forecast \\(\\hat{y}\\) is in the mid-risk band:   $$ \\hat{y}^{(r)} = \\hat{y} + d_{\\text{mid}} $$</li> <li>If baseline forecast \\(\\hat{y}\\) is in the high-risk band:   $$ \\hat{y}^{(r)} = \\hat{y} + d_{\\text{high}} $$</li> </ul> <p>Deltas can be:</p> <ul> <li>global (fallback) via <code>global_deltas</code>, and/or</li> <li>per-key overrides via <code>per_key_deltas</code>, keyed by a segment key column   (e.g., <code>interface</code>).</li> </ul> Notes <p>This class is intentionally a policy artifact (parameters + deterministic application). It does not learn deltas; it only stores and applies them.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALTwoBandPolicy.get_deltas","title":"<code>get_deltas(key=None)</code>","text":"<p>Return deltas for a key (or the global deltas if none/unknown).</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALTwoBandPolicy.adjust_forecast","title":"<code>adjust_forecast(df, forecast_col, *, key_col=None)</code>","text":"<p>Apply the two-band additive RAL policy to a forecast column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the forecast to adjust.</p> required <code>forecast_col</code> <code>str</code> <p>Column name containing baseline forecast values.</p> required <code>key_col</code> <code>str</code> <p>Column name containing keys for per-key deltas (e.g., \"interface\"). If omitted, the global deltas are applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Adjusted forecast values as a series named \"readiness_forecast\".</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALTwoBandPolicy.transform","title":"<code>transform(df, forecast_col, *, key_col=None)</code>","text":"<p>Transform the input DataFrame by applying the forecast adjustment.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALTwoBandPolicy.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize to a JSON-friendly dict.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALTwoBandPolicy.from_dict","title":"<code>from_dict(d)</code>  <code>classmethod</code>","text":"<p>Deserialize from a dict produced by <code>to_dict()</code>.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALThresholdTwoBandPolicy","title":"<code>RALThresholdTwoBandPolicy</code>  <code>dataclass</code>","text":"<p>Canonical RAL policy artifact: learnable thresholds + additive deltas (Option E).</p> <p>This policy generalizes :class:<code>~eb_optimization.policies.ral_policy.RALTwoBandPolicy</code> by allowing both thresholds and deltas to be specified globally and overridden per-key (e.g., per interface).</p> <p>Application (for each row) uses the thresholds for the row's key (or global):   - add d_high when yhat &gt;= high   - add d_mid  when mid &lt;= yhat &lt; high</p> Notes <ul> <li>This is still a policy artifact, not an optimizer.</li> <li>Guardrails (like min tail count) should be enforced by the tuner that produces it.</li> </ul>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALThresholdTwoBandPolicy.get_thresholds","title":"<code>get_thresholds(key=None)</code>","text":"<p>Return thresholds for a key (or the global thresholds if none/unknown).</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALThresholdTwoBandPolicy.get_deltas","title":"<code>get_deltas(key=None)</code>","text":"<p>Return deltas for a key (or the global deltas if none/unknown).</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALThresholdTwoBandPolicy.adjust_forecast","title":"<code>adjust_forecast(df, forecast_col, *, key_col=None)</code>","text":"<p>Apply the canonical (threshold + delta) two-band RAL policy.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the forecast to adjust.</p> required <code>forecast_col</code> <code>str</code> <p>Column name containing baseline forecast values.</p> required <code>key_col</code> <code>str</code> <p>Column name containing keys for per-key overrides (e.g., \"interface\"). If omitted, global thresholds and deltas are applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Adjusted forecast values as a series named \"readiness_forecast\".</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALThresholdTwoBandPolicy.adjust_forecast_capped","title":"<code>adjust_forecast_capped(df, forecast_col, *, key_col=None, lower=0.0, upper=1.0)</code>","text":"<p>Apply the canonical policy and optionally cap the adjusted forecast.</p> <p>This is a low-risk guardrail for domains with known physical bounds.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the forecast to adjust.</p> required <code>forecast_col</code> <code>str</code> <p>Column name containing baseline forecast values.</p> required <code>key_col</code> <code>str</code> <p>Column name containing keys for per-key overrides (e.g., \"interface\").</p> <code>None</code> <code>lower</code> <code>float</code> <p>Lower cap applied via <code>np.maximum</code>.</p> <code>0.0</code> <code>upper</code> <code>float or None</code> <p>Upper cap applied via <code>np.minimum</code>. Use None to disable the upper cap.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>Adjusted and (optionally) capped forecast as \"readiness_forecast\".</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALThresholdTwoBandPolicy.transform","title":"<code>transform(df, forecast_col, *, key_col=None)</code>","text":"<p>Transform the input DataFrame by applying the forecast adjustment.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALThresholdTwoBandPolicy.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize to a JSON-friendly dict.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.RALThresholdTwoBandPolicy.from_dict","title":"<code>from_dict(d)</code>  <code>classmethod</code>","text":"<p>Deserialize from a dict produced by <code>to_dict()</code>.</p>"},{"location":"packages/eb-optimization/api/policies/ral_policy/#eb_optimization.policies.ral_policy.apply_ral_policy","title":"<code>apply_ral_policy(df, forecast_col, policy=DEFAULT_RAL_POLICY)</code>","text":"<p>Convenience functional wrapper to apply a RALPolicy.</p>"},{"location":"packages/eb-optimization/api/policies/tau_policy/","title":"Tau policy","text":"<p>This section documents tau-based policy implementations provided by <code>eb-optimization</code>.</p> <p>Tau policies define threshold- or quantile-based decision rules that control when and how forecast-driven actions are triggered, enabling consistent handling of risk tolerance and sensitivity across optimization workflows.</p>"},{"location":"packages/eb-optimization/api/policies/tau_policy/#eb_optimization.policies.tau_policy","title":"<code>eb_optimization.policies.tau_policy</code>","text":"<p>Tau (\u03c4) policy artifacts for eb-optimization.</p> <p>This module defines frozen governance for selecting a tolerance \u03c4 used by HR@\u03c4.</p> <ul> <li>tuning/tau.py: calibration logic (estimating \u03c4 from residuals)</li> <li>policies/tau_policy.py: frozen configuration + deterministic application wrappers</li> </ul> <p>Policies should be stable, auditable, and safe to apply at runtime.</p>"},{"location":"packages/eb-optimization/api/policies/tau_policy/#eb_optimization.policies.tau_policy.TauPolicy","title":"<code>TauPolicy</code>  <code>dataclass</code>","text":"<p>Frozen \u03c4 policy configuration.</p> <p>This is the governance object you can persist, version, and ship to downstream consumers.</p> Notes <ul> <li><code>estimate_kwargs</code> are passed through to <code>estimate_tau</code>.</li> <li>If <code>cap_with_global</code> is True, entity \u03c4 values are capped by a global cap   derived from the full residual distribution at <code>global_cap_quantile</code>.</li> </ul>"},{"location":"packages/eb-optimization/api/policies/tau_policy/#eb_optimization.policies.tau_policy.apply_tau_policy","title":"<code>apply_tau_policy(y, yhat, policy=DEFAULT_TAU_POLICY)</code>","text":"<p>Apply a frozen \u03c4 policy to produce \u03c4 (global).</p> <p>Returns:</p> Type Description <code>(tau, diagnostics)</code>"},{"location":"packages/eb-optimization/api/policies/tau_policy/#eb_optimization.policies.tau_policy.apply_tau_policy_hr","title":"<code>apply_tau_policy_hr(y, yhat, policy=DEFAULT_TAU_POLICY)</code>","text":"<p>Apply \u03c4 policy, then compute HR@\u03c4.</p> <p>Returns:</p> Type Description <code>(hr, tau, diagnostics)</code>"},{"location":"packages/eb-optimization/api/policies/tau_policy/#eb_optimization.policies.tau_policy.apply_entity_tau_policy","title":"<code>apply_entity_tau_policy(df, *, entity_col, y_col, yhat_col, policy=DEFAULT_TAU_POLICY, include_diagnostics=True)</code>","text":"<p>Apply a frozen \u03c4 policy per entity (with optional global cap governance).</p> <p>This wraps tuning.estimate_entity_tau but pins governance via TauPolicy.</p>"},{"location":"packages/eb-optimization/api/search/grid/","title":"Grid search","text":"<p>This section documents grid-based search utilities provided by <code>eb-optimization</code>.</p> <p>Grid search utilities support systematic exploration of discrete parameter spaces when tuning optimization policies and decision thresholds.</p>"},{"location":"packages/eb-optimization/api/search/grid/#eb_optimization.search.grid","title":"<code>eb_optimization.search.grid</code>","text":"<p>Grid construction utilities for optimization search spaces.</p> <p>This module provides small, deterministic helpers for constructing bounded, interpretable parameter grids used by offline optimization routines.</p> <p>Responsibilities: - Create numerically stable, reproducible grids for scalar parameters - Enforce positivity and boundary constraints - Standardize grid behavior across tuners</p> <p>Non-responsibilities: - Evaluating objectives - Selecting optimal parameters - Performing any optimization logic</p> <p>Design philosophy: This utility favors bounded, discrete search spaces for interpretability, auditability, and deployability of learned policies.</p>"},{"location":"packages/eb-optimization/api/search/grid/#eb_optimization.search.grid.make_float_grid","title":"<code>make_float_grid(x_min, x_max, step, decimals=10)</code>","text":"<p>Create a numerically robust 1D grid over a closed interval.</p> <p>This utility is used throughout optimization to create bounded, interpretable candidate sets for discrete parameter search (e.g., uplift multipliers, thresholds).</p> <p>The returned grid:</p> <ul> <li>starts at <code>x_min</code></li> <li>increments by <code>step</code></li> <li>includes <code>x_max</code> (to the extent permitted by floating-point arithmetic)</li> <li>is clipped and de-duplicated for numerical stability</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x_min</code> <code>float</code> <p>Lower bound for the grid (inclusive). Must be strictly positive.</p> required <code>x_max</code> <code>float</code> <p>Upper bound for the grid (inclusive). Must be greater than or equal to <code>x_min</code>.</p> required <code>step</code> <code>float</code> <p>Step size between candidates. Must be strictly positive.</p> required <code>decimals</code> <code>int</code> <p>Rounding precision used to stabilize floats and de-duplicate.</p> <code>10</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 1D array of unique grid values in ascending order.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>step</code> is not strictly positive, if <code>x_min</code> is not strictly positive, or if <code>x_max &lt; x_min</code>.</p> Notes <p>This utility ensures reproducible and stable grid construction for parameter tuning and optimization purposes, while favoring discrete, bounded search spaces for interpretability and deployability.</p>"},{"location":"packages/eb-optimization/api/search/kernels/","title":"Kernel-based search","text":"<p>This section documents kernel-based search utilities provided by <code>eb-optimization</code>.</p> <p>Kernel-based search utilities support continuous or adaptive exploration of parameter spaces by leveraging kernel functions to model response surfaces and guide optimization.</p>"},{"location":"packages/eb-optimization/api/search/kernels/#eb_optimization.search.kernels","title":"<code>eb_optimization.search.kernels</code>","text":"<p>Generic discrete-search kernels for eb-optimization.</p> <p>This module defines mechanical optimization primitives for selecting an argmin or argmax over a finite candidate set.</p> Responsibilities <ul> <li>Iterate over a discrete candidate set</li> <li>Evaluate a scalar objective function</li> <li>Apply deterministic tie-breaking rules</li> <li>Return the selected candidate and its score</li> </ul> Non-responsibilities <ul> <li>Defining candidate grids (handled by <code>search.grid</code>)</li> <li>Computing domain-specific objectives (e.g., cost, HR@\u03c4, utility)</li> <li>Inspecting data distributions or residuals</li> <li>Returning diagnostics, plots, or policies</li> </ul> Design philosophy <p>These kernels are intentionally simple, deterministic, and domain-agnostic. They serve as reusable building blocks for higher-level tuning logic (<code>tuning</code> modules), enabling consistent and auditable optimization behavior across the Electric Barometer ecosystem.</p>"},{"location":"packages/eb-optimization/api/search/kernels/#eb_optimization.search.kernels.argmin_over_candidates","title":"<code>argmin_over_candidates(candidates, score_fn, *, tie_break='first')</code>","text":"<p>Select the candidate that minimizes a scalar score.</p> <p>Parameters:</p> Name Type Description Default <code>candidates</code> <code>Iterable[T]</code> <p>Iterable of candidate values (e.g., floats, ints, tuples).</p> required <code>score_fn</code> <code>Callable[[T], float]</code> <p>Function mapping a candidate to a scalar score.</p> required <code>tie_break</code> <code>Literal['first', 'last', 'closest_to_zero']</code> <p>Deterministic tie-breaking rule: - <code>\"first\"</code>: first candidate with minimal score - <code>\"last\"</code>: last candidate with minimal score - <code>\"closest_to_zero\"</code>: among ties, choose candidate with smallest   absolute value</p> <code>'first'</code> <p>Returns:</p> Type Description <code>(best_candidate, best_score)</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>candidates</code> is empty or if <code>score_fn</code> returns a non-finite value.</p>"},{"location":"packages/eb-optimization/api/search/kernels/#eb_optimization.search.kernels.argmax_over_candidates","title":"<code>argmax_over_candidates(candidates, score_fn, *, tie_break='first')</code>","text":"<p>Select the candidate that maximizes a scalar score.</p> <p>This is the argmax analogue of <code>argmin_over_candidates</code>.</p>"},{"location":"packages/eb-optimization/api/tuning/cost_ratio/","title":"Cost ratio tuning","text":"<p>This section documents utilities for estimating and tuning cost-ratio parameters in <code>eb-optimization</code>.</p> <p>Cost ratio tuning supports calibration of asymmetric over- and under-prediction penalties based on observed outcomes, enabling alignment between optimization behavior and business risk preferences.</p>"},{"location":"packages/eb-optimization/api/tuning/cost_ratio/#eb_optimization.tuning.cost_ratio","title":"<code>eb_optimization.tuning.cost_ratio</code>","text":"<p>Cost ratio (R) tuning utilities.</p> <p>This module provides calibration helpers for selecting the underbuild-to-overbuild cost ratio:</p> \\[     R = \\frac{c_u}{c_o} \\] <p>These routines belong in eb-optimization because they choose/govern parameters from data over a candidate set (grid search + calibration diagnostics). They are not metric primitives (eb-metrics) and are not runtime policies (eb-optimization/policies).</p> <p>Layering: - search/   : reusable candidate-space utilities (grids, kernels) - tuning/   : define candidate grids + objectives + return calibration artifacts - policies/ : frozen artifacts that apply parameters deterministically at runtime</p> Selection strategy <p><code>estimate_R_cost_balance</code> can select the optimal ratio in two equivalent ways:</p> <p>1) selection=\"curve\" (default)    - Materialize the full sensitivity curve over the candidate grid.    - Select \\(R^*\\) by a direct reduction on the curve (NumPy argmin over <code>gap</code>).    - This is typically faster and emphasizes that the curve is the primary audit artifact.</p> <p>2) selection=\"kernel\"    - Materialize the full sensitivity curve over the candidate grid.    - Select \\(R^*\\) using the generic candidate-search kernel (<code>argmin_over_candidates</code>)      by scoring each candidate using the curve-derived <code>gap</code>.    - This preserves a consistent \u201ckernel-governed\u201d selection pathway that matches other      search utilities in the library.</p> <p>Both strategies are deterministic and use the same tie-breaking semantics: the first candidate (in filtered grid order) achieving the minimum <code>gap</code> is chosen.</p> Returning governance artifacts <p>If <code>return_curve=False</code> (default), <code>estimate_R_cost_balance</code> returns the selected ratio <code>R_star</code> as a plain float (backwards compatible).</p> <p>If <code>return_curve=True</code>, it returns a <code>CostRatioEstimate</code> object that mirrors the intent of <code>TauEstimate</code>: it includes the selected ratio plus the candidate grid actually searched, the selection strategy, the tie-break rule, sample count, and a diagnostics payload alongside the full sensitivity curve.</p> Entity-level calibration (recommended API) <p>Entity-level calibration produces richer outputs than a flat DataFrame can cleanly represent (because each entity has its own sensitivity curve).</p> <p>To keep both: - analysis convenience (legacy DataFrame), and - gold-standard \u201cpersistable artifact\u201d structure,</p> <p><code>estimate_entity_R_from_balance</code> supports two return modes:</p> <p>1) Default (backwards compatible):    - Returns a DataFrame with one row per entity and scalar outputs (no curves).    - This preserves legacy column names (<code>R</code>, <code>diff</code>) for compatibility.</p> <p>2) <code>return_result=True</code>:    - Returns an <code>EntityCostRatioEstimate</code> artifact containing:      - <code>table</code>: one row per entity with standardized scalar outputs + per-entity diagnostics                (no DataFrame-in-cell curves)      - <code>curves</code>: dict mapping entity_id -&gt; sensitivity curve DataFrame      - shared governance metadata (method, grid, selection, tie_break)</p> Serialization guidance <ul> <li>The entity-level <code>table</code> is designed to be Parquet/CSV friendly.</li> <li>The <code>diagnostics</code> dict is intended to remain JSON-serializable (floats/bools/ints/str).</li> <li>Full per-entity curves are kept in the <code>curves</code> mapping to avoid object dtype columns.</li> </ul>"},{"location":"packages/eb-optimization/api/tuning/cost_ratio/#eb_optimization.tuning.cost_ratio.CostRatioEstimate","title":"<code>CostRatioEstimate</code>  <code>dataclass</code>","text":"<p>Result container for cost-ratio (R) calibration.</p> <p>This is intentionally \"TauEstimate-like\": downstream code can persist or log a single object that fully describes the calibration decision.</p> <p>Attributes:</p> Name Type Description <code>R_star</code> <code>float</code> <p>Selected cost ratio from the candidate grid.</p> <code>method</code> <code>str</code> <p>Method identifier used to produce the estimate (e.g., \"cost_balance\").</p> <code>n</code> <code>int</code> <p>Number of samples used in the calibration (after validation).</p> <code>grid</code> <code>ndarray</code> <p>The filtered candidate grid actually searched (strictly positive values). The order of this grid defines tie-breaking.</p> <code>selection</code> <code>str</code> <p>Selection strategy used once the curve is computed (\"curve\" or \"kernel\").</p> <code>tie_break</code> <code>str</code> <p>Tie-breaking rule applied when multiple candidates achieve the same objective. For this routine it is always \"first\".</p> <code>diagnostics</code> <code>dict[str, Any]</code> <p>Method-specific diagnostic metadata intended for governance and reporting. Keys include: - \"over_cost_const\": float - \"min_gap\": float - \"degenerate_perfect_forecast\": bool - \"rel_min_gap\": float - \"grid_sensitivity\": dict[str, float] - \"grid_instability_log\": float - \"identifiability_thresholds\": dict[str, float] - \"is_identifiable\": bool</p> <code>rel_min_gap</code> <code>float</code> <p>Relative imbalance at the chosen point: min_gap / over_cost_const (or inf if over_cost_const==0 and min_gap&gt;0). This is a cost-separation diagnostic.</p> <code>R_min</code> <code>float</code> <p>Minimum R_star across built-in grid perturbations (base/exclude/shift).</p> <code>R_max</code> <code>float</code> <p>Maximum R_star across built-in grid perturbations (base/exclude/shift).</p> <code>grid_instability_log</code> <code>float</code> <p>log(R_max / R_min) across built-in perturbations. This is a grid-sensitivity diagnostic capturing weak identifiability due to discretization.</p> <code>is_identifiable</code> <code>bool</code> <p>Boolean summary derived from conservative thresholds on rel_min_gap and grid_instability_log. This does not change selection; it only reports stability.</p> <code>curve</code> <code>DataFrame</code> <p>Sensitivity curve / diagnostics for each candidate ratio. Columns are: - R - under_cost - over_cost - gap  (= |under_cost - over_cost|)</p>"},{"location":"packages/eb-optimization/api/tuning/cost_ratio/#eb_optimization.tuning.cost_ratio.EntityCostRatioEstimate","title":"<code>EntityCostRatioEstimate</code>  <code>dataclass</code>","text":"<p>Entity-level cost ratio calibration artifact.</p> <p>This container is designed to be persistable and ergonomic:</p> <ul> <li><code>table</code> can be saved to Parquet/CSV without DataFrame-in-cell object columns.</li> <li><code>curves</code> retains the full per-entity sensitivity curves for audit/plotting.</li> <li>Shared metadata captures the governance context of the run.</li> </ul> <p>Attributes:</p> Name Type Description <code>entity_col</code> <code>str</code> <p>Name of the entity identifier column used in <code>table</code>.</p> <code>method</code> <code>str</code> <p>Method identifier used to produce the estimate (e.g., \"cost_balance\").</p> <code>grid</code> <code>ndarray</code> <p>The filtered candidate grid actually searched (strictly positive values). The order of this grid defines tie-breaking for each entity.</p> <code>selection</code> <code>str</code> <p>Selection strategy used once each curve is computed (\"curve\" or \"kernel\").</p> <code>tie_break</code> <code>str</code> <p>Tie-breaking rule applied when multiple candidates achieve the same objective. For this routine it is always \"first\".</p> <code>table</code> <code>DataFrame</code> <p>One row per entity with scalar results and summary diagnostics. Columns include: - entity_col - R_star - n - under_cost - over_cost - gap - diagnostics  (dict; intended to be JSON-serializable)</p> <code>curves</code> <code>dict[Any, DataFrame]</code> <p>Mapping of entity_id -&gt; sensitivity curve DataFrame for that entity. Each curve has columns: [R, under_cost, over_cost, gap].</p>"},{"location":"packages/eb-optimization/api/tuning/cost_ratio/#eb_optimization.tuning.cost_ratio.estimate_R_cost_balance","title":"<code>estimate_R_cost_balance(y_true, y_pred, R_grid=(0.5, 1.0, 2.0, 3.0), co=1.0, sample_weight=None, *, return_curve=False, selection='curve')</code>","text":"<p>Estimate a global cost ratio \\(R = c_u / c_o\\) via cost balance.</p> <p>This routine selects a single, global cost ratio \\(R\\) by searching a candidate grid and choosing the value where the total weighted underbuild cost is closest to the total weighted overbuild cost.</p> <p>For each candidate \\(R\\) in <code>R_grid</code>:</p> \\[ \\begin{aligned} c_{u,i} &amp;= R \\cdot c_{o,i} \\\\ s_i &amp;= \\max(0, y_i - \\hat{y}_i) \\\\ e_i &amp;= \\max(0, \\hat{y}_i - y_i) \\\\ C_u(R) &amp;= \\sum_i w_i \\; c_{u,i} \\; s_i \\\\ C_o(R) &amp;= \\sum_i w_i \\; c_{o,i} \\; e_i \\end{aligned} \\] <p>and the selected value is:</p> \\[ R^* = \\arg\\min_R \\; \\left| C_u(R) - C_o(R) \\right|. \\] <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ArrayLike</code> <p>Realized demand (non-negative), shape (n_samples,).</p> required <code>y_pred</code> <code>ArrayLike</code> <p>Forecast demand (non-negative), shape (n_samples,). Must match <code>y_true</code>.</p> required <code>R_grid</code> <code>Sequence[float]</code> <p>Candidate ratios to search. Only strictly positive values are considered. The filtered grid order is preserved for tie-breaking.</p> <code>(0.5, 1.0, 2.0, 3.0)</code> <code>co</code> <code>float | ArrayLike</code> <p>Overbuild cost coefficient \\(c_o\\). May be scalar or 1D array of shape (n_samples,). Underbuild cost is implied as \\(c_{u,i} = R \\cdot c_{o,i}\\).</p> <code>1.0</code> <code>sample_weight</code> <code>ArrayLike | None</code> <p>Optional non-negative weights. If None, all intervals receive weight 1.0.</p> <code>None</code> <code>return_curve</code> <code>bool</code> <p>If True, return a CostRatioEstimate containing both the chosen R and the full sensitivity curve diagnostics over the filtered grid, plus governance metadata (grid used, selection strategy, tie-break rule, sample size).</p> <code>False</code> <code>selection</code> <code>Literal['curve', 'kernel']</code> <p>Strategy used to select \\(R^*\\) once the sensitivity curve has been computed:</p> <ul> <li>\"curve\"  : Select via NumPy argmin over the curve's <code>gap</code> column.              This is typically faster and treats the curve as the primary              artifact.</li> <li>\"kernel\" : Select via <code>argmin_over_candidates</code>, scoring each candidate using              curve-derived gap values. This maintains consistency with other              candidate-search kernels.</li> </ul> <p>Both strategies are deterministic and share the same tie-breaking behavior.</p> <code>'curve'</code> <p>Returns:</p> Type Description <code>float or CostRatioEstimate</code> <p>By default returns the selected cost ratio minimizing |under_cost - over_cost|.</p> <p>If <code>return_curve=True</code>, returns a CostRatioEstimate with: - R_star - method, n, grid, selection, tie_break, diagnostics - rel_min_gap, R_min, R_max, grid_instability_log, is_identifiable - curve : pd.DataFrame with columns [R, under_cost, over_cost, gap]</p> <p>Tie-breaking: - In the degenerate perfect-forecast case (zero error everywhere), chooses   the candidate closest to 1.0. - Otherwise, if multiple candidates yield the same minimal gap, the first   encountered candidate (in filtered grid order) is returned.</p>"},{"location":"packages/eb-optimization/api/tuning/cost_ratio/#eb_optimization.tuning.cost_ratio.estimate_entity_R_from_balance","title":"<code>estimate_entity_R_from_balance(df, entity_col, y_true_col, y_pred_col, ratios=(0.5, 1.0, 2.0, 3.0), co=1.0, sample_weight_col=None, *, return_result=False, selection='curve')</code>","text":"<p>Estimate an entity-level cost ratio via a cost-balance grid search.</p> <p>This function estimates a per-entity underbuild-to-overbuild cost ratio:</p> \\[     R_e = \\frac{c_{u,e}}{c_o} \\] <p>by searching over a user-provided grid of candidate ratios.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing entity identifiers, true values, and predictions.</p> required <code>entity_col</code> <code>str</code> <p>Column identifying the entity (e.g., store, restaurant, item).</p> required <code>y_true_col</code> <code>str</code> <p>Column containing realized demand.</p> required <code>y_pred_col</code> <code>str</code> <p>Column containing forecast demand.</p> required <code>ratios</code> <code>Sequence[float]</code> <p>Candidate ratio grid. For backward compatibility, the default return mode requires all candidates be positive. In artifact mode (<code>return_result=True</code>), the grid is filtered to strictly positive candidates and the filtered order governs tie-breaking.</p> <code>(0.5, 1.0, 2.0, 3.0)</code> <code>co</code> <code>float</code> <p>Overbuild cost coefficient (scalar). Underbuild cost coefficient for a given candidate is <code>cu = R * co</code>.</p> <code>1.0</code> <code>sample_weight_col</code> <code>str | None</code> <p>Optional column containing non-negative sample weights.</p> <code>None</code> <code>return_result</code> <code>bool</code> <p>If False (default), returns a backward-compatible DataFrame with one row per entity using legacy column names (<code>R</code>, <code>diff</code>).</p> <p>If True, returns an <code>EntityCostRatioEstimate</code> artifact with: - <code>table</code>: one row per entity with standardized columns (R_star, gap, etc.) - <code>curves</code>: dict mapping entity_id -&gt; curve DataFrame with columns [R, under_cost, over_cost, gap] - shared governance metadata (method, grid, selection, tie_break)</p> <code>False</code> <code>selection</code> <code>Literal['curve', 'kernel']</code> <p>Strategy used to select each entity's \\(R^*\\) once its curve has been computed:</p> <ul> <li>\"curve\"  : Select via NumPy argmin over the curve's <code>gap</code> column.</li> <li>\"kernel\" : Select via <code>argmin_over_candidates</code>, scoring each candidate using curve-derived <code>gap</code>.</li> </ul> <p>Both strategies are deterministic and share the same tie-breaking behavior (\"first\").</p> <code>'curve'</code> <p>Returns:</p> Type Description <code>DataFrame or EntityCostRatioEstimate</code> <p>If <code>return_result=False</code>, returns a DataFrame with one row per entity (legacy schema). If <code>return_result=True</code>, returns a persistable artifact with per-entity curves.</p>"},{"location":"packages/eb-optimization/api/tuning/ral/","title":"Readiness adjustment tuning","text":"<p>This section documents utilities for estimating and tuning readiness adjustment layer (RAL) parameters in <code>eb-optimization</code>.</p> <p>RAL tuning supports calibration of readiness thresholds and adjustment behavior based on operational outcomes, enabling balanced tradeoffs between service risk and cost efficiency.</p>"},{"location":"packages/eb-optimization/api/tuning/ral/#eb_optimization.tuning.ral","title":"<code>eb_optimization.tuning.ral</code>","text":"<p>Offline tuning for the Readiness Adjustment Layer (RAL).</p> <p>This module contains optimization logic for selecting RAL policy parameters by minimizing Electric Barometer objectives (primarily Cost-Weighted Service Loss) over historical data.</p> <p>Responsibilities: - Search bounded uplift grids to select optimal RAL parameters - Produce portable RALPolicy artifacts - Emit audit-ready diagnostics for governance and analysis</p> <p>Non-responsibilities: - Applying policies to forecasts - Defining metric math (delegated to <code>eb-metrics</code>) - Production-time inference or real-time decisioning</p>"},{"location":"packages/eb-optimization/api/tuning/ral/#eb_optimization.tuning.ral.tune_ral_policy","title":"<code>tune_ral_policy(df, *, forecast_col, actual_col, cu=2.0, co=1.0, uplift_min=1.0, uplift_max=1.15, grid_step=0.01, segment_cols=None, sample_weight_col=None)</code>","text":"<p>Tune a Readiness Adjustment Layer (RAL) policy via discrete grid search.</p> <p>This function performs offline tuning to select multiplicative uplift factors that convert a baseline forecast into an operationally conservative readiness forecast.</p> <p>The optimization objective is Cost-Weighted Service Loss (CWSL).</p>"},{"location":"packages/eb-optimization/api/tuning/sensitivity/","title":"Sensitivity analysis","text":"<p>This section documents sensitivity analysis utilities provided by <code>eb-optimization</code>.</p> <p>Sensitivity analysis supports evaluation of how optimization outcomes respond to changes in policy parameters, enabling assessment of robustness and identification of critical thresholds.</p>"},{"location":"packages/eb-optimization/api/tuning/sensitivity/#eb_optimization.tuning.sensitivity","title":"<code>eb_optimization.tuning.sensitivity</code>","text":"<p>CWSL cost-ratio sensitivity utilities.</p> <p>This module provides helpers for computing a sensitivity curve of Cost-Weighted Service Loss (CWSL) across a grid of cost ratios:</p> \\[ R = \\frac{c_u}{c_o} \\] <p>Given an overbuild cost coefficient \\(c_o\\) and ratio \\(R\\), the implied underbuild cost is:</p> \\[ c_u = R \\cdot c_o \\] Why this lives in eb-optimization <p>Computing a metric across a candidate grid of hyperparameters (like a cost ratio R) is an analysis / calibration workflow rather than a metric primitive.</p> <ul> <li>eb-metrics remains the source of truth for metric math (e.g., <code>cwsl</code>).</li> <li>eb-optimization owns grid-based evaluation, diagnostics, and tuning utilities.</li> </ul> <p>This module therefore contains: - <code>cwsl_sensitivity</code>: array-level sweep (grid evaluation) - <code>compute_cwsl_sensitivity_df</code>: DataFrame-oriented wrapper (tidy long-form output)</p>"},{"location":"packages/eb-optimization/api/tuning/sensitivity/#eb_optimization.tuning.sensitivity.cwsl_sensitivity","title":"<code>cwsl_sensitivity(y_true, y_pred, *, R_list=(0.5, 1.0, 2.0, 3.0), co=1.0, sample_weight=None)</code>","text":"<p>Evaluate CWSL across a grid of cost ratios (cost sensitivity analysis).</p> <p>For each candidate ratio:</p> \\[ R = \\frac{c_u}{c_o} \\] <p>holding <code>co</code> fixed and setting:</p> \\[ c_u = R \\cdot co \\] <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray | Sequence[float]</code> <p>Realized demand values (non-negative).</p> required <code>y_pred</code> <code>ndarray | Sequence[float]</code> <p>Forecast values (non-negative).</p> required <code>R_list</code> <code>Sequence[float] | ndarray | Iterable[float]</code> <p>Candidate cost ratios to evaluate. Non-finite and non-positive values are ignored.</p> <code>(0.5, 1.0, 2.0, 3.0)</code> <code>co</code> <code>float | ndarray</code> <p>Overbuild cost coefficient. Can be scalar or per-interval array.</p> <code>1.0</code> <code>sample_weight</code> <code>ndarray | Sequence[float] | None</code> <p>Optional non-negative weights per interval.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[float, float]</code> <p>Mapping <code>{R: cwsl_value}</code> for each valid <code>R</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid ratios remain after filtering, if inputs are invalid, or if sample_weight contains negatives.</p>"},{"location":"packages/eb-optimization/api/tuning/sensitivity/#eb_optimization.tuning.sensitivity.compute_cwsl_sensitivity_df","title":"<code>compute_cwsl_sensitivity_df(df, *, actual_col='actual_qty', forecast_col='forecast_qty', R_list=(0.5, 1.0, 2.0, 3.0), co=1.0, group_cols=None, sample_weight_col=None)</code>","text":"<p>Compute CWSL sensitivity curves from a DataFrame.</p>"},{"location":"packages/eb-optimization/api/tuning/tau/","title":"Tau tuning","text":"<p>This section documents utilities for estimating and tuning tau parameters in <code>eb-optimization</code>.</p> <p>Tau tuning supports calibration of threshold- and quantile-based decision parameters that control when optimization policies trigger actions, enabling consistent sensitivity and risk tolerance across optimization workflows.</p>"},{"location":"packages/eb-optimization/api/tuning/tau/#eb_optimization.tuning.tau","title":"<code>eb_optimization.tuning.tau</code>","text":"<p>Data-driven tolerance (\u03c4) selection utilities for HR@\u03c4.</p> <p>This module provides deterministic, residual-only methods for selecting the tolerance parameter \u03c4 used by the hit-rate metric HR@\u03c4 (hit rate within an absolute-error band).</p> <p>The hit-rate metric is:</p> \\[ \\mathrm{HR}@\\tau = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\left(|y_i-\\hat{y}_i|\\le \\tau\\right) \\] <p>Here, \u03c4 defines an acceptability band: the maximum absolute error considered operationally acceptable.</p> Design notes <ul> <li>\u03c4 is estimated from historical residuals only (no exogenous data, no model assumptions).</li> <li>The module supports global \u03c4 estimation and entity-level \u03c4 estimation.</li> <li>Optional governance controls allow capping entity \u03c4 values by a global cap to prevent   tolerance inflation.</li> </ul>"},{"location":"packages/eb-optimization/api/tuning/tau/#eb_optimization.tuning.tau.TauEstimate","title":"<code>TauEstimate</code>  <code>dataclass</code>","text":"<p>Result container for \u03c4 estimation.</p>"},{"location":"packages/eb-optimization/api/tuning/tau/#eb_optimization.tuning.tau.hr_at_tau","title":"<code>hr_at_tau(y, yhat, tau)</code>","text":"<p>Compute HR@\u03c4: fraction of observations whose absolute error is within \u03c4.</p>"},{"location":"packages/eb-optimization/api/tuning/tau/#eb_optimization.tuning.tau.estimate_tau","title":"<code>estimate_tau(y, yhat, method='target_hit_rate', *, target_hit_rate=0.9, grid=None, grid_size=101, grid_quantiles=(0.0, 0.99), knee_rule='slope_threshold', slope_threshold=0.0025, lambda_=0.1, tau_max=None, tau_floor=0.0, tau_cap=None)</code>","text":"<p>Estimate a global tolerance \u03c4 from residuals.</p>"},{"location":"packages/eb-optimization/api/tuning/tau/#eb_optimization.tuning.tau.estimate_entity_tau","title":"<code>estimate_entity_tau(df, *, entity_col, y_col, yhat_col, method='target_hit_rate', min_n=30, estimate_kwargs=None, cap_with_global=False, global_cap_quantile=0.99, include_diagnostics=True)</code>","text":"<p>Estimate \u03c4 per entity from residuals.</p>"},{"location":"packages/eb-optimization/api/tuning/tau/#eb_optimization.tuning.tau.hr_auto_tau","title":"<code>hr_auto_tau(y, yhat, method='target_hit_rate', **estimate_kwargs)</code>","text":"<p>Estimate \u03c4 from residuals, then compute HR@\u03c4. Returns (hr, tau, diagnostics).</p>"},{"location":"papers/","title":"Papers","text":"<p>This section provides direct access to the formal research and technical foundations of the Electric Barometer framework.</p> <p>All papers are authored and maintained in the eb-papers repository. This page serves as a navigation and download index for the latest authoritative PDF builds.</p>"},{"location":"papers/#electric-barometer-ecosystem","title":"Electric Barometer Ecosystem","text":"<p>Foundational research defining readiness-based forecast evaluation, asymmetric loss, admissibility, and governance for operational decision systems. These papers establish the core theoretical and technical primitives that underpin the Electric Barometer ecosystem.</p> Topic Description PDF Forecast Readiness Framework (FRF) Establishes a unified framework for evaluating forecast performance through readiness, asymmetric loss, admissibility, and governance rather than accuracy alone. Tech \u00b7 Exec Cost-Weighted Service Loss (CWSL) Defines an asymmetric, cost-weighted loss function for evaluating forecast error relative to operational underbuild and overbuild risk. Tech \u00b7 Exec CWSL Ratio Calibration (CWSLR) Formalizes sensitivity analysis and calibration of the cost ratios governing asymmetric loss in CWSL. Tech \u00b7 Exec Hit Rate at Tolerance (HR@\u03c4) Measures the proportion of forecasts that fall within an acceptable tolerance band, capturing operational reliability rather than magnitude of error. Tech \u00b7 Exec Tolerance Sensitivity for HR@\u03c4 (HR\u03c4) Analyzes how tolerance selection affects acceptability rates and readiness interpretation. Tech \u00b7 Exec No-Shortfall Level (NSL) Defines a binary coverage metric indicating whether forecasts avoid operational shortfall events within a decision horizon. Tech \u00b7 Exec Underbuild Depth (UD) Quantifies the severity of forecast shortfalls conditional on a shortfall occurring. Tech \u00b7 Exec Forecast Readiness Score (FRS) Aggregates multiple readiness and reliability metrics into a single interpretable readiness signal. Tech \u00b7 Exec Forecast Admissibility Surface (FAS) Determines whether individual forecast slices are structurally admissible for modeling and control based on baseline error anatomy and support characteristics. Tech Forecast Primitive Compatibility (FPC) Diagnoses structural incompatibility between forecast primitives and operational demand processes. Tech \u00b7 Exec Demand Quantization Compatibility (DQC) Formalizes admissibility of discrete demand, zero-demand states, and operating-hour semantics for evaluation and control. Tech \u00b7 Exec Forecast Governance Defines deterministic governance gates that enforce structural validity, readiness thresholds, and policy constraints prior to action. Tech \u00b7 Exec Readiness Adjustment Layer (RAL) Specifies a controlled post-evaluation adjustment mechanism that modifies forecasts to satisfy readiness and governance constraints under asymmetric cost. Tech \u00b7 Exec"},{"location":"papers/#operational-control-frameworks","title":"Operational Control Frameworks","text":"<p>Applied control frameworks that operationalize Electric Barometer primitives for specific decision environments under demand uncertainty. These frameworks consume the core ecosystem constructs to produce concrete, policy-driven decision systems.</p> Topic Description PDF Limited Time Offer Readiness (LTOR) Defines a readiness-centric control framework for managing limited-time offers under demand uncertainty, linking forecasts, loss signals, and operational resolution. Tech \u00b7 Exec"},{"location":"papers/#notes-on-versions-and-governance","title":"Notes on versions and governance","text":"<ul> <li>Download links point to the latest authoritative build produced by the release workflow (<code>*_latest.pdf</code>)</li> <li>Versioned PDFs are also produced per tag and remain archived as release assets</li> <li>Updates are deliberate, reviewable, and traceable</li> </ul> <p>This ensures the Electric Barometer framework remains credible, auditable, and durable as it evolves.</p>"},{"location":"reference/","title":"Reference","text":"<p>The Reference section provides authoritative, lookup-oriented documentation for the Electric Barometer ecosystem. It is intended for readers who already understand the conceptual framework and need precise definitions, structures, or cross-links.</p> <p>Unlike Guides, which are task-oriented, and Concepts, which explain why the system is structured as it is, Reference pages focus on what exists and how it is defined.</p>"},{"location":"reference/#what-belongs-in-reference","title":"What belongs in Reference","text":"<p>Reference documentation includes:</p> <ul> <li>Canonical definitions and terminology</li> <li>Structural overviews and schemas</li> <li>Enumerations of components or layers</li> <li>Stable contracts and interfaces</li> <li>Cross-cutting lookup material</li> </ul> <p>Reference pages prioritize clarity, completeness, and stability over narrative flow.</p>"},{"location":"reference/#what-does-not-belong-in-reference","title":"What does not belong in Reference","text":"<p>Reference documentation intentionally avoids:</p> <ul> <li> <p>Step-by-step instructions   (see Guides)</p> </li> <li> <p>Conceptual motivation or philosophy   (see Concepts)</p> </li> <li> <p>Policy decisions or tradeoff rationale   (see Optimization)</p> </li> <li> <p>Implementation details or code-level APIs   (see repository-specific documentation)</p> </li> </ul>"},{"location":"reference/#reference-sections","title":"Reference sections","text":"<p>The Reference section includes the following areas:</p>"},{"location":"reference/#glossaries-and-terminology","title":"Glossaries and terminology","text":"<ul> <li>Glossary \u2014 canonical definitions of core terms used throughout the ecosystem</li> </ul>"},{"location":"reference/#metrics-reference","title":"Metrics reference","text":"<ul> <li>Metrics \u2014 definitions and interpretation of evaluation metrics</li> <li>CWSL \u2014 reference documentation for Cost-Weighted Service Loss</li> </ul>"},{"location":"reference/#optimization-reference","title":"Optimization reference","text":"<ul> <li>Optimization \u2014 policy, tuning, and decision logic structures</li> </ul>"},{"location":"reference/#ecosystem-structure","title":"Ecosystem structure","text":"<ul> <li>Ecosystem Map \u2014 how repositories and layers fit together</li> </ul>"},{"location":"reference/#research-and-formal-definitions","title":"Research and formal definitions","text":"<ul> <li>Papers \u2014 formal technical notes and theoretical foundations</li> </ul>"},{"location":"reference/#how-to-use-the-reference-section","title":"How to use the Reference section","text":"<p>Use Reference pages when you need to:</p> <ul> <li>Confirm the meaning of a term</li> <li>Verify assumptions encoded by a metric</li> <li>Understand how components relate structurally</li> <li>Locate the authoritative definition of a concept</li> <li>Cross-reference documentation across sections</li> </ul> <p>Reference pages are designed to be consulted repeatedly, not read linearly.</p>"},{"location":"reference/#stability-and-governance","title":"Stability and governance","text":"<p>Reference documentation is treated as stable and authoritative.</p> <p>Changes to reference material should:</p> <ul> <li>Be deliberate and reviewed</li> <li>Preserve backward interpretability where possible</li> <li>Be aligned with released versions of the ecosystem</li> <li>Avoid silent or informal updates</li> </ul> <p>This stability supports governance, auditability, and long-term maintainability.</p>"},{"location":"reference/#where-to-go-next","title":"Where to go next","text":"<p>If you are:</p> <ul> <li>Looking for how to do something \u2192 see Guides</li> <li>Looking for why something exists \u2192 see Concepts</li> <li>Looking for how decisions are made \u2192 see Optimization</li> <li>Looking for formal theory \u2192 see Papers</li> </ul> <p>The Reference section exists to ground the rest of the documentation in precise, shared understanding.</p> <p>Electric Barometer documentation is designed to separate explanation, instruction, and reference. This section is where definitions live.</p>"},{"location":"start/","title":"Start Here","text":"<p>Welcome to Electric Barometer.</p> <p>This section is your entry point into the Electric Barometer framework. It is designed to orient you quickly, establish the correct mental model, and route you to the right parts of the documentation based on your goals.</p> <p>Electric Barometer is not a single library or model. It is a decision-aware evaluation and governance framework for forecasting systems operating under asymmetric cost and uncertainty.</p>"},{"location":"start/#what-electric-barometer-helps-you-do","title":"What Electric Barometer helps you do","text":"<p>Electric Barometer helps you:</p> <ul> <li>Evaluate forecasting systems under asymmetric cost</li> <li>Make tradeoffs explicit rather than implicit</li> <li>Separate measurement from decision-making</li> <li>Govern how forecasts influence operational decisions</li> <li>Maintain reproducibility and explainability over time</li> </ul> <p>If you have ever had a forecast that was \u201caccurate\u201d but still led to poor outcomes, Electric Barometer is designed for that situation.</p>"},{"location":"start/#how-this-documentation-is-organized","title":"How this documentation is organized","text":"<p>This documentation is organized by intent, not by repository or codebase.</p> <p>You will encounter five major sections:</p> <ul> <li> <p>Concepts \u2014 the foundational ideas and vocabulary   (why the system is structured the way it is)</p> </li> <li> <p>Guides \u2014 task-oriented instructions and workflows   (how to do common things)</p> </li> <li> <p>Metrics \u2014 evaluation lenses and interpretation   (what is being measured)</p> </li> <li> <p>Optimization \u2014 policy, tuning, and decision rules   (how tradeoffs are resolved)</p> </li> <li> <p>Papers \u2014 formal definitions and theoretical foundations   (the mathematical and research grounding)</p> </li> </ul> <p>Each section builds on the previous ones, but they are designed to be navigated independently as needed.</p>"},{"location":"start/#where-to-begin-recommended-paths","title":"Where to begin (recommended paths)","text":"<p>Choose the path that best matches your goal.</p>"},{"location":"start/#if-you-are-new-to-electric-barometer","title":"If you are new to Electric Barometer","text":"<p>Start with: - Quickstart \u2014 a guided, practical orientation - Concepts \u2014 the core mental model</p>"},{"location":"start/#if-you-want-to-understand-the-system-philosophy","title":"If you want to understand the system philosophy","text":"<p>Go to: - Concepts, starting with   Problem Framing and   Asymmetric Cost</p>"},{"location":"start/#if-you-want-to-run-evaluations-or-contribute","title":"If you want to run evaluations or contribute","text":"<p>Begin with: - Guides   especially Run an Evaluation</p>"},{"location":"start/#if-you-are-interested-in-metrics","title":"If you are interested in metrics","text":"<p>Explore: - Metrics   including Cost-Weighted Service Loss (CWSL)</p>"},{"location":"start/#if-you-are-defining-or-tuning-decision-logic","title":"If you are defining or tuning decision logic","text":"<p>See: - Optimization   including Policies and   Cost Ratio Optimization</p>"},{"location":"start/#what-electric-barometer-is-not","title":"What Electric Barometer is not","text":"<p>To avoid confusion, Electric Barometer is not:</p> <ul> <li>A forecasting model</li> <li>A single metric or loss function</li> <li>An automated decision engine</li> <li>A replacement for domain judgment</li> </ul> <p>It is a framework for making forecasting-driven decisions explicit, governed, and defensible.</p>"},{"location":"start/#a-simple-mental-model","title":"A simple mental model","text":"<p>At a high level, Electric Barometer follows this sequence:</p> <ol> <li>Frame the decision problem</li> <li>Evaluate forecasting system behavior</li> <li>Apply readiness considerations</li> <li>Tune policy and cost assumptions</li> <li>Make governed decisions</li> </ol> <p>This separation is intentional. Each step has a different purpose and set of responsibilities.</p>"},{"location":"start/#understanding-the-ecosystem","title":"Understanding the ecosystem","text":"<p>Electric Barometer is implemented as an ecosystem of repositories, each with a focused responsibility.</p> <p>To see how everything fits together, read: - Ecosystem Map</p> <p>That page explains how documentation, features, metrics, optimization, integration, and research relate to one another.</p>"},{"location":"start/#what-to-read-next","title":"What to read next","text":"<p>If you are unsure where to go next, the safest progression is:</p> <ol> <li>Quickstart</li> <li>Concepts</li> <li>Guides</li> <li>Metrics</li> <li>Optimization</li> </ol> <p>Each section deepens understanding without requiring you to jump ahead prematurely.</p> <p>Electric Barometer rewards careful thinking and explicit assumptions. Start here, move deliberately, and let evaluation inform decisions rather than replace them.</p>"},{"location":"start/ecosystem-map/","title":"Ecosystem Map","text":"<p>The Electric Barometer ecosystem is composed of a set of focused repositories, each responsible for a specific layer of the forecasting, evaluation, and decisioning lifecycle. These components are designed to work together through shared concepts and contracts, while remaining independently versioned and governed.</p> <p>This page provides a high-level map of the ecosystem and explains how the pieces fit together.</p>"},{"location":"start/ecosystem-map/#design-philosophy","title":"Design philosophy","text":"<p>The Electric Barometer ecosystem follows a small number of design principles:</p> <ul> <li>Separation of concerns \u2014 forecasting, evaluation, policy, and decisioning are distinct layers</li> <li>Explicit contracts \u2014 assumptions and interfaces are made visible</li> <li>Independent evolution \u2014 repositories are versioned and released independently</li> <li>Centralized concepts, distributed implementation \u2014 core ideas live in this documentation; implementation details live in their respective repositories</li> </ul> <p>The goal is clarity, not consolidation.</p>"},{"location":"start/ecosystem-map/#ecosystem-at-a-glance","title":"Ecosystem at a glance","text":"<p>At a high level, the ecosystem spans five functional layers:</p> <ol> <li> <p>Concepts and Documentation    Define the mental model, vocabulary, and system structure.</p> </li> <li> <p>Features and Adapters    Prepare inputs and connect external systems.</p> </li> <li> <p>Metrics and Evaluation    Measure forecasting system behavior under explicit assumptions.</p> </li> <li> <p>Optimization and Policy    Resolve tradeoffs and translate evaluation into decisions.</p> </li> <li> <p>Research and Formalization    Provide mathematical and theoretical grounding.</p> </li> </ol> <p>Each layer is represented by one or more repositories.</p>"},{"location":"start/ecosystem-map/#core-documentation","title":"Core documentation","text":""},{"location":"start/ecosystem-map/#eb-docs","title":"eb-docs","text":"<p>This repository serves as the central documentation hub for the ecosystem.</p> <p>It contains:</p> <ul> <li>Conceptual foundations</li> <li>Guides and workflows</li> <li>Ecosystem-level references</li> <li>Navigation and integration across repositories</li> </ul> <p>It does not duplicate code-level documentation. Instead, it routes readers to the appropriate source of truth.</p>"},{"location":"start/ecosystem-map/#features-and-data-preparation","title":"Features and data preparation","text":""},{"location":"start/ecosystem-map/#eb-features","title":"eb-features","text":"<p>The features repository defines reusable feature transforms used by forecasting systems.</p> <p>Responsibilities include:</p> <ul> <li>Feature transform definitions</li> <li>Input/output contracts</li> <li>Validation logic</li> <li>Feature-level documentation</li> </ul> <p>Feature transforms are treated as governed, reusable components rather than ad hoc preprocessing steps.</p> <p>See Add a Feature Transform for contribution guidance.</p>"},{"location":"start/ecosystem-map/#adapters-and-integration","title":"Adapters and integration","text":""},{"location":"start/ecosystem-map/#eb-adapters","title":"eb-adapters","text":"<p>The adapters repository contains integration layers that connect Electric Barometer to external systems and environments.</p> <p>Adapters handle:</p> <ul> <li>Data ingestion and extraction</li> <li>Schema translation</li> <li>Environment-specific I/O</li> <li>External service integration</li> </ul> <p>Adapters isolate operational concerns from evaluation and decision logic.</p> <p>See Add an Adapter for details.</p>"},{"location":"start/ecosystem-map/#metrics-and-evaluation","title":"Metrics and evaluation","text":""},{"location":"start/ecosystem-map/#eb-metrics","title":"eb-metrics","text":"<p>The metrics repository defines evaluation measures used to assess forecasting system behavior.</p> <p>It includes:</p> <ul> <li>Cost-aware and decision-facing metrics</li> <li>Parameterized metric definitions</li> <li>Validation and interpretation guidance</li> </ul> <p>Metrics measure behavior; they do not make decisions.</p> <p>The flagship metric in this repository is Cost-Weighted Service Loss (CWSL). See Metrics for an overview.</p>"},{"location":"start/ecosystem-map/#optimization-and-policy","title":"Optimization and policy","text":""},{"location":"start/ecosystem-map/#eb-optimization","title":"eb-optimization","text":"<p>The optimization repository contains logic and frameworks for translating evaluation outputs into governed decisions.</p> <p>It covers:</p> <ul> <li>Cost ratio configuration</li> <li>Policy definition and tuning</li> <li>Tie-breaking and selection logic</li> <li>Readiness-aware optimization</li> </ul> <p>Optimization operates above metrics and models, encoding preference and intent explicitly.</p> <p>See Optimization to understand this layer.</p>"},{"location":"start/ecosystem-map/#forecasting-and-execution","title":"Forecasting and execution","text":""},{"location":"start/ecosystem-map/#eb-integration","title":"eb-integration","text":"<p>The integration repository coordinates end-to-end execution across features, forecasting systems, evaluation, and optimization.</p> <p>It provides:</p> <ul> <li>Workflow orchestration</li> <li>System assembly</li> <li>Execution configuration</li> <li>End-to-end validation</li> </ul> <p>This repository ties the ecosystem together without redefining core logic.</p>"},{"location":"start/ecosystem-map/#research-and-formal-foundations","title":"Research and formal foundations","text":""},{"location":"start/ecosystem-map/#eb-papers","title":"eb-papers","text":"<p>The papers repository contains formal technical notes and research papers that define the mathematical and theoretical foundations of the ecosystem.</p> <p>It includes:</p> <ul> <li>Formal metric definitions</li> <li>Theoretical frameworks</li> <li>Extended analysis and proofs</li> </ul> <p>These artifacts serve as stable, citable references.</p>"},{"location":"start/ecosystem-map/#how-the-pieces-fit-together","title":"How the pieces fit together","text":"<p>A typical lifecycle flows as follows:</p> <ol> <li>Feature transforms prepare inputs</li> <li>Adapters connect data and execution environments</li> <li>Forecasting systems generate predictions</li> <li>Metrics evaluate system behavior</li> <li>Optimization applies policy and readiness</li> <li>Decisions are selected and governed</li> <li>Releases preserve reproducibility</li> </ol> <p>Each step corresponds to one or more repositories, coordinated through shared concepts rather than tight coupling.</p>"},{"location":"start/ecosystem-map/#navigating-the-ecosystem","title":"Navigating the ecosystem","text":"<p>If you are:</p> <ul> <li>New to Electric Barometer \u2014 start with Concepts</li> <li>Running evaluations \u2014 see Guides</li> <li>Working with metrics \u2014 explore Metrics</li> <li>Defining policies \u2014 review Optimization</li> <li>Looking for formal definitions \u2014 consult Papers</li> </ul> <p>This documentation site provides the map; each repository provides the detail.</p>"},{"location":"start/ecosystem-map/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Continue with Start Here for onboarding</li> <li>Read Concepts to understand system philosophy</li> <li>Explore Guides for hands-on workflows</li> </ul> <p>The Electric Barometer ecosystem is designed to scale in complexity without losing clarity. This map is your guide to how the pieces work together.</p>"},{"location":"start/quickstart/","title":"Quickstart: End-to-End Forecast Readiness Workflow","text":"<p>A guided, canonical path through the Electric Barometer system\u2014from a preprocessed dataset to a governed, execution-ready forecast.</p> <p>This page is not a modeling tutorial and it is not a \u201cone weird trick\u201d shortcut. It is the shortest correct route through the framework: you will (1) lock semantics, (2) validate admissibility, (3) evaluate readiness, (4) apply controlled adjustment only when allowed, and (5) finish with auditable artifacts suitable for operational use.</p>"},{"location":"start/quickstart/#what-youll-accomplish","title":"What you\u2019ll accomplish","text":"<ul> <li>Convert \u201cready-to-rock\u201d data into Electric Barometer contracts with explicit semantics.</li> <li>Run a model zoo to produce comparable forecast candidates.</li> <li>Evaluate candidates using FRF diagnostics (NSL, CWSL, UD, HR@\u03c4, FRS) under valid units.</li> <li>Issue a deterministic GovernanceDecision (DQC \u2192 FPC \u2192 policy closure).</li> <li>Apply RAL only when admissible, then select a winner based on readiness\u2014not just accuracy.</li> </ul>"},{"location":"start/quickstart/#what-this-assumes","title":"What this assumes","text":"<ul> <li>You already have a preprocessed dataset and a defined forecast resolution (e.g., 30-min, hourly, daily).</li> <li>You know the forecast entity definition (e.g., store\u2013item, item, workload stream).</li> <li>You can produce baseline forecasts from one or more models (internal or EB model zoo).</li> </ul>"},{"location":"start/quickstart/#what-this-does-not-cover","title":"What this does not cover","text":"<ul> <li>Data cleaning, feature engineering, or training infrastructure</li> <li>Deep theory and derivations (see the Papers / Technical Notes section)</li> <li>Domain-specific operating procedures (these belong in dedicated Guides)</li> </ul>"},{"location":"start/quickstart/#inputs-outputs-at-a-glance","title":"Inputs \u2192 Outputs (at a glance)","text":"<p>Inputs - Preprocessed demand history (+ any covariates) - Forecast horizon + resolution - Operational cost assumptions (as governance parameters)</p> <p>Outputs - Contract-aligned demand + forecast artifacts - FRF evaluation report(s) - GovernanceDecision (authoritative semantics + policy) - Optional RAL-adjusted forecasts - A defensible \u201cwinner\u201d model/policy bundle for deployment</p>"},{"location":"start/quickstart/#end-to-end-forecast-readiness-workflow","title":"End-to-End Forecast Readiness Workflow","text":"<p>The diagram below illustrates the canonical execution path for producing an execution-ready forecast using the Electric Barometer framework. Forecast generation and readiness evaluation occur across multiple candidates, but semantic adjudication and control are applied only after a single forecast is selected. Once entered, the governance and control stages are single-threaded and irreversible.</p> <pre><code>flowchart TD\n\n  subgraph L1 [\"Layer 1 - Forecast Generation\"]\n    A[\"Step 1 - Contractify demand\"] --&gt; B[\"Step 2 - Model zoo: train and forecast candidates\"]\n    B --&gt; C[\"Forecast candidates (multiple)\"]\n  end\n\n  subgraph L2 [\"Layer 2 - Readiness Evaluation (FRF)\"]\n    C --&gt; D[\"Step 3.1 - Sensitivity: CWSL over R-grid; HR over tau-grid\"]\n    D --&gt; E[\"Step 3.2 - Choose reference parameters R_ref and tau_ref\"]\n    E --&gt; F[\"Step 3.3 - Reference FRF panel across candidates\"]\n    F --&gt; G{Select one leading forecast}\n  end\n\n  subgraph L3 [\"Layer 3 - Semantics, Governance, Control\"]\n    G --&gt; H[\"Step 4 - DQC: unit semantics\"]\n    H --&gt; I[\"Step 5 - FPC: control admissibility\"]\n    I --&gt; J[\"Step 6 - Governance decision\"]\n    J --&gt; K{RAL allowed?}\n    K -- Yes --&gt; L[\"Step 7 - RAL: bounded adjustment\"]\n    K -- No --&gt; M[\"No adjustment: pass-through\"]\n    L --&gt; N[\"Step 8 - Final readiness and deployment artifacts\"]\n    M --&gt; N\n  end</code></pre>"},{"location":"start/quickstart/#step-1-express-demand-using-electric-barometer-contracts","title":"Step 1 \u2014 Express Demand Using Electric Barometer Contracts","text":"<p>Before any forecasting or evaluation occurs, realized demand must be expressed using Electric Barometer data contracts. This step establishes the authoritative semantic definition of the forecasting problem: what is being forecast, at what resolution, in what units, and under what structural assumptions.</p> <p>Electric Barometer does not operate directly on arbitrary tables or ad hoc DataFrames. Instead, all downstream evaluation, governance, and policy logic assumes that demand has been materialized into a contract-defined representation.</p> <p>At this stage, you should:</p> <ul> <li>Define the forecast entity (e.g., item, store\u2013item pair, workload stream).</li> <li>Confirm the forecast resolution (the atomic time interval at which demand is   forecast, evaluated, and acted upon) and the corresponding time index used to   align observations.</li> <li>Map realized demand into an Electric Barometer demand contract   (typically <code>PanelDemandV1</code>) using <code>eb-adapters</code>.</li> <li>Ensure required semantic fields are populated, including observability indicators   and any known structural-zero or non-observable intervals.</li> </ul> <p>No forecasting, evaluation, or readiness logic is applied here. This step exists solely to lock the semantic meaning of demand so that all downstream models and diagnostics operate on a shared, governed interpretation.</p>"},{"location":"start/quickstart/#step-2-generate-candidate-forecasts-using-the-electric-barometer-model-zoo","title":"Step 2 \u2014 Generate Candidate Forecasts Using the Electric Barometer Model Zoo","text":"<p>With demand semantics locked via Electric Barometer contracts, the next step is to generate candidate forecasts using one or more forecasting models. Electric Barometer provides a model adapter layer and a corresponding model zoo execution path to support this process.</p> <p>Model adapters are implemented in <code>eb-adapters</code> and wrap supported forecasting libraries (e.g., LightGBM, CatBoost, XGBoost, Prophet, statsmodels). These adapters standardize model interfaces and materialize forecast outputs into Electric Barometer forecast contracts. The orchestration of multiple candidate models (the \u201cmodel zoo\u201d) is performed by <code>eb-evaluation</code>.</p> <p>At this stage, you should:</p> <ul> <li>Select one or more supported model adapters from <code>eb-adapters</code>   (e.g., LightGBM, CatBoost, XGBoost).</li> <li>Train the corresponding models using contract-aligned demand data.</li> <li>Generate baseline forecasts for the defined horizon and resolution.</li> <li>Materialize each model\u2019s output as an Electric Barometer forecast contract   (e.g., <code>PanelPointForecastV1</code>) so all candidates share a common semantic representation.</li> </ul> <p>No asymmetric cost, tolerance interpretation, readiness adjustment, or policy logic is applied here. Forecasts produced in this step are treated strictly as candidates, not decisions.</p> <p>This step exists to populate the comparison space. Model evaluation, admissibility checks, governance, and readiness adjustment occur downstream and must not be conflated with forecast generation.</p>"},{"location":"start/quickstart/#step-3-evaluate-all-candidate-forecasts-using-the-full-frf","title":"Step 3 \u2014 Evaluate All Candidate Forecasts Using the Full FRF","text":"<p>Once the model zoo has produced a set of candidate forecasts, all candidates must be evaluated holistically using the Electric Barometer Forecast Readiness Framework (FRF). This step compares models based on operational readiness\u2014not just statistical accuracy.</p> <p>Using <code>eb-evaluation</code>, compute the full FRF metric suite for every forecast candidate:</p> <ul> <li>No\u2013Shortfall Level (NSL) \u2014 how often demand is fully covered</li> <li>Cost\u2013Weighted Service Loss (CWSL) \u2014 asymmetric economic exposure</li> <li>Underbuild Depth (UD) \u2014 severity of shortfalls when they occur</li> <li>Hit Rate @ \u03c4 (HR@\u03c4) \u2014 tolerance stability</li> <li>Forecast Readiness Score (FRS) \u2014 composite readiness summary</li> </ul> <p>Results are typically reviewed using FRF evaluation panels, which present all metrics side-by-side for each model. These panels surface tradeoffs between service protection, cost exposure, and failure severity that are not visible through single-metric evaluation.</p> <p>At the conclusion of this step:</p> <ul> <li>Compare all candidate models using their full FRF readiness profiles.</li> <li>Select one leading forecast candidate that best aligns with readiness objectives   and operational priorities.</li> <li>Discard remaining candidates from further processing.</li> </ul> <p>This is the final model-selection step. No demand semantics, governance logic, or readiness adjustment is applied yet. The selected forecast proceeds unchanged into semantic adjudication and control in the next stages.</p>"},{"location":"start/quickstart/#step-3-evaluate-all-candidates-with-frf-sensitivity-first-then-reference-selection","title":"Step 3 \u2014 Evaluate All Candidates with FRF (Sensitivity First, Then Reference Selection)","text":"<p>Before selecting a leading forecast candidate, evaluate every model holistically using the Forecast Readiness Framework (FRF). Because key FRF metrics depend on governed parameters (cost ratio <code>R</code> for CWSL and tolerance <code>\u03c4</code> for HR@\u03c4), begin with sensitivity analysis and then produce a single reference comparison panel for selection.</p>"},{"location":"start/quickstart/#step-31-sensitivity-evaluation-r-grid-and-grid","title":"Step 3.1 \u2014 Sensitivity evaluation (R-grid and \u03c4-grid)","text":"<ul> <li>Score each candidate forecast across a small governed <code>R_grid</code> (CWSL-R surface).</li> <li>Score each candidate forecast across a governed <code>\u03c4_grid</code> (HR-\u03c4 surface).</li> <li>Use panels to compare readiness profiles and identify fragile vs stable candidates.</li> </ul>"},{"location":"start/quickstart/#step-32-choose-reference-parameters-r-and","title":"Step 3.2 \u2014 Choose reference parameters (R and \u03c4)","text":"<ul> <li>Select or calibrate <code>R*</code> using a deterministic policy (e.g., cost-balance calibration).</li> <li>Select or calibrate <code>\u03c4*</code> using a governed rule (e.g., target hit-rate / knee rule).</li> </ul>"},{"location":"start/quickstart/#step-33-reference-frf-panel-and-candidate-selection","title":"Step 3.3 \u2014 Reference FRF panel and candidate selection","text":"<ul> <li>Re-score all candidates under the reference parameters (<code>R*</code>, <code>\u03c4*</code>).</li> <li>Produce a single comparison panel (rows = models; columns = FRF metrics).</li> <li>Select one leading forecast candidate to advance unchanged to DQC.</li> </ul>"},{"location":"start/quickstart/#step-4-demand-quantization-compatibility-dqc","title":"Step 4 \u2014 Demand Quantization Compatibility (DQC)","text":"<p>After a single leading forecast candidate has been selected using FRF evaluation, the next step is to determine whether forecast errors and tolerances can be interpreted in raw continuous units, or whether demand must be treated as quantized.</p> <p>Demand Quantization Compatibility (DQC) is a structural diagnostic applied to realized demand. It determines the valid unit system for interpreting forecast error, tolerance, and readiness metrics. DQC does not compare models and does not modify forecasts.</p> <p>At this stage, you should:</p> <ul> <li>Analyze realized demand for evidence of quantization or unit packing.</li> <li>Determine whether demand is:</li> <li>continuous-like,</li> <li>quantized to a fixed grid, or</li> <li>piecewise packed.</li> <li>If quantized, infer the governing demand grid (\u0394*).</li> <li>Declare the admissible unit system for evaluation and control   (raw units vs snapped/grid units).</li> </ul> <p>The outcome of DQC establishes how forecast error and tolerance must be interpreted downstream. In particular, it determines whether tolerance parameters (e.g., <code>\u03c4</code> for HR@\u03c4) and any future readiness adjustments must be expressed in grid units rather than raw numeric units.</p> <p>No readiness adjustment, cost recalibration, or model selection occurs here. DQC exists solely to ensure that all subsequent interpretation and control operate in a valid and well-defined unit space.</p>"},{"location":"start/quickstart/#step-5-forecast-primitive-compatibility-fpc","title":"Step 5 \u2014 Forecast Primitive Compatibility (FPC)","text":"<p>With demand semantics resolved via DQC, the next step is to determine whether the selected forecast primitive itself is compatible with readiness control. Forecast Primitive Compatibility (FPC) assesses whether controlled adjustment (e.g., scaling, tolerance-based intervention) can improve readiness in a meaningful and stable way.</p> <p>FPC is not a performance metric and it is not a model comparison tool. It is a structural admissibility check applied to the selected forecast primitive under the now-declared unit semantics.</p> <p>At this stage, you should:</p> <ul> <li>Examine how the selected forecast responds to small, controlled perturbations   (e.g., scale adjustments or tolerance variation).</li> <li>Evaluate whether readiness metrics (NSL, CWSL, UD) respond:</li> <li>monotonically,</li> <li>smoothly,</li> <li>and predictably to such perturbations.</li> <li>Identify pathological behavior, such as:</li> <li>unstable coverage response,</li> <li>cost explosions with minimal service gain,</li> <li>or discrete jumps that make control ill-defined.</li> </ul> <p>Based on this analysis, classify the forecast primitive as: - Compatible \u2014 readiness control is admissible and well-behaved, - Marginal \u2014 limited or tightly constrained control may be possible, - Incompatible \u2014 readiness control should not be applied.</p> <p>FPC does not modify forecasts. It exists solely to determine whether downstream readiness adjustment is structurally justified. If a forecast primitive is incompatible, all subsequent control logic must default to identity behavior.</p>"},{"location":"start/quickstart/#step-6-governance-decision","title":"Step 6 \u2014 Governance Decision","text":"<p>With demand semantics established (DQC) and forecast primitive admissibility determined (FPC), the next step is to issue a Governance Decision. Governance is the point at which diagnostics are closed and the system declares what interpretations and actions are authoritatively allowed.</p> <p>The Governance Decision fuses: - the unit semantics declared by DQC, - the control admissibility verdict from FPC, - and any external policy constraints or operational rules.</p> <p>At this stage, you should:</p> <ul> <li>Confirm the authoritative unit system for evaluation and control   (raw units vs snapped/grid units).</li> <li>Declare how tolerance parameters (e.g., <code>\u03c4</code>) must be interpreted   under the resolved semantics.</li> <li>Decide whether readiness adjustment is:</li> <li>allowed,</li> <li>restricted,</li> <li>or prohibited entirely.</li> <li>Encode any bounds, safeguards, or default behaviors required by policy.</li> </ul> <p>The output of this step is a single, explicit Governance Decision artifact that authoritatively defines: - how forecast error is interpreted, - which readiness controls (if any) are permitted, - and the constraints under which they must operate.</p> <p>No forecasts are modified here. Governance exists to close ambiguity and prevent implicit or ad hoc decision-making in downstream execution.</p>"},{"location":"start/quickstart/#step-7-readiness-adjustment-layer-ral","title":"Step 7 \u2014 Readiness Adjustment Layer (RAL)","text":"<p>If and only if the Governance Decision permits readiness control, the selected forecast may be passed through the Readiness Adjustment Layer (RAL). RAL applies bounded, deterministic adjustments to the forecast to improve operational readiness under asymmetric cost\u2014without retraining models or altering forecast semantics.</p> <p>RAL is a control layer, not a tuning or learning process. It operates strictly within the constraints declared by governance and defaults to identity behavior when adjustment is not allowed or not beneficial.</p> <p>At this stage, you should:</p> <ul> <li>Apply readiness adjustment to the selected forecast using only the controls   permitted by governance (e.g., bounded scaling).</li> <li>Ensure all adjustments respect:</li> <li>the unit semantics declared by DQC,</li> <li>the admissibility constraints from FPC,</li> <li>and any policy bounds specified in governance.</li> <li>Re-evaluate readiness metrics (NSL, CWSL, UD, HR@\u03c4, FRS) to verify that adjustment   produces a defensible improvement.</li> </ul> <p>If no admissible adjustment exists, RAL must return the original forecast unchanged.</p> <p>RAL produces a new forecast artifact that is: - semantically equivalent to the original forecast, - explicitly governed, - and optimized for readiness rather than statistical accuracy.</p>"},{"location":"start/quickstart/#step-8-final-readiness-assessment-and-deployment-artifacts","title":"Step 8 \u2014 Final Readiness Assessment and Deployment Artifacts","text":"<p>The final step consolidates the outputs of evaluation, governance, and (optional) readiness adjustment into a set of authoritative, execution-ready artifacts. At this point, all semantics are closed, all admissibility decisions have been made, and the forecast is ready to be consumed by downstream operational systems.</p> <p>At this stage, you should:</p> <ul> <li>Perform a final readiness assessment using the full FRF metric suite   (NSL, CWSL, UD, HR@\u03c4, FRS) on the governed forecast output   (adjusted or unadjusted).</li> <li>Confirm that readiness metrics reflect the intended tradeoffs under the   declared cost asymmetry, tolerance interpretation, and unit semantics.</li> <li>Package and persist the following artifacts:</li> <li>the final forecast (in Electric Barometer forecast contract form),</li> <li>the Governance Decision artifact,</li> <li>the FRF evaluation results used to justify deployment,</li> <li>and any RAL parameters applied (if applicable).</li> </ul> <p>The outputs of this step constitute the official forecasting decision. These artifacts are designed to be auditable, reproducible, and interpretable independently of the modeling process that produced them.</p> <p>No further tuning, adjustment, or reinterpretation should occur beyond this point without re-entering the framework at an earlier step.</p>"},{"location":"workflows/","title":"Workflows","text":"<p>This section describes how Electric Barometer is used in practice.</p> <p>Electric Barometer is not organized around \u201ctrain a model \u2192 score a model.\u201d It is organized around repeatable decision workflows in which evaluation is structurally valid, assumptions are governed, and outputs are deployable as explicit policy artifacts.</p>"},{"location":"workflows/#what-a-workflow-is-in-electric-barometer","title":"What a workflow is in Electric Barometer","text":"<p>A workflow is a deterministic pipeline that begins with inputs (data, forecasts, candidate configurations) and ends with an auditable outcome (metrics, selections, policy decisions, and reproducibility artifacts).</p> <p>Workflows are intentionally designed to enforce:</p> <ul> <li>Separation of concerns (diagnostics diagnose, governance decides, downstream systems execute)</li> <li>Structural validity (units and primitives must be admissible before interpretation)</li> <li>Decision closure (no \u201cinterpretation drift\u201d after a decision artifact is issued)</li> <li>Reproducibility (results are traceable to fixed inputs, thresholds, and versions)</li> </ul>"},{"location":"workflows/#workflow-map","title":"Workflow map","text":"<p>The workflows in this section are ordered as a complete pipeline:</p> <ul> <li> <p>Data \u2192 Forecast   Define evaluation entities, windows, and admissible unit conventions; produce forecast-ready panels and candidate forecast sets.</p> </li> <li> <p>Forecast \u2192 Evaluation   Compute readiness diagnostics (e.g., coverage, asymmetric loss, tolerance behavior) under governed interpretation semantics.</p> </li> <li> <p>Evaluation \u2192 Selection   Select among candidate forecasts using readiness-oriented objectives and tie-breaking rules, without collapsing evaluation into model training.</p> </li> <li> <p>Selection \u2192 Policy   Convert evaluated outcomes into explicit, bounded policies suitable for deployment (including readiness adjustment allowability and policy structure).</p> </li> <li> <p>Reproducibility Artifacts   Record the minimal artifact set required to reproduce an evaluation, a selection, and a policy decision exactly.</p> </li> </ul>"},{"location":"workflows/#scope-and-non-goals","title":"Scope and non-goals","text":"<p>These workflow docs are intentionally narrow. They describe how Electric Barometer is executed, not how to build forecasting models.</p> <p>In particular, this section does not attempt to:</p> <ul> <li>prescribe modeling methods or feature engineering strategies,</li> <li>optimize models directly against readiness metrics,</li> <li>replace domain judgment or operational constraints,</li> <li>define organizational processes (approvals, ownership, operating cadences).</li> </ul> <p>Instead, the goal is to document the mechanics of an auditable readiness pipeline.</p>"},{"location":"workflows/#intended-audience","title":"Intended audience","text":"<p>These workflows are written for:</p> <ul> <li>operators / decision owners who need defensible readiness behavior,</li> <li>analysts and applied scientists evaluating candidate systems under asymmetric risk,</li> <li>platform and data engineers implementing repeatable evaluation and governance pipelines.</li> </ul> <p>If you are new to the framework, start with Concepts first, then return here to see how those ideas are operationalized.</p>"},{"location":"workflows/#conventions","title":"Conventions","text":"<p>Across all workflow pages:</p> <ul> <li>\u201cEvaluation\u201d refers to computing diagnostics under declared assumptions.</li> <li>\u201cSelection\u201d refers to choosing among candidates using those diagnostics.</li> <li>\u201cPolicy\u201d refers to explicit, bounded, versioned rules suitable for downstream execution.</li> <li>Any parameter that changes interpretation (e.g., tolerance, asymmetry, snapping semantics) is treated as a governed artifact, not an ad hoc knob.</li> </ul> <p>This section aims to make the pipeline obvious, repeatable, and reviewable.</p>"},{"location":"workflows/data-to-forecast/","title":"Data \u2192 Forecast","text":"<p>This workflow defines the boundary between raw data and forecast candidates that are admissible for readiness evaluation.</p> <p>Electric Barometer does not assume that any forecast can be evaluated safely. Before diagnostics, selection, or policy are applied, inputs must be structured such that units, resolution, entities, and horizons are explicit and stable. This workflow establishes those prerequisites.</p>"},{"location":"workflows/data-to-forecast/#objective","title":"Objective","text":"<p>The objective of the Data \u2192 Forecast workflow is to produce a forecast-ready evaluation panel and a set of candidate forecasts that:</p> <ul> <li>share a common evaluation resolution and horizon,</li> <li>are aligned to declared entities and aggregation semantics,</li> <li>admit governed interpretation downstream,</li> <li>and are reproducible from declared inputs.</li> </ul> <p>This workflow terminates at forecast generation. It does not evaluate, score, or select forecasts.</p>"},{"location":"workflows/data-to-forecast/#inputs","title":"Inputs","text":"<p>This workflow consumes the following categories of inputs:</p>"},{"location":"workflows/data-to-forecast/#1-realized-demand-data","title":"1. Realized demand data","text":"<p>Observed demand values indexed by: - entity (e.g., item, location, workload stream), - time (at the evaluation resolution), - and any declared hierarchy levels.</p> <p>Demand data is treated as authoritative and immutable within the evaluation window. No smoothing, imputation, or transformation decisions are implied by this workflow.</p>"},{"location":"workflows/data-to-forecast/#2-evaluation-configuration","title":"2. Evaluation configuration","text":"<p>Explicit declarations of: - evaluation resolution (e.g., 30-minute, hourly, daily), - evaluation window and horizon, - entity granularity (single entity vs panel), - inclusion / exclusion rules (e.g., closed periods, out-of-scope entities).</p> <p>These declarations define what will be evaluated, not how performance will be judged.</p>"},{"location":"workflows/data-to-forecast/#3-forecast-generation-mechanisms","title":"3. Forecast generation mechanisms","text":"<p>One or more forecasting systems capable of producing baseline forecasts aligned to the declared configuration.</p> <p>Electric Barometer imposes no constraints on: - model class, - training methodology, - feature engineering, - or learning paradigm.</p> <p>Forecasts are treated as opaque outputs at this stage.</p>"},{"location":"workflows/data-to-forecast/#workflow-steps","title":"Workflow steps","text":""},{"location":"workflows/data-to-forecast/#step-1-declare-the-evaluation-frame","title":"Step 1: Declare the evaluation frame","text":"<p>The evaluation frame defines the coordinate system in which all downstream reasoning occurs.</p> <p>At minimum, this includes: - time resolution, - evaluation horizon, - entity identifiers, - and aggregation semantics.</p> <p>Once declared, this frame must remain fixed for the duration of the workflow. Any change to the frame constitutes a new evaluation context.</p>"},{"location":"workflows/data-to-forecast/#step-2-assemble-the-realized-demand-panel","title":"Step 2: Assemble the realized demand panel","text":"<p>Realized demand is aligned to the evaluation frame without interpretation.</p> <p>Key requirements: - all values are indexed consistently, - missing observations are represented explicitly, - no assumptions are made about continuity, smoothness, or distribution.</p> <p>Structural properties of demand (e.g., discreteness, intermittency) are not inferred here. They are diagnosed later.</p>"},{"location":"workflows/data-to-forecast/#step-3-generate-candidate-forecasts","title":"Step 3: Generate candidate forecasts","text":"<p>Each forecasting system produces a baseline forecast aligned to the evaluation frame.</p> <p>At this stage: - forecasts are not adjusted, - no readiness intervention is applied, - no cost or tolerance assumptions are introduced.</p> <p>Each forecast is treated as a candidate, not as a decision.</p>"},{"location":"workflows/data-to-forecast/#step-4-validate-structural-alignment","title":"Step 4: Validate structural alignment","text":"<p>Before evaluation, forecasts are checked for structural compatibility with the frame:</p> <ul> <li>correct resolution and horizon,</li> <li>non-negativity (where required),</li> <li>consistent indexing,</li> <li>and absence of implicit aggregation or smoothing.</li> </ul> <p>Forecasts that fail basic structural alignment are excluded before evaluation, not penalized downstream.</p>"},{"location":"workflows/data-to-forecast/#outputs","title":"Outputs","text":"<p>This workflow produces two artifacts:</p>"},{"location":"workflows/data-to-forecast/#1-forecast-ready-evaluation-panel","title":"1. Forecast-ready evaluation panel","text":"<p>A panel containing: - realized demand, - aligned forecast candidates, - entity and time indices, - and evaluation metadata.</p> <p>This panel is the sole input to downstream diagnostics.</p>"},{"location":"workflows/data-to-forecast/#2-forecast-candidate-set","title":"2. Forecast candidate set","text":"<p>A finite, explicitly enumerated set of forecasts eligible for evaluation.</p> <p>No ranking, scoring, or filtering occurs here beyond structural admissibility.</p>"},{"location":"workflows/data-to-forecast/#scope-and-non-goals","title":"Scope and non-goals","text":"<p>This workflow explicitly does not:</p> <ul> <li>diagnose demand structure,</li> <li>assess forecast quality or readiness,</li> <li>apply asymmetric cost,</li> <li>tune or adjust forecasts,</li> <li>or determine policy allowability.</li> </ul> <p>Those responsibilities belong to later workflows.</p> <p>The purpose of Data \u2192 Forecast is preparation and constraint, not judgment.</p>"},{"location":"workflows/data-to-forecast/#governance-notes","title":"Governance notes","text":"<p>All declarations made in this workflow are binding.</p> <p>Downstream stages may not: - reinterpret resolution, - alter entity definitions, - or introduce implicit assumptions about units or continuity.</p> <p>If such changes are required, the workflow must be rerun under a new evaluation context.</p>"},{"location":"workflows/data-to-forecast/#transition-to-evaluation","title":"Transition to evaluation","text":"<p>Once this workflow completes, the system has everything required to ask:</p> <p>Given these forecasts and this demand, under admissible interpretation, how ready is the system?</p> <p>That question is addressed in Forecast \u2192 Evaluation.</p>"},{"location":"workflows/evaluation-to-selection/","title":"Evaluation \u2192 Selection","text":"<p>This workflow defines how evaluated forecasts are compared and selected without collapsing evaluation into optimization or model training.</p> <p>In Electric Barometer, evaluation produces diagnostic evidence, not decisions. Selection is a separate, explicit step that transforms evaluated candidates into a chosen forecast under declared readiness objectives and tie-breaking rules.</p>"},{"location":"workflows/evaluation-to-selection/#objective","title":"Objective","text":"<p>The objective of the Evaluation \u2192 Selection workflow is to select one forecast candidate per evaluation context using readiness-oriented criteria, while preserving:</p> <ul> <li>interpretive validity (units and primitives already governed),</li> <li>separation of diagnostics from decisions,</li> <li>and auditability of why a candidate was chosen.</li> </ul> <p>Selection does not modify forecasts, apply control, or encode policy.</p>"},{"location":"workflows/evaluation-to-selection/#inputs","title":"Inputs","text":"<p>This workflow consumes:</p>"},{"location":"workflows/evaluation-to-selection/#1-evaluated-forecast-candidates","title":"1. Evaluated forecast candidates","text":"<p>For each candidate forecast: - readiness diagnostics (e.g., NSL, CWSL, FRS, UD, HR@\u03c4), - diagnostic metadata (evaluation window, resolution, unit semantics), - and any admissibility flags produced upstream.</p> <p>All diagnostics are assumed to be computed under authoritative interpretation semantics.</p>"},{"location":"workflows/evaluation-to-selection/#2-selection-criteria","title":"2. Selection criteria","text":"<p>Explicit, declared rules defining how candidates are compared, such as: - primary readiness objective(s), - secondary criteria for trade-offs, - and deterministic tie-breakers.</p> <p>Selection criteria are policy declarations, not learned or inferred preferences.</p>"},{"location":"workflows/evaluation-to-selection/#3-governance-constraints-informational","title":"3. Governance constraints (informational)","text":"<p>Outputs from structural diagnostics (e.g., DQC, FPC) that constrain admissible selection behavior.</p> <p>Selection may not override or reinterpret these constraints.</p>"},{"location":"workflows/evaluation-to-selection/#workflow-steps","title":"Workflow steps","text":""},{"location":"workflows/evaluation-to-selection/#step-1-filter-inadmissible-candidates","title":"Step 1: Filter inadmissible candidates","text":"<p>Candidates that are structurally inadmissible are removed prior to comparison.</p> <p>Examples include: - forecasts failing primitive compatibility requirements, - candidates evaluated under non-authoritative unit systems, - or outputs flagged as incompatible by governance diagnostics.</p> <p>Inadmissible candidates are excluded, not penalized.</p>"},{"location":"workflows/evaluation-to-selection/#step-2-define-the-selection-objective","title":"Step 2: Define the selection objective","text":"<p>Selection objectives must be explicit and stable.</p> <p>Common objectives include: - maximizing Forecast Readiness Score (FRS), - minimizing Cost-Weighted Service Loss (CWSL) subject to coverage constraints, - or prioritizing reliability (NSL) with bounded cost exposure.</p> <p>Only declared diagnostics may be used.</p>"},{"location":"workflows/evaluation-to-selection/#step-3-compare-candidates-under-the-objective","title":"Step 3: Compare candidates under the objective","text":"<p>Candidates are compared using the declared objective and secondary criteria.</p> <p>Key properties: - no reweighting of diagnostics, - no aggregation across incompatible contexts, - no smoothing or interpolation across candidates.</p> <p>Comparison is deterministic.</p>"},{"location":"workflows/evaluation-to-selection/#step-4-resolve-ties-and-ambiguities","title":"Step 4: Resolve ties and ambiguities","text":"<p>When multiple candidates satisfy the primary objective equivalently, declared tie-breaking rules are applied (e.g., lower variance, simpler primitive, lower surplus exposure).</p> <p>Ad hoc judgment is explicitly out of scope.</p>"},{"location":"workflows/evaluation-to-selection/#outputs","title":"Outputs","text":"<p>This workflow produces:</p>"},{"location":"workflows/evaluation-to-selection/#1-selected-forecast","title":"1. Selected forecast","text":"<p>A single forecast candidate designated as the selection outcome for the evaluation context.</p> <p>The selected forecast remains unmodified.</p>"},{"location":"workflows/evaluation-to-selection/#2-selection-rationale","title":"2. Selection rationale","text":"<p>A minimal, structured record containing: - the objective used, - the diagnostics evaluated, - the reason the selected candidate dominated alternatives, - and any tie-breaking applied.</p> <p>This rationale is required for audit and reproducibility.</p>"},{"location":"workflows/evaluation-to-selection/#scope-and-non-goals","title":"Scope and non-goals","text":"<p>This workflow does not:</p> <ul> <li>retrain or adjust models,</li> <li>apply readiness adjustment,</li> <li>encode operational policy,</li> <li>or reinterpret diagnostic semantics.</li> </ul> <p>Selection chooses among evaluated options; it does not alter them.</p>"},{"location":"workflows/evaluation-to-selection/#governance-notes","title":"Governance notes","text":"<p>Selection is subordinate to governance.</p> <p>If governance diagnostics declare a forecast primitive incompatible or a representation inadmissible, selection must respect that constraint.</p> <p>Selection cannot legitimize structurally invalid behavior through performance.</p>"},{"location":"workflows/evaluation-to-selection/#transition-to-policy","title":"Transition to policy","text":"<p>After selection, the system has answered:</p> <p>Which forecast should we consider under readiness evaluation?</p> <p>The next question is:</p> <p>What are we allowed to do with it?</p> <p>That question is addressed in Selection \u2192 Policy.</p>"},{"location":"workflows/forecast-to-evaluation/","title":"Forecast \u2192 Evaluation","text":"<p>This workflow defines how forecasts are evaluated diagnostically under the Forecast Readiness Framework.</p> <p>In Electric Barometer, evaluation is not a scoring exercise and not a proxy for decision-making. Its purpose is to characterize readiness behavior under governed interpretation semantics without prescribing action.</p>"},{"location":"workflows/forecast-to-evaluation/#objective","title":"Objective","text":"<p>The objective of the Forecast \u2192 Evaluation workflow is to compute structurally valid readiness diagnostics for each forecast candidate, given realized demand, while enforcing:</p> <ul> <li>admissible unit interpretation,</li> <li>separation of diagnostics from decisions,</li> <li>and auditability of assumptions.</li> </ul> <p>Evaluation answers the question:</p> <p>How does this forecast behave under readiness-relevant criteria, assuming declared costs, tolerances, and units?</p> <p>It does not answer whether the forecast should be selected or deployed.</p>"},{"location":"workflows/forecast-to-evaluation/#inputs","title":"Inputs","text":"<p>This workflow consumes:</p>"},{"location":"workflows/forecast-to-evaluation/#1-forecast-ready-evaluation-panel","title":"1. Forecast-ready evaluation panel","text":"<p>The panel produced by Data \u2192 Forecast, containing: - realized demand, - aligned forecast candidates, - entity and time indices, - and evaluation metadata.</p> <p>This panel is treated as fixed for the duration of evaluation.</p>"},{"location":"workflows/forecast-to-evaluation/#2-evaluation-parameters","title":"2. Evaluation parameters","text":"<p>Explicitly declared parameters governing interpretation, including: - asymmetric cost ratios, - tolerance values or grids, - evaluation windows and aggregation rules.</p> <p>These parameters are treated as governed assumptions, not tuning knobs.</p>"},{"location":"workflows/forecast-to-evaluation/#3-structural-diagnostics-upstream","title":"3. Structural diagnostics (upstream)","text":"<p>Diagnostics that constrain how evaluation is interpreted, including: - Demand Quantization Compatibility (DQC), - and any declared snapping or representation requirements.</p> <p>Evaluation must respect these constraints.</p>"},{"location":"workflows/forecast-to-evaluation/#workflow-steps","title":"Workflow steps","text":""},{"location":"workflows/forecast-to-evaluation/#step-1-resolve-admissible-representation","title":"Step 1: Resolve admissible representation","text":"<p>Before computing metrics, evaluation resolves how values are to be interpreted.</p> <p>Based on upstream diagnostics: - demand may be treated as continuous-like, or - interpreted on a discrete grid with snapping enforced.</p> <p>This step establishes the authoritative unit system for all downstream diagnostics.</p>"},{"location":"workflows/forecast-to-evaluation/#step-2-compute-primitive-diagnostics","title":"Step 2: Compute primitive diagnostics","text":"<p>Evaluation computes readiness-oriented diagnostics for each forecast candidate, such as: - coverage and shortfall indicators, - asymmetric cost metrics, - tolerance-based hit rates, - conditional severity diagnostics.</p> <p>These metrics are descriptive, not prescriptive. They summarize behavior under declared assumptions.</p>"},{"location":"workflows/forecast-to-evaluation/#step-3-diagnose-responsiveness-and-structure","title":"Step 3: Diagnose responsiveness and structure","text":"<p>Where applicable, diagnostics are evaluated across: - tolerance ranges, - cost asymmetry sweeps, - or bounded adjustment envelopes.</p> <p>The purpose is to observe responsiveness, not to optimize outcomes.</p> <p>These observations support later compatibility and governance decisions.</p>"},{"location":"workflows/forecast-to-evaluation/#step-4-record-evaluation-context","title":"Step 4: Record evaluation context","text":"<p>All diagnostics are recorded together with: - the governing unit system, - evaluation parameters, - and the evaluation frame.</p> <p>Diagnostics without context are considered invalid.</p>"},{"location":"workflows/forecast-to-evaluation/#outputs","title":"Outputs","text":"<p>This workflow produces:</p>"},{"location":"workflows/forecast-to-evaluation/#1-evaluated-forecast-diagnostics","title":"1. Evaluated forecast diagnostics","text":"<p>For each forecast candidate: - a complete set of readiness diagnostics, - computed under authoritative interpretation semantics, - indexed to the evaluation context.</p> <p>These diagnostics are immutable once produced.</p>"},{"location":"workflows/forecast-to-evaluation/#2-evaluation-metadata","title":"2. Evaluation metadata","text":"<p>A structured record of: - assumptions, - thresholds, - unit conventions, - and diagnostic versions.</p> <p>This metadata is required for audit and reproducibility.</p>"},{"location":"workflows/forecast-to-evaluation/#scope-and-non-goals","title":"Scope and non-goals","text":"<p>This workflow explicitly does not:</p> <ul> <li>select a forecast,</li> <li>apply readiness adjustment,</li> <li>encode operational policy,</li> <li>or collapse diagnostics into a single decision score.</li> </ul> <p>Evaluation characterizes behavior; it does not decide.</p>"},{"location":"workflows/forecast-to-evaluation/#governance-notes","title":"Governance notes","text":"<p>Evaluation is constrained by governance but does not issue governance decisions.</p> <p>If evaluation reveals structural incompatibility or invalid interpretation, that information is passed forward. It is not resolved here.</p> <p>No evaluation result is authoritative without declared unit semantics and parameters.</p>"},{"location":"workflows/forecast-to-evaluation/#transition-to-selection","title":"Transition to selection","text":"<p>Once evaluation completes, the system can answer:</p> <p>Given governed assumptions, how do these forecasts behave relative to readiness objectives?</p> <p>Choosing among them is a separate step, addressed in Evaluation \u2192 Selection.</p>"},{"location":"workflows/reproducibility-artifacts/","title":"Reproducibility Artifacts","text":"<p>This page defines the minimal, authoritative artifact set required to reproduce any evaluation, selection, or policy decision produced under the Electric Barometer framework.</p> <p>Reproducibility in Electric Barometer is not aspirational. It is a hard requirement enforced through explicit artifacts, fixed assumptions, and deterministic workflows.</p>"},{"location":"workflows/reproducibility-artifacts/#why-reproducibility-is-treated-as-an-artifact","title":"Why reproducibility is treated as an artifact","text":"<p>Operational readiness decisions often outlive: - the people who made them, - the models that generated forecasts, - and the systems that executed them.</p> <p>As a result, Electric Barometer treats reproducibility not as a logging concern, but as a deliberate output of every workflow stage. If a result cannot be reproduced exactly from declared artifacts, it is considered incomplete.</p>"},{"location":"workflows/reproducibility-artifacts/#reproducibility-principle","title":"Reproducibility principle","text":"<p>A result is reproducible if\u2014and only if\u2014the following question can be answered unambiguously:</p> <p>Given the same declared inputs, parameters, and versions, would this workflow produce the same diagnostics, selections, and policy decisions?</p> <p>If the answer is \u201cnot guaranteed,\u201d required artifacts are missing.</p>"},{"location":"workflows/reproducibility-artifacts/#required-artifact-categories","title":"Required artifact categories","text":"<p>Reproducibility requires artifacts from each stage of the workflow pipeline.</p>"},{"location":"workflows/reproducibility-artifacts/#1-evaluation-context-artifact","title":"1. Evaluation context artifact","text":"<p>Defines what was evaluated.</p> <p>Must include: - entity definitions and identifiers, - evaluation resolution and horizon, - aggregation semantics, - inclusion / exclusion rules, - and the evaluation window.</p> <p>This artifact establishes the coordinate system for all downstream reasoning.</p>"},{"location":"workflows/reproducibility-artifacts/#2-input-data-snapshot","title":"2. Input data snapshot","text":"<p>Defines what was observed.</p> <p>Must include: - realized demand values aligned to the evaluation frame, - explicit representation of missing or zero-demand intervals, - and a versioned reference to the data source.</p> <p>Downstream artifacts may reference this snapshot but may not alter it.</p>"},{"location":"workflows/reproducibility-artifacts/#3-forecast-candidate-manifest","title":"3. Forecast candidate manifest","text":"<p>Defines what was evaluated.</p> <p>Must include: - an explicit enumeration of forecast candidates, - identifiers or hashes sufficient to retrieve each forecast, - confirmation of structural alignment to the evaluation frame.</p> <p>Forecasts not listed in this manifest are considered out of scope.</p>"},{"location":"workflows/reproducibility-artifacts/#4-evaluation-parameter-declaration","title":"4. Evaluation parameter declaration","text":"<p>Defines how behavior was interpreted.</p> <p>Must include: - asymmetric cost ratios, - tolerance values or grids, - snapping or representation requirements, - and any fixed thresholds used during evaluation.</p> <p>All parameters are treated as governed assumptions.</p>"},{"location":"workflows/reproducibility-artifacts/#5-diagnostic-output-bundle","title":"5. Diagnostic output bundle","text":"<p>Defines what was observed diagnostically.</p> <p>Must include: - readiness diagnostics for each forecast candidate, - evaluated under authoritative unit semantics, - with references to diagnostic definitions and versions.</p> <p>Diagnostics without parameter and unit context are invalid.</p>"},{"location":"workflows/reproducibility-artifacts/#6-selection-rationale-if-applicable","title":"6. Selection rationale (if applicable)","text":"<p>Defines why a forecast was chosen.</p> <p>Must include: - the selection objective, - any secondary criteria or tie-breakers, - and the dominance logic applied.</p> <p>This artifact explains selection without requiring re-evaluation.</p>"},{"location":"workflows/reproducibility-artifacts/#7-governance-decision-artifact-if-applicable","title":"7. Governance decision artifact (if applicable)","text":"<p>Defines what actions are allowed.</p> <p>Must include: - the authoritative governance decision, - declared unit system and tolerance semantics, - readiness adjustment allowability, - and explicit reasoning statements.</p> <p>This artifact is binding and terminates interpretation.</p>"},{"location":"workflows/reproducibility-artifacts/#determinism-and-versioning","title":"Determinism and versioning","text":"<p>All reproducibility artifacts must satisfy:</p> <ul> <li> <p>Determinism   Identical artifacts must produce identical outcomes.</p> </li> <li> <p>Explicit versioning   Diagnostic definitions, thresholds, and policies must be versioned or hashable.</p> </li> <li> <p>No hidden state   No workflow may rely on implicit defaults, learned state, or external context.</p> </li> </ul>"},{"location":"workflows/reproducibility-artifacts/#what-is-intentionally-excluded","title":"What is intentionally excluded","text":"<p>Reproducibility artifacts do not require: - model training code, - feature definitions, - hyperparameters, - or optimization internals.</p> <p>Electric Barometer reproduces decisions, not model training processes.</p>"},{"location":"workflows/reproducibility-artifacts/#governance-notes","title":"Governance notes","text":"<p>Reproducibility is enforced by governance.</p> <p>If required artifacts are missing, incomplete, or inconsistent: - evaluation results are non-authoritative, - selection outcomes are invalid, - and policy decisions must fail explicitly.</p> <p>Silent degradation is not permitted.</p>"},{"location":"workflows/reproducibility-artifacts/#summary","title":"Summary","text":"<p>Reproducibility artifacts ensure that Electric Barometer workflows are: - reviewable, - auditable, - transferable across teams and systems, - and defensible over time.</p> <p>They are not optional documentation\u2014they are part of the system\u2019s output.</p>"},{"location":"workflows/selection-to-policy/","title":"Selection \u2192 Policy","text":"<p>This workflow defines how a selected forecast is transformed into an authoritative policy decision suitable for downstream execution.</p> <p>In Electric Barometer, selection identifies which forecast to consider. Policy determines what actions are allowed. These are not the same step, and they are never collapsed.</p>"},{"location":"workflows/selection-to-policy/#objective","title":"Objective","text":"<p>The objective of the Selection \u2192 Policy workflow is to issue a binding governance outcome that declares:</p> <ul> <li>admissible interpretation semantics,</li> <li>readiness adjustment allowability,</li> <li>and execution constraints,</li> </ul> <p>given a selected forecast and its evaluated diagnostics.</p> <p>This workflow terminates interpretation. Once complete, no further diagnostic reasoning is admissible downstream.</p>"},{"location":"workflows/selection-to-policy/#inputs","title":"Inputs","text":"<p>This workflow consumes:</p>"},{"location":"workflows/selection-to-policy/#1-selected-forecast","title":"1. Selected forecast","text":"<p>The output of Evaluation \u2192 Selection.</p> <p>The selected forecast is treated as fixed and unmodified.</p>"},{"location":"workflows/selection-to-policy/#2-evaluation-diagnostics","title":"2. Evaluation diagnostics","text":"<p>The full diagnostic bundle associated with the selected forecast, including: - readiness metrics, - responsiveness observations, - and structural diagnostic outputs.</p> <p>All diagnostics are assumed to be evaluated under authoritative unit semantics.</p>"},{"location":"workflows/selection-to-policy/#3-structural-compatibility-diagnostics","title":"3. Structural compatibility diagnostics","text":"<p>Governance-relevant diagnostics, including: - Demand Quantization Compatibility (DQC), - Forecast Primitive Compatibility (FPC).</p> <p>These diagnostics constrain policy allowability.</p>"},{"location":"workflows/selection-to-policy/#4-governance-parameters","title":"4. Governance parameters","text":"<p>Explicit thresholds and policy rules governing: - tolerance interpretation, - readiness adjustment constraints, - and admissibility rules.</p> <p>These parameters are declared inputs, not tunable preferences.</p>"},{"location":"workflows/selection-to-policy/#workflow-steps","title":"Workflow steps","text":""},{"location":"workflows/selection-to-policy/#step-1-resolve-authoritative-representation","title":"Step 1: Resolve authoritative representation","text":"<p>Governance declares the single admissible representation under which policy is evaluated.</p> <p>Based on DQC: - raw-unit interpretation may be permitted, or - grid-aligned snapping may be mandatory.</p> <p>This declaration is exclusive. Mixed representations are not allowed.</p>"},{"location":"workflows/selection-to-policy/#step-2-assess-readiness-adjustment-allowability","title":"Step 2: Assess readiness adjustment allowability","text":"<p>Using authoritative representation semantics, governance evaluates Forecast Primitive Compatibility (FPC) to determine whether readiness adjustment is:</p> <ul> <li>Allowed,</li> <li>Conditionally allowed (under explicit constraints),</li> <li>or Disallowed.</li> </ul> <p>This determination is structural, not performance-based.</p>"},{"location":"workflows/selection-to-policy/#step-3-declare-policy-outcome","title":"Step 3: Declare policy outcome","text":"<p>Governance encodes the outcome as an explicit policy declaration, including: - interpretation semantics, - snapping requirements (if any), - readiness adjustment allowability, - and any binding constraints.</p> <p>No optimization or negotiation occurs at this stage.</p>"},{"location":"workflows/selection-to-policy/#step-4-issue-governance-decision-artifact","title":"Step 4: Issue governance decision artifact","text":"<p>The workflow emits a single Governance Decision artifact containing: - the governing unit system, - tolerance interpretation rules, - readiness policy status, - and explicit reasoning statements.</p> <p>This artifact is authoritative and binding.</p>"},{"location":"workflows/selection-to-policy/#outputs","title":"Outputs","text":"<p>This workflow produces:</p>"},{"location":"workflows/selection-to-policy/#1-governance-decision-artifact","title":"1. Governance decision artifact","text":"<p>The terminal output of the workflow pipeline.</p> <p>Downstream systems must consume this artifact directly and may not reinterpret or override it.</p>"},{"location":"workflows/selection-to-policy/#2-policy-metadata","title":"2. Policy metadata","text":"<p>A structured record linking the policy decision to: - the evaluation context, - the selected forecast, - and the diagnostics used.</p> <p>This metadata supports audit and reproducibility.</p>"},{"location":"workflows/selection-to-policy/#scope-and-non-goals","title":"Scope and non-goals","text":"<p>This workflow does not:</p> <ul> <li>adjust forecasts,</li> <li>select among alternative policies,</li> <li>optimize operational outcomes,</li> <li>or override upstream diagnostics.</li> </ul> <p>Policy declares admissibility; execution determines outcomes.</p>"},{"location":"workflows/selection-to-policy/#governance-notes","title":"Governance notes","text":"<p>Governance enforces decision closure.</p> <p>If required diagnostics are missing, inconsistent, or inadmissible: - no policy is issued, - the workflow fails explicitly, - and downstream execution is prohibited.</p> <p>Graceful degradation is intentionally disallowed.</p>"},{"location":"workflows/selection-to-policy/#summary","title":"Summary","text":"<p>The Selection \u2192 Policy workflow ensures that:</p> <ul> <li>readiness decisions are structurally valid,</li> <li>assumptions are explicit and enforceable,</li> <li>and execution systems receive unambiguous guidance.</li> </ul> <p>It is the boundary where evaluation ends and responsibility begins.</p>"}]}